<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Leon&#39;s Blog</title>
    <link>http://blog.songru.org/</link>
    <description>Recent content on Leon&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Sat, 18 Mar 2017 09:54:19 +0800</lastBuildDate>
    <atom:link href="http://blog.songru.org/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Ubuntu修复Grub引导(备忘)</title>
      <link>http://blog.songru.org/posts/linux/Ubuntu-grub/</link>
      <pubDate>Sat, 18 Mar 2017 09:54:19 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/linux/Ubuntu-grub/</guid>
      <description>&lt;p&gt;我的系统是win7 + Ubuntu双系统, 但win7基本不用了. 由于/home目录快满了, 于是在Ubuntu中删除了win7系统下的C盘所在的分区. 但是我没有注意到我的引导程序是安装在这个分区, 导致重启之后两个系统都进不去了.&lt;/p&gt;

&lt;p&gt;下面是修复步骤:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;使用Ubuntu的U盘启动盘进入试用模式, 打开命令行, 在这个模式下可以查看以及操作系统的分区.&lt;/li&gt;
&lt;li&gt;安装testdisk软件, 用于恢复删除的分区.&lt;/li&gt;
&lt;li&gt;使用sudo或root运行testdisk_static, 选择&lt;code&gt;create -&amp;gt; Disk /dev/sda(选择需要恢复的磁盘) -&amp;gt; Proceed -&amp;gt; Intel -&amp;gt; Analyse -&amp;gt; Backup(备份当前的分区表, 不行还可以使用它进行恢复) -&amp;gt; Quick Search&lt;/code&gt;,  之后会查找出你丢失的分区, 然后可以选择&lt;code&gt;write&lt;/code&gt;进行写入.&lt;/li&gt;
&lt;li&gt;再找回分区时, 发现testdisk给我们多生成了一个分区, 据我的理解也没有影响, 因为我的C盘分区是磁盘的第一个分区, 以0号扇区为起点. 而引导器数据是写入在分区的第一个磁道上, 所以如果恢复后的C盘还是从0号扇区开始, 那么就相当于引导数据也恢复了.&lt;/li&gt;
&lt;li&gt;这样就找回了误删的分区, 重启电脑出现了easybcd创建的启动项, 可以进入win7系统, 但是不能进入ubuntu, 显示一个命令行&lt;code&gt;grub rescue &amp;gt;&lt;/code&gt;.不知道什么原因.&lt;/li&gt;
&lt;li&gt;再次用Ubuntu启动盘进入试用, &lt;code&gt;fdisk -l&lt;/code&gt;找到&lt;code&gt;/&lt;/code&gt;分区,然后使用&lt;code&gt;grub2-install /dev/sda&lt;/code&gt;安装grub(大概是这个命令&amp;hellip;.)&lt;/li&gt;
&lt;li&gt;再次重启, 显示的是grub2的启动项, 选择Ubuntu, 这一次没有出现&lt;code&gt;grub rescue &amp;gt;&lt;/code&gt;模式, 而是进入了启动界面, 但是一直进不去系统.&lt;/li&gt;
&lt;li&gt;从显示的信息可以看到, 应该是开机自动挂载出现了问题. 这是因为我使用testdisk恢复分区时, 虽然恢复了第一个C盘分区, 但是软件还自动修复了一些分区上的错误, 给我多加了一个分区. 这就导致各个分区的设备目录名(/dev/sda1这些)发生了变化, 所以需要更新/etc/fstab中的设备目录名.&lt;/li&gt;
&lt;li&gt;重启, 进入Ubuntu的recovery mode, 进入root命令模式. 因为默认的根目录和home目录是使用UUID进行挂载的, 所以它们是不会变的, 只需要把其他的自定义的自动挂载项注释掉即可(或者更新).&lt;/li&gt;
&lt;li&gt;但是发现不能编辑, 提示是read only模式, 重新挂载根目录&amp;rdquo;&lt;code&gt;mount –o remount, rw /&lt;/code&gt;&amp;rdquo;, 再编辑即可.&lt;/li&gt;
&lt;li&gt;重启&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Linux Socket使用小结</title>
      <link>http://blog.songru.org/posts/linux/Linux-Socket/</link>
      <pubDate>Sat, 18 Mar 2017 09:54:01 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/linux/Linux-Socket/</guid>
      <description>&lt;p&gt;&lt;strong&gt;1. 关于recv, read, write, send方法&lt;/strong&gt;
recv较read多了一个flag参数，如果flag为0，则recv相当于read，write与send类似。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. 关于阻塞&lt;/strong&gt;
recv方法默认会阻塞，直到有数据读取或者对方关闭了socket。返回读取到的字节数，这个数可能会小于指定要读取的大小，所以socket编程应该是需要自己协商需要读取的数据长度（比如把长度放在最开始）。
send方法默认不会阻塞，send之后立马返回，数据应该是已经发送到了网络上。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. 总结&lt;/strong&gt;
socket就有点像是两个命名管道， &lt;code&gt;pipe_1[]&lt;/code&gt; 与 &lt;code&gt;pipe_2[]&lt;/code&gt;比如：
server端读取的是&lt;code&gt;pipe_1[0]&lt;/code&gt;的数据，client端向&lt;code&gt;pipe_1[1]&lt;/code&gt;写入数据。
client端读取的是&lt;code&gt;pipe_2[0]&lt;/code&gt;的数据，server端向&lt;code&gt;pipe_2[1]&lt;/code&gt;写入数据。
读数据时默认会阻塞，等待另一端向管道写入数据；写数据默认不阻塞，写入的数据直接到pipe中去了，就算写入端关闭了socket，读写的还能从pipe中获取已写入的数据。&lt;/p&gt;

&lt;p&gt;可以通过select函数检测文件描述符的状态，比如有数据可读时则去读之类的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux IPC总结(Posix和System V对比)</title>
      <link>http://blog.songru.org/posts/linux/Linux-IPC/</link>
      <pubDate>Sat, 18 Mar 2017 09:53:43 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/linux/Linux-IPC/</guid>
      <description>

&lt;h2 id=&#34;1-管道-pipe:37b1137f87a98db4271649d677fe0ecf&#34;&gt;1. 管道(Pipe)&lt;/h2&gt;

&lt;h3 id=&#34;1-1-匿名管道-unnamed-pipe:37b1137f87a98db4271649d677fe0ecf&#34;&gt;1.1 匿名管道(Unnamed Pipe)&lt;/h3&gt;

&lt;h4 id=&#34;1-1-1-popen:37b1137f87a98db4271649d677fe0ecf&#34;&gt;1.1.1 popen&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
FILE* popen (const char *command, const char *open_mode);
int pclose(FILE *stream_to_close);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　popen()函数通过创建一个管道，调用fork()产生一个子进程，执行一个shell以运行命令来开启一个进程。这个管道必须由pclose()函数关闭，而不是fclose()函数。pclose()函数关闭标准I/O流，等待命令执行结束，然后返回shell的终止状态。如果shell不能被执行，则pclose()返回的终止状态与shell已执行exit一样。&lt;/p&gt;

&lt;p&gt;　　type参数只能是读或者写中的一种，得到的返回值（标准I/O流）也具有和type相应的只读或只写类型。如果type是&amp;rdquo;r&amp;rdquo;则文件指针连接到command的标准输出；如果type是&amp;rdquo;w&amp;rdquo;则文件指针连接到command的标准输入。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; #include &amp;lt;unistd.h&amp;gt;
 #include &amp;lt;stdlib.h&amp;gt;
 #include &amp;lt;stdio.h&amp;gt;
 #include &amp;lt;string.h&amp;gt;
 
 int main()
 {
     FILE *read_fp = NULL;
     FILE *write_fp = NULL;
     char buffer[BUFSIZ + 1]; 
     printf(&amp;quot;BUFSIZ: %d\n&amp;quot;, BUFSIZ);
     int chars_read = 0;
     
     //初始化缓冲区
     memset(buffer, &#39;\0&#39;, sizeof(buffer));
     //打开ls和grep进程
     read_fp = popen(&amp;quot;ls -l /home/lsr&amp;quot;, &amp;quot;r&amp;quot;);
     write_fp = popen(&amp;quot;grep rwxrwxr-x&amp;quot;, &amp;quot;w&amp;quot;);
     //两个进程都打开成功
     if(read_fp &amp;amp;&amp;amp; write_fp)
     {   
         //读取一个数据块
         chars_read = fread(buffer, sizeof(char), BUFSIZ, read_fp);
         while(chars_read &amp;gt; 0)
         {
             //设置结尾字符
             buffer[chars_read] = &#39;\0&#39;;
             //把数据写入grep进程
             fwrite(buffer, sizeof(char), chars_read, write_fp);
             //还有数据可读，循环读取数据，直到读完所有数据
             chars_read = fread(buffer, sizeof(char), BUFSIZ, read_fp);
         }
         //关闭文件流
         pclose(read_fp);
         pclose(write_fp);
         exit(EXIT_SUCCESS);
     }   
     exit(EXIT_FAILURE);
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;1-1-2-pipe:37b1137f87a98db4271649d677fe0ecf&#34;&gt;1.1.2 pipe&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;unistd.h&amp;gt;
int pipe(int file_descriptor[2]);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pipe&lt;/code&gt;函数跟&lt;code&gt;popen&lt;/code&gt;函数的一个重大区别是，&lt;code&gt;popen&lt;/code&gt;函数是基于文件流（FILE）工作的，而&lt;code&gt;pipe&lt;/code&gt;是基于文件描述符工作的，所以在使用&lt;code&gt;pipe&lt;/code&gt;后，数据必须要用底层的&lt;code&gt;read&lt;/code&gt;和&lt;code&gt;write&lt;/code&gt;调用来读取和发送。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pipe&lt;/code&gt;函数会创建两个文件描述符, 并且不要用&lt;code&gt;file_descriptor[0]&lt;/code&gt;写数据，也不要用&lt;code&gt;file_descriptor[1]&lt;/code&gt;读数据，其行为未定义的，但在有些系统上可能会返回-表示调用失败。数据只能从&lt;code&gt;file_descriptor[0]&lt;/code&gt;中读取，数据也只能写入到&lt;code&gt;file_descriptor[1]&lt;/code&gt;, 不能倒过来。&lt;/p&gt;

&lt;p&gt;它的一个&lt;strong&gt;缺点&lt;/strong&gt;，就是通信的进程，它们的关系一定是父子进程的关系，这就使得它的使用受到了一点的限制，但是我们可以使用命名管道来解决这个问题。&lt;/p&gt;

&lt;h3 id=&#34;1-2-命名管道-named-pipe:37b1137f87a98db4271649d677fe0ecf&#34;&gt;1.2 命名管道(Named Pipe)&lt;/h3&gt;

&lt;p&gt;命名管道也被称为FIFO文件，它是一种特殊类型的文件，它在文件系统中以文件名的形式存在，但是它的行为却和之前所讲的没有名字的管道（匿名管道）类似。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
#include &amp;lt;sys/types.h&amp;gt;
#include &amp;lt;sys/stat.h&amp;gt;
int mkfifo(const char *filename, mode_t mode);
int mknod(const char *filename, mode_t mode | S_IFIFO, (dev_t)0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这两个函数都能创建一个FIFO文件，注意是创建一个真实存在于文件系统中的文件，filename指定了文件名，而mode则指定了文件的读写权限。
&lt;code&gt;mknod&lt;/code&gt;是比较老的函数，而使用&lt;code&gt;mkfifo&lt;/code&gt;函数更加简单和规范，所以建议在可能的情况下，尽量使用&lt;code&gt;mkfifo&lt;/code&gt;而不是&lt;code&gt;mknod&lt;/code&gt;。
在Linux系统的shell下, 也可以使用&lt;code&gt;mkfifo&lt;/code&gt;或者&lt;code&gt;mknod&lt;/code&gt;命令创建管道文件, 然后通过流重定向&lt;code&gt;echo hello &amp;gt; myfifo&lt;/code&gt;可以向管道写入, 通过&lt;code&gt;cat mypipe&lt;/code&gt;可以读取管道数据.&lt;/p&gt;

&lt;p&gt;在C++程序中, 也是使用&lt;code&gt;open&lt;/code&gt;函数打开一个管道, 然后在不同的进程中可以只用&lt;code&gt;read&lt;/code&gt;和&lt;code&gt;write&lt;/code&gt;函数读取和写入数据到管道中.&lt;/p&gt;

&lt;h2 id=&#34;2-信号-signal:37b1137f87a98db4271649d677fe0ecf&#34;&gt;2. 信号(Signal)&lt;/h2&gt;

&lt;p&gt;信号是UNIX和Linux系统响应某些条件而产生的一个事件，接收到该信号的进程会相应地采取一些行动。通常信号是由一个错误产生的。但它们还可以作为进程间通信或修改行为的一种方式，明确地由一个进程发送给另一个进程。一个信号的产生叫生成，接收到一个信号叫捕获。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;signal.h&amp;gt;
void (*signal(int sig, void (*func)(int)))(int);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;程序可用使用&lt;code&gt;signal&lt;/code&gt;函数来指定接受到某种信号后的处理方式，也可以选择忽略和恢复其默认行为来工作。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;signal&lt;/code&gt;函数定义的是某一个信号的行为, 我们还可以使用信息集的系列函数来定义多个信号的行为, 比如可以设定一些我们需要处理的信号，并设置一些我们不需要处理的信号.&lt;/p&gt;

&lt;h2 id=&#34;3-信号量-semaphore:37b1137f87a98db4271649d677fe0ecf&#34;&gt;3. 信号量(Semaphore)&lt;/h2&gt;

&lt;p&gt;信号量的使用主要是用来&lt;strong&gt;保护共享资源&lt;/strong&gt;，使得资源在一个时刻只有一个进程（线程）所拥有。
信号量的值为正的时候，说明它空闲。所测试的线程可以锁定而使用它。若为0，说明它被占用，测试的线程要进入睡眠队列中，等待被唤醒。&lt;/p&gt;

&lt;h3 id=&#34;3-1-posix信号量:37b1137f87a98db4271649d677fe0ecf&#34;&gt;3.1 POSIX信号量&lt;/h3&gt;

&lt;p&gt;对POSIX来说，信号量是个&lt;strong&gt;非负整数&lt;/strong&gt;。常用于线程间同步。
POSIX信号量的引用头文件是&lt;code&gt;&amp;lt;semaphore.h&amp;gt;&lt;/code&gt;, 其相关函数名都是sem_op这个带下划线的形式.&lt;/p&gt;

&lt;h3 id=&#34;3-1-2-无名信号量:37b1137f87a98db4271649d677fe0ecf&#34;&gt;3.1.2 无名信号量&lt;/h3&gt;

&lt;p&gt;无名信号量常用于多线程间的同步，同时也用于相关进程间的同步。也就是说，无名信号量必须是多个进程（线程）的共享变量，无名信号量要保护的变量也必须是多个进程（线程）的共享变量，这两个条件是缺一不可的。
POSIX无名信号量由&lt;code&gt;sem_init&lt;/code&gt;函数创建:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;semaphore.h&amp;gt;
int sem_init(sem_t *sem, int pshared, unsigned int value);

Link with -pthread.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-共享内存-shared-memory:37b1137f87a98db4271649d677fe0ecf&#34;&gt;4. 共享内存(Shared memory)&lt;/h2&gt;

&lt;h2 id=&#34;5-消息队列-message-queue:37b1137f87a98db4271649d677fe0ecf&#34;&gt;5. 消息队列(Message queue)&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Centos 7安装tmate和tmate-slave</title>
      <link>http://blog.songru.org/posts/linux/CentOS7-Install-tmate&amp;tmate-slave/</link>
      <pubDate>Sat, 18 Mar 2017 09:48:26 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/linux/CentOS7-Install-tmate&amp;tmate-slave/</guid>
      <description>&lt;pre&gt;&lt;code&gt;yum install gcc kernel-devel make ncurses-devel
yum install git cmake ruby zlib-devel openssl-devel libevent-devel ncurses-devel
yum group install &amp;quot;Development Tools&amp;quot;
yum install libssh libssh-devel

# Install msgpack
yum install msgpack msgpack-devel python-msgpack
# If not work, then install with source
git clone https://github.com/msgpack/msgpack-c.git
cd msgpack-c
cmake .
make
sudo make install

# Install tmate
./autogen.sh &amp;amp;&amp;amp; \
./configure  &amp;amp;&amp;amp; \
make         &amp;amp;&amp;amp; \
make install

# If not work, install libevent2 with source
wget https://github.com/downloads/libevent/libevent/libevent-2.0.21-stable.tar.gz
# cd to libevent2 src
./configure --prefix=/usr/local
make &amp;amp;&amp;amp; make install

# Install tmate-slave
git clone https://github.com/tmate-io/tmate-slave.git &amp;amp;&amp;amp; cd tmate-slave
./create_keys.sh # This will generate SSH keys, remember the keys fingerprints.
./autogen.sh &amp;amp;&amp;amp; ./configure &amp;amp;&amp;amp; make

# Start
sudo ./tmate-slave -p 222
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Ubuntu 14.04 笔记本双显卡CUDA 安装</title>
      <link>http://blog.songru.org/posts/linux/NoteBook-Ubuntu-Install/</link>
      <pubDate>Sat, 18 Mar 2017 09:36:53 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/linux/NoteBook-Ubuntu-Install/</guid>
      <description>

&lt;h1 id=&#34;ubuntu-14-04-笔记本双显卡cuda-安装:b5033d739bcdb1d7b241dd7a0f168519&#34;&gt;Ubuntu 14.04 笔记本双显卡CUDA 安装&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;安装ubuntu系统&lt;/li&gt;
&lt;li&gt;在系统设置 -&amp;gt;  软件和更新 -&amp;gt; 附加驱动 中如果有nvidia驱动， 则表示该系统支持cuda， 可以继续后面步骤。&lt;/li&gt;
&lt;li&gt;下载新版的NVIDIA显卡驱动， CUDA的run file里面的驱动或者是deb中的都太老了, 不适合新版tensorflow。&lt;/li&gt;
&lt;li&gt;安装新版的NVIDIA驱动：&lt;br /&gt;
&lt;code&gt;sh NVIDIA-Linux-x86_64-375.26.run -no-x-check -no-nouveau-check -no-opengl-files&lt;/code&gt;
–no-x-check 安装驱动时关闭X服务
–no-nouveau-check 安装驱动时禁用nouveau
–no-opengl-files 只安装驱动文件，不安装OpenGL文件&lt;/li&gt;
&lt;li&gt;通过run文件安装cuda，选择不安装显卡驱动&lt;/li&gt;
&lt;li&gt;成功&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Sequence to Sequence Learning with Neural Networks</title>
      <link>http://blog.songru.org/posts/notebook/Sequence-to-Sequence-Learning-with-Neural-Networks/</link>
      <pubDate>Fri, 03 Jun 2016 14:55:16 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/notebook/Sequence-to-Sequence-Learning-with-Neural-Networks/</guid>
      <description>&lt;p&gt;该论文使用Encoder-Decoder模型, 进行end-to-end的训练来进行机器翻译. 该论文与&lt;a href=&#34;http://blog.songru.org/posts/notebook/Neural_Machine_Translation_By_Jointly_Learning_To_Align_And_Translate_NOTE/&#34;&gt;Neural Machine Translation By Jointly Learning To Align And Translate&lt;/a&gt;的主要区别在于:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;该论文中不是使用Bidirectional RNN, 而是使用了multi-layer RNN, 发现比shallow RNN效果好(可能因为deep结构包含更多的隐藏状态).&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;该论文将输入序列进行反向, 再一次输入到Encoder的RNN中.&lt;br /&gt;
正常顺序输入的Encoder序列与Decoder序列之间有一个比较大的&amp;rdquo;minimal time lag&amp;rdquo;, 将输入序列方向之后, 虽然输入序列与输出序列对应词语之间的平均距离没有改变, 但是输入序列最前面的一些词语与输出序列的对应词语更加近了, 也就是说&amp;rdquo;minimal time lag&amp;rdquo;减小了.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;该论文没有使用attention.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;该论文在输出翻译句子时使用了&lt;a href=&#34;https://en.wikipedia.org/wiki/Beam_search&#34;&gt;beam search&lt;/a&gt;方法, 而不是传统的greedy search方法.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;下面介绍一下Sequence to Sequence Learning当中的&lt;strong&gt;beam search&lt;/strong&gt;:&lt;br /&gt;
在训练好模型之后, 预测阶段我们需要逐个词语的进行预测, 通常的做法(greedy search)是从Decoder的第一个时刻开始选取概率最大的词作为下一个时刻的输入, 这样依次预测得到最终的结果. 但是这里存在一个问题, 就是&lt;strong&gt;最可能的预测结果序列可能并不是从选取的最可能的那个词语开始的.&lt;/strong&gt;为了找到概率最大的预测结果, 可以简单的采用列举所有可能的输出序列, 然后选取最大概率的一个, 但是计算的复杂度与句子长度呈指数级增长, 效率太低.&lt;/p&gt;

&lt;p&gt;Beam search的思想是首先指定一个数$b$, 称$b$为beam size或beam width. 接下来要做的不是找到最有可能的第一个词, 而是第一个词中最可能的前$b$个(这$b$个候选词就成为beam); 接下来由第一个词预测第二个词, 依次使用选出的$b$个候选词进行预测, 并计算所有这些长度为2的序列的概率(一共$b*n$个这样的序列, $n$为词典大小), 从中选出概率最大的$b$个. 接下来使用前面选定的b个最大概率序列(长度为2)来计算概率最大的长度为3的前$n$个序列, 以此类推.&lt;br /&gt;
Beam search的计算量是greedy search的$b$倍, $b$一般不会取太大(2-5貌似).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;So the RNN estimates the joint distribution:&lt;br /&gt;
$p(X_1, X_2, X_3, \cdots, X_N)$ over a set of random variables&lt;br /&gt;
What we really want is the mode of this distribution, that is the point with the highest probability. One way to get this is through sampling from the joint distribution and taking the highest probability sample, but this is slow.&lt;br /&gt;
The RNN factors the joint distribution for 3 random variables in this way:&lt;br /&gt;
$$p(x_1, x_2, x_3) = p(x_3 \mid x_1,x_2) * p(x_2 \mid x_1) * p(x_1)$$&lt;br /&gt;
Beam search uses a heuristic that assumes that chains of random variables with high probability have fairly high probability conditionals. Basically you take the k highest probability solutions for $p(x_1)$, then for each of those take the k highest probability solutions for $p(x_2 \mid x_1)$. You then take the k of those with the highest value for $p(x_2 \mid x_1) * p(x_1)$ and repeat.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Opinion Mining with Deep Recurrent Neural Networks笔记</title>
      <link>http://blog.songru.org/posts/notebook/Opinion_Mining_with_Deep_Recurrent_Neural_Networks_NOTE/</link>
      <pubDate>Tue, 24 May 2016 21:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/notebook/Opinion_Mining_with_Deep_Recurrent_Neural_Networks_NOTE/</guid>
      <description>&lt;p&gt;本文对传统的RNN进行改进，结合BidirectionalRNN，提出了Deep bidirectional RNN。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.songru.org/img/1464096878586.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;传统的RNN&lt;/strong&gt;（图a）信息传播方向是从前往后一个方向，某个时刻t的隐藏状态仅仅包含它之前词语的语义信息：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464097111614.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;br /&gt;
$f$为非线性激活函数（比如sigmoid），$g$为输出的计算函数（比如softmax）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bidirectional RNN&lt;/strong&gt;（图b）包含前向和后向RNN两个部分，分别从相反的反向进行信息的传播，再将每个时刻的两个方向的隐藏状态联合起来计算输出值：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464097064201.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;可以看到，前向RNN和反向RNN的参数是互相独立的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep  RNN&lt;/strong&gt;（图c）是将多个传统RNN进行叠加而来，即每一层计算隐藏状态的输入都为上一层的输出。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep Bidirectional  RNN&lt;/strong&gt;（图d）则是将Bidirectional RNN与Deep RNN结合起来：&lt;br /&gt;
第$i$层（$i&amp;gt;1$）第$t$个时刻的前向隐藏状态$\overrightarrow{{h_t}^{(i)}}$依赖于三个输入：第$i-1$层$t$时刻的前向隐藏状态$\overrightarrow{{h_t}^{(i-1)}}$和后向隐藏状态$\overleftarrow{{h_t}^{(i-1)}}$以及第$i$层$t-1$时刻的前向隐藏状态.&lt;br /&gt;
第$i$层（$i&amp;gt;1$）第$t$个时刻的后向隐藏状态$\overleftarrow{{h_t}^{(i)}}$的计算同理:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464098995066.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;第1层隐藏状态机算比较特殊,也比较简单:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464099041075.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;输出的计算有两种选择:一是使用所有时刻的隐藏状态计算输出,或者仅使用最后一个时刻的隐藏状态.这里使用第二种方案:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464097751597.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;该文是通过堆叠（stack）RNN的方式达到Deep RNN的结构，&lt;a href=&#34;http://arxiv.org/abs/1312.6026&#34;&gt;Pascanu等&lt;/a&gt;的论文采取了另一种思路进行Deep RNN的扩展。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ubuntu下matplotlib绘图中文乱码</title>
      <link>http://blog.songru.org/posts/linux/Ubuntu_Matplotlib_fix/</link>
      <pubDate>Sat, 23 Apr 2016 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/linux/Ubuntu_Matplotlib_fix/</guid>
      <description>

&lt;p&gt;原因：在Ubuntu下安装了各种中文字体，但是修改matplotlibrc文件后，均提示找不到该字体，猜测可能matplotlib字体列表与系统字体列表不同。&lt;/p&gt;

&lt;h2 id=&#34;方法一-持久性修改:c4a9591d3c6525e63df3a6e304bb9db6&#34;&gt;方法一（持久性修改）&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;首先查看matplotlib支持的中文字体&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# -*- coding: utf-8 -*-
from matplotlib.font_manager import FontManager
import subprocess

fm = FontManager()
mat_fonts = set(f.name for f in fm.ttflist)

output = subprocess.check_output(
    &#39;fc-list :lang=zh -f &amp;quot;%{family}\n&amp;quot;&#39;, shell=True)
# print &#39;*&#39; * 10, &#39;系统可用的中文字体&#39;, &#39;*&#39; * 10
# print output
zh_fonts = set(f.split(&#39;,&#39;, 1)[0] for f in output.split(&#39;\n&#39;))
available = mat_fonts &amp;amp; zh_fonts

print &#39;*&#39; * 10, &#39;可用的字体&#39;, &#39;*&#39; * 10
for f in available:
    print f
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出为：
Droid Sans Fallback
YaHei Consolas Hybrid
就是求出系统字体列表与matplotlib字体列表的交集&lt;/p&gt;

&lt;p&gt;2.修改matplotlibrc文件&lt;br /&gt;
Ubuntu默认对应的是/etc/matplotlibrc，可以复制到～/.matplotlibrc/matplotlibrc，然后配置后者即可
修改&lt;strong&gt;font.sans-serif&lt;/strong&gt;为上面的一个输出结果即可, 还需要修改&lt;strong&gt;axes.unicode_minus&lt;/strong&gt;为&lt;strong&gt;False&lt;/strong&gt;,否则图像是负号&amp;rsquo;-&amp;lsquo;会显示为方块.&lt;/p&gt;

&lt;h2 id=&#34;方法二-临时性修改:c4a9591d3c6525e63df3a6e304bb9db6&#34;&gt;方法二（临时性修改）&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# -*- coding: utf-8 -*-
 import matplotlib as mpl
 import matplotlib.pyplot as plt
 
 mpl.rcParams[&#39;font.sans-serif&#39;] = [&#39;Droid Sans Fallback&#39;] # 指定字体名字
 mpl.rcParams[&#39;axes.unicode_minus&#39;] = False #解决保存图像是负号&#39;-&#39;显示为方块的问题
 plt.figure()
 plt.xlabel(u&#39;性别&#39;)
 plt.ylabel(u&#39;人数&#39;)
 plt.xticks((0,1),(u&#39;男&#39;,u&#39;女&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# -*- coding: utf-8 -*-
 import matplotlib.pyplot as plt
 from matplotlib import font_manager

 zh_font = font_manager.FontProperties(fname=r&#39;/home/lsr/Documents/simsun.ttf&#39;, size=14) # 指定字体文件
 
 plt.figure()
 plt.xlabel(u&#39;性别&#39;, fontproperties=zh_font) # 使用字体配置
 plt.ylabel(u&#39;人数&#39;,fontproperties=zh_font)
 plt.xticks((0,1),(u&#39;男&#39;,u&#39;女&#39;)) # 没有使用字体配置，乱码
 plt.bar(left = (0,1),height = (1,0.5),width = 0.35)
 plt.show()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Neural Machine Translation By Jointly Learning To Align And Translate笔记</title>
      <link>http://blog.songru.org/posts/notebook/Neural_Machine_Translation_By_Jointly_Learning_To_Align_And_Translate_NOTE/</link>
      <pubDate>Sun, 27 Mar 2016 22:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/notebook/Neural_Machine_Translation_By_Jointly_Learning_To_Align_And_Translate_NOTE/</guid>
      <description>&lt;p&gt;本文主要创新在传统的神经机器翻译上进行改进，确切的说是改进了基本的RNN Encoder-Decoder模型,提出了Alignment model，即实现了Attention model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;传统的encoder-decoder模型如下图所示:
&lt;img src=&#34;http://blog.songru.org/img/1458651310553.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
该模型通过神经网络将所有的输入信息压缩为一个固定长度的向量$w$,然后通过decoder的神经网络解码这个$w$，最后得出翻译结果.&lt;br /&gt;
使用固定长度的向量是该模型的一个缺点,因为该向量很难将所有需要的信息编码到其中，对于长度大的句子,该模型的效果会显著下降.&lt;/p&gt;

&lt;p&gt;本文作者提出的改进模型结构为:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458651809449.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
主要有4个方面改进:&lt;br /&gt;
1. &lt;strong&gt;GRU 和 Bidirectional RNN&lt;/strong&gt;&lt;br /&gt;
RNN网络的结果采用的是GRU单元，双向神经网络(Bidirectional).一个BiRNN由前向(forward)和后向(backward)RNN组成,前向RNN &amp;amp;(\stackrel{\rightarrow}{\mbox{f}})&amp;amp; 按顺序读取输入序列(从&amp;amp;(x_f)&amp;amp;到&amp;amp;(x_{T_x})&amp;amp;)，计算得出前向RNN的隐含状态序列&amp;amp;((\vec{h_1},\cdots,\vec{h_{T_x}} ))&amp;amp; 。反向RNN$\stackrel{\leftarrow}{\mbox{f}}$则逆序读取输入序列(从$x_{T_x}$到$x_1$)，计算得出$(\stackrel{\leftarrow}{h_1},\cdots,\stackrel{\leftarrow}{h_{T_x}})$ .&lt;br /&gt;
对于一个词$x_j$直接concatenating前向$\vec{h_j}$与后向$\stackrel{\leftarrow}{h_j}$，即$h_j=\left[\begin{array}{c}\stackrel{\rightarrow}{h_j}  \\  \stackrel{\leftarrow}{h_j}  \end{array} \right]$，计算时词向量矩阵$E$是前向与后向网络共享的，其他参数则不是.&lt;br /&gt;
2. &lt;strong&gt;对齐模型(alignment model)&lt;/strong&gt;&lt;br /&gt;
该模型也可以说是实现了注意力模型(Attention model).&lt;br /&gt;
通过该模型计算得到输入时刻$j$在预测输出时刻$i$时所占的权重$\alpha _{ij}$.比如说翻译&lt;strong&gt;&amp;ldquo;我喜欢飞机&amp;rdquo;&lt;/strong&gt;到&lt;strong&gt;&amp;ldquo;I like airplane&amp;rdquo;&lt;/strong&gt;,翻译输出&lt;strong&gt;&amp;ldquo;I&amp;rdquo;&lt;/strong&gt;时，&lt;strong&gt;&amp;ldquo;我&amp;rdquo;&lt;/strong&gt;字所占的权重会比较大.&lt;br /&gt;
权重$\alpha _{ij}$是通过一个单层的多层感知机计算得到,该模型与算法中其他部分同时训练:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458736905308.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
3. &lt;strong&gt;Encoder 和 Decoder&lt;/strong&gt;&lt;br /&gt;
Encoder的计算如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458737365201.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
$E$表示词向量的矩阵，$x_i$表示$i$时刻的词,是一个$k$维的向量(词典大小维度的向量,应该是one-hot的)，反向过程的计算与上面类似。&lt;/p&gt;

&lt;p&gt;Decoder的计算如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458737469906.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
初始的隐藏状态为：$$s_0 = tanh(W_s\stackrel{\leftarrow}{h_1})$$&lt;br /&gt;
每一步的context向量都需要通过Alignment model重新计算:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738176634.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738239955.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
最后计算$y_i$的概率:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738415785.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738447372.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738462004.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
使用了maxout（第二个公式），取相邻两个数中较大的一个。需要计算词典中所有词出现的概率，最后取最大的那一个？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;具体的实现细节见论文的附录&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation笔记</title>
      <link>http://blog.songru.org/posts/notebook/Learning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation_NOTE/</link>
      <pubDate>Sun, 27 Mar 2016 21:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/notebook/Learning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation_NOTE/</guid>
      <description>&lt;p&gt;本文主要创新在于提出一个新的神经网络模型RNN Encoder-Decoder，并提出GRU单元.将训练的模型作为standard phrase-based statistical machine translation system的一部分，用于计算phrase table中的每一个phrase的得分。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;1. RNN Encoder-Decoder&lt;/strong&gt;
该模型由两个RNN组成，分别作为Encoder和Decoder，Encoder的作用是读取一个变长的序列数据，将其编码为一个固定长度的向量，再通过Decoder将这个向量解码为一个变长的序列数据.&lt;/p&gt;

&lt;p&gt;Encoder为一个由GRU单元组成的RNN网络:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458734687528.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
$e(x_t)$代表t时刻输入词的向量表示，初始的隐藏状态$h^{(0)}$固定为$0$，最后计算得出一个固定长度的编码结果：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458735958249.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
Decoder也为一个由GRU单元组成的RNN网络:
它首先初始化隐藏状态：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458736097645.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
其网络的计算公式为：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458736032536.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
这里每一个时刻的计算都用到了Encoder传递过来的编码向量$c$，并且它的值是不变的。&lt;br /&gt;
最后通过Softmax计算概率（没看懂），还有maxout。。。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theano 中使用pydot报错</title>
      <link>http://blog.songru.org/posts/machine-learning/theano%20pydot/</link>
      <pubDate>Sun, 03 Jan 2016 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/machine-learning/theano%20pydot/</guid>
      <description>&lt;p&gt;运行:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 切换为使用cpu,有这句var_with_name_simple=True的时候会报错...,换成gpu就不报错...
theano.printing.pydotprint(forward_prop, var_with_name_simple=True, compact=True, outfile=&#39;img/nn-theano-forward_prop.png&#39;, format=&#39;png&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;报错:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pywintypes.error(2,&#39;RegOpenKeyEx&#39;,&#39;\xcf\xb5\xcd\xb3\xd5\xd2\xb2\xbb\xb5\xb...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因是未安装好graphviz,于是下载&lt;a href=&#34;http://www.graphviz.org/Download_windows.php&#34;&gt;安装包&lt;/a&gt;,并把安装目录的bin加入到path中(重要!)&lt;/p&gt;

&lt;p&gt;后来又报错:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RuntimeError: Failed to import pydot. You must install pydot for `pydotprint` to work.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因是使用的pyparsing版本太高(我的是2.0.3),使用低版本的即可:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    pip install pyparsing==1.5.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我还安装了pydot2,不知有什么用:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    pip install pydot2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我之后做了一个实验,再把pyparsing升级到2.0.3,报了一个不一样的错:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;InvocationException: Program terminated with status: -1073741819. stderr follows: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将pyparsing再降级即可&amp;hellip;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Window7安装theano、anaconda、CUDA</title>
      <link>http://blog.songru.org/posts/machine-learning/theano%20cuda%20install/</link>
      <pubDate>Sat, 02 Jan 2016 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/machine-learning/theano%20cuda%20install/</guid>
      <description>

&lt;p&gt;最开始我使用的是anaconda3，但是一直没有成功，并且很多库对python2的支持更好，所以最好尝试使用anaconda2.
因为要使用mingw，但是新版的anaconda都没有自带mingw，所以下载了老版本anaconda1.9.2.&lt;/p&gt;

&lt;h1 id=&#34;1-安装cuda:10082a9e681b37c239307c871d1bb53d&#34;&gt;1.安装CUDA&lt;/h1&gt;

&lt;p&gt;我的gpu是GTX 750ti|，使用的是CUDA7，安装简单（已久忘了细节，传说中使用默认路径安装好些，笔记本上是GTX 950M，CUDA7.5）。安装好CUDA后，跑程序可能遇到cuda is installed,but unavailale的错误，这个我查的结果说是显卡驱动太低，然后我就更新为最新的显卡驱动！！
安装完之后在cmd下执行nvcc -V查看版本成功的话，则表示小成。
要的话，进到sample目录，用相应的vs打开解决方案，然后生成解决方案，中途可能会有无法打开”d3dx9.h”、”d3dx10.h”、”d3dx11.h”头文件，可以&lt;a href=&#34;http://www.microsoft.com/en-us/download/details.aspx?id=6812&#34;&gt;下载DXSDK_Jun10.exe&lt;/a&gt;，然后安装到默认目录下；再编译工程即可；然后到bin目录跑生成的程序（deviceQuery.exe还有一些图形程序等），没问题的话则大成了。&lt;br /&gt;
下面是一个详细的步骤（copy来的）:&lt;br /&gt;
　　　1. 查看本机配置，查看显卡类型是否支持NVIDIA GPU，选中计算机&amp;ndash;&amp;gt; 右键属性 &amp;ndash;&amp;gt; 设备管理器 &amp;ndash;&amp;gt; 显示适配器：NVIDIA GeForce GT 610，在&lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;这里&lt;/a&gt;可以查到相应显卡的compute capability；&lt;br /&gt;
　　　2. 在&lt;a href=&#34;http://www.nvidia.cn/Download/index.aspx?lang=cn&#34;&gt;这里&lt;/a&gt;下载合适驱动347.88-desktop-win8-win7-winvista-64bit-international-whql.exe 并安装；&lt;br /&gt;
　　　3. 从&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;https://developer.nvidia.com/cuda-toolkit&lt;/a&gt;   根据本机类型下载相应的最新版本CUDA7.0安装；&lt;br /&gt;
　　　4. 按照&lt;a href=&#34;http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-microsoft-windows/index.html#axzz3W8BU10Ol&#34;&gt;官方文档&lt;/a&gt;步骤，验证是否安装正确：&lt;br /&gt;
　　　　　(1) 打开C:\ProgramData\NVIDIACorporation\CUDA Samples\v7.0目录下的Samples_vs2010.sln工程，分别在Debug、Release x64下编译整个工程；&lt;br /&gt;
　　　　　(2) 编译过程中，会提示找不到”d3dx9.h”、”d3dx10.h”、”d3dx11.h”头文件，可以&lt;a href=&#34;http://www.microsoft.com/en-us/download/details.aspx?id=6812&#34;&gt;下载DXSDK_Jun10.exe&lt;/a&gt;，然后安装到默认目录下；再编译工程即可；&lt;br /&gt;
　　　　　(3) 打开C:\ProgramData\NVIDIACorporation\CUDA Samples\v7.0\bin\win64\Release目录，打开cmd命令行，将deviceQuery.exe直接拖到cmd中，回车，会显示GPU显卡、CUDA版本等相关信息，最后一行显示：Result = PASS；&lt;br /&gt;
　　　　　(4) 将bandwidthTest.exe拖到cmd中，回车，会显示Device0: GeForce GT 610等相关信息，后面也会有一行显示：Result = PASS；&lt;/p&gt;

&lt;h1 id=&#34;2-安装anaconda:10082a9e681b37c239307c871d1bb53d&#34;&gt;2.安装anaconda&lt;/h1&gt;

&lt;p&gt;开始使用的是anacond192版本，最后都配置成功了，但是这个版本的ipython notebook等等东西版本太低，我就用conda update了一下，导致一切都玩完了，theano又跑不起来了。所以后来就换成了anaconda2.4.1，是anaconda2的最新版本，但是没有mingw（这时我已经知道怎么配了，所以没有也不怕，我自己安）。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;安装anaconda很简单，傻瓜式的&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;装mingw，运行conda install mingw即可&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;装theano，因为conda库中没有theano，所以使用pip安装，执行pip install theano即可&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;配置.theanorc.txt&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在home目录下(我的是c:/uesrs/lisongru)创建.theanorc.txt,输入一下内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[blas]
ldflags =
[gcc]
cxxflags = -IE:\Anaconda2\MinGW    #安装的mingw目录
[nvcc]
fastmath = True
flags=-LE:\Anaconda2\libs
compiler-bindir=C:\Program Files (x86)\Microsoft Visual Studio 10.0\VC\bin   #vs的目录,不知是否可以不要,因为下面path也配了
flags =  -arch=sm_30   #这句没有也行,好像,未测试
base_compiledir=path_to_a_directory_without_such_characters   #这句没有也行,好像,未测试
[global]
openmp = False
floatX = float32
device = gpu   #cpu则使用cpu
allow_input_downcast=True
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;3-配置环境变量:10082a9e681b37c239307c871d1bb53d&#34;&gt;3.配置环境变量&lt;/h1&gt;

&lt;p&gt;这一步非常关键,开始我的电脑有cygwin，并且也在path中，然后运行import theano总是报错，原因是cygwin被用来编译了，但我们要用的是。。。（我也不清楚),后来删除它的path,加入vs_bin的path,就好了.
下面是一些相关的path&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;VS10_VC_BIN%        #vs的bin目录,和上面的compiler-bindir一样,这必须有,没有则包找不到cl.exe...
%CUDA_PATH%\bin     #安装cuda好像默认会有,没有自己加,有它才能在cmd中nvcc -V查看
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5\libnvvp;        #cuda安装自带
C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common      #自带
%ANACONDA2_HOME%;%ANACONDA2_SCRIPTS%;%ANACONDA2_BIN%        #anaconda自带
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;4-遇到的问题:10082a9e681b37c239307c871d1bb53d&#34;&gt;4.遇到的问题&lt;/h1&gt;

&lt;p&gt;在笔记本上安装好一切后，报collect2: ld returned 1 exit status错，查的结果说是python 32位与64位冲突，那时笔记本上正好有32位的一个python，就卸载了，但还是不行。
再查，说是少了libpython包,安装之:conda install libpython，然后就好了。。。（这里有点奇怪，在台式机上没有这个包也成功了）。&lt;br /&gt;
&amp;gt; Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux命令</title>
      <link>http://blog.songru.org/posts/linux/linuxComands/</link>
      <pubDate>Tue, 22 Sep 2015 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/linux/linuxComands/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;移动文件（夹）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mv originalDir/source.txt targetDir/target.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;删除文件（夹）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rm dir/target.file   //删除文件
rm -r dir //删除文件夹
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;利用软件源下载安装软件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install package 安装包
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;解压文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf XXX压缩包
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建文件夹&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir dir    // -p参数可以创建子文件夹
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;touch dir/a.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;解压文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf file.tar.gz    //解压到当前目录
tar -zxvf file.tar.gz -C dir    //解压到制定目录
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是zip格式的话：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;unzip  abc.zip -d /home/test/
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;复制文件（夹）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp target.file copy.file    //复制文件
cp -r targetDir copyDir //复制文件夹，使用-r递归复制
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;打印当前路径&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pwd
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;管理员权限&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo otherComand...     //使用root权限执行后续命令,一次性的
sudo su    //登录root,以后的命令都是使用root权限
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看文件系统使用情况信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    df -Tha    //T显示分区类型，h易读格式，a显示所有   
    fdisk -l /dev/sda    // 查看指定磁盘分区情况
    parted /dev/sda    // parted为分区工具，p可查看分区情况，quit退出
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看文件(夹)大小&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;du filename/dir    //-s易于阅读,-h显示总览
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看内存使用情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;free -m    //-m表示单位为m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;任务管理器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;top     //退出按 q
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看系统信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;uname -a    //-a查看所有信息
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看网络接口信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ifconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;MD5校验&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo md5sum filename
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看文件(夹)权限情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ls -l path/filename      //查看path路径下名为filename的文件或文件夹的权限
ls -ls path    //查看path路径下的所有文件的权限
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改文件(夹)权限(change mode)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo chmod -（代表类型）×××（所有者）×××（组用户）×××（其他用户）    //其中×××指文件名（也可以是文件夹名，不过要在chmod后加-ld）,使用 -R 参数来递归修改所有子文件   


//三位数的每一位都表示一个用户类型的权限设置。取值是0～7，即二进制的[000]~[111],这个三位的二进制数的每一位分别表示读、写、执行权限。      
//如000表示三项权限均无，而100表示只读。这样，我们就有了下面的对应：
0 [000] 无任何权限
4 [100] 只读权限
6 [110] 读写权限
7 [111] 读写执行权限


//常用的
sudo chmod 600 ××× （只有所有者有读和写的权限）
sudo chmod 644 ××× （所有者有读和写的权限，组用户只有读的权限）
sudo chmod 700 ××× （只有所有者有读和写以及执行的权限）
sudo chmod 666 ××× （每个人都有读和写的权限）
sudo chmod 777 ××× （每个人都有读和写以及执行的权限）
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改文件所有者(change owner)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo chown -R lsr software/    // -R 递归修改 software文件夹所有者为lsr
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;读取文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat a.txt   # 读取a.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;转换文件编码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;iconv -f gbk -t utf-8   # 将文件编码从gbk转到utf8
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查找行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep -E &amp;quot;&amp;lt;content&amp;gt;|&amp;lt;contenttitle&amp;gt;&amp;quot;  # 保留含有&amp;lt;content&amp;gt;或者&amp;lt;contenttitle&amp;gt;的行,-E表示使用正则
grep -v &amp;quot;hehe&amp;quot; # -v 排除匹配的行
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;删除文本中指定内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tr -d &amp;quot;&amp;lt;content&amp;gt;|&amp;lt;contenttitle&amp;gt;|&amp;lt;/content&amp;gt;|&amp;lt;/contenttitle&amp;gt;|&amp;gt;&amp;quot;   # -d表示删除,删除引号内的字符
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多个命令连续执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat a.txt | iconv -f gbk -t utf-8 | grep -E &amp;quot;&amp;lt;content&amp;gt;|&amp;lt;contenttitle&amp;gt;&amp;quot; | tr -d &amp;quot;&amp;lt;content&amp;gt;|&amp;lt;contenttitle&amp;gt;|&amp;lt;/content&amp;gt;|&amp;lt;/contenttitle&amp;gt;|&amp;gt;&amp;quot; &amp;gt; corpus.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查找文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;find SogouCS/ -name &amp;quot;m_*&amp;quot;   # 在SogouCS目录中按文件名-name查找含有 m_*的文件
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命令参数传递&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# xargs将文件名传递过来, -i 表示使用传过来的参数替换 {}
find SogouCS/ -name &amp;quot;m_*&amp;quot; | xargs -i mv {} SogouCS_M/
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;大小写转换&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tr A-Z a-z  # 全部转换为小写
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;文本替换&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 替换’为&#39;，′为&#39;，&#39;为&#39; , -e 表示多个编辑,即多个sed同时, s表示替换,g表示全局替换
sed -e &amp;quot;s/’/&#39;/g&amp;quot; -e &amp;quot;s/′/&#39;/g&amp;quot; -e &amp;quot;s/&#39;&#39;/ /g&amp;quot;


# 替换A-Za-z&#39;_ \n之外的字符为空格,-c 表示排除..之外的都..
tr -c &amp;quot;A-Za-z&#39;_ \n&amp;quot; &amp;quot; &amp;quot;


# 例子             输入文件                                                                    输出到文件
sed -e &amp;quot;s/’/&#39;/g&amp;quot; -e &amp;quot;s/′/&#39;/g&amp;quot; -e &amp;quot;s/&#39;&#39;/ /g&amp;quot; &amp;lt; news.2012.en.shuffled | tr -c &amp;quot;A-Za-z&#39;_ \n&amp;quot; &amp;quot; &amp;quot; &amp;gt; news.2012.en.shuffled-norm0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;从第3000行开始，显示1000行。即显示3000~3999行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat filename | tail -n +3000 | head -n 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;显示1000行到3000行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat filename| head -n 3000 | tail -n +1000
*注意两种方法的顺序
分解：
    tail -n 1000：显示最后1000行
    tail -n +1000：从1000行开始显示，显示1000行以后的
    tail -n -1000：相当于tail -n 1000
    head -n 1000：显示前面1000行
    head -n +1000：显示前面1000行
    head -n -1000：从第1行到倒数1000行
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;用sed命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -n &#39;5,10p&#39; filename # 查看文件的第5行到第10行, p表示打印匹配行
sed -n &#39;5p&#39; filename # 只显示第5行
sed -n &#39;5,$p&#39; filename # 查看文件的第5行到最后一行, $表示最后一行
sed -n &#39;3,/movie/&#39;p temp.txt   # 只在第3行查找movie并打印
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Linux统计文件行数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wc -l file # - c 统计字节数, - l 统计行数, - w 统计字数。


1.统计demo目录下，js文件数量：
find demo/ -name &amp;quot;*.js&amp;quot; | wc -l


2.统计demo目录下所有js文件代码行数：
find demo/ -name &amp;quot;*.js&amp;quot; | xargs cat | wc -l 或 
wc -l `find ./ -name &amp;quot;*.js&amp;quot;` | tail -n 1 # 取最后一行,总行数


3.统计demo目录下所有js文件代码行数，过滤了空行：
find /demo -name &amp;quot;*.js&amp;quot; | xargs cat | grep -v ^$ | wc -l
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;打印进程信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ps aux
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ssh连接主机,并执行命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh username@ip &#39;command&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;shell中的一种for循环&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for((i=1;i&amp;lt;=10;i++))
do
    echo compute-0-$i
    ssh compute-0-$i &amp;quot;nvidia-smi | grep &#39;%&#39; &amp;quot;
 done
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;awk中的while循环&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;awk &#39;BEGIN{s=&amp;quot;&amp;quot;;i=11}{while(i&amp;lt;=NF){ s=s&amp;quot; &amp;quot;&amp;quot;&amp;quot;$i;i++; } print s;}&#39; # 提取出command &amp;quot;&amp;quot;用于拼接字符串, NF表示总的字段数
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;半角字符与全角字符之间的unicode码相差65248,而全角空格为12288,半角为32,需特殊对待.&lt;/p&gt;

&lt;p&gt;java代码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 全角转半角
 * @param str
 * @return
 */
public static String FullWidth2HalfWidth(String str){
    if (null == str || str.length() &amp;lt;= 0) {
        return &amp;quot;&amp;quot;;
    }
    char []cs;
    cs = str.toCharArray();
    int k;
    for(int i = 0; i &amp;lt; cs.length; i++){
        k = (int)cs[i];
        if(k &amp;gt;= 65281 &amp;amp;&amp;amp; k &amp;lt;= 65374){
            cs[i] -= 65248;
        }else if(k == 12288 || k == 58380){
            cs[i] = 32;
        }
    }
    return new String(cs);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写的bat代码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@echo off
echo Deploying updates to GitHub...

:: Build the project.
hugo -t red

:: Add changes to git.
git add -A

:: Commit changes.
set msg=rebuilding site %date% %time%
if not &amp;quot;%1&amp;quot;==&amp;quot;&amp;quot; (
  set msg=%1
)
git commit -m &amp;quot;%msg&amp;quot;

:: Push source and build repos.
git push origin hugo
git subtree push --prefix=public origin master

pause
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对应的shell代码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
echo -e &amp;quot;\033[0;32mDeploying updates to GitHub...\033[0m&amp;quot;

# Build the project.
hugo -t red

# Add changes to git.
git add -A

# Commit changes.
msg=&amp;quot;rebuilding site `date`&amp;quot;
if [ $# -eq 1 ]
  then msg=&amp;quot;$1&amp;quot;
fi
git commit -m &amp;quot;$msg&amp;quot;

# Push source and build repos.
git push origin hugo
git subtree push --prefix=public origin master
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>【自然语言处理之三】最小编辑距离（Minimum Edit Distance）</title>
      <link>http://blog.songru.org/posts/nlp/nlp_3_MinimumEditDistance/</link>
      <pubDate>Sun, 13 Sep 2015 21:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/nlp/nlp_3_MinimumEditDistance/</guid>
      <description>

&lt;h1 id=&#34;一-最小编辑距离:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;一、最小编辑距离&lt;/h1&gt;

&lt;h2 id=&#34;1-1-定义:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.1 定义&lt;/h2&gt;

&lt;p&gt;最小编辑距离（Minimum Edit Distance，MED），又称Levenshtein距离，是指两个字符串之间，由一个转成另一个所需要的最少编辑操作次数。允许的编辑操作包括：将一个字符替换成另一个字符（substitution，s），插入一个字符（insert，i）或者删除一个字符（delete，d），如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_1.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;1-2-求解:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.2 求解&lt;/h2&gt;

&lt;p&gt;在大学算法设计相关课程上，想必大家都已经学习过使用动态规划算法解最小编辑距离，形式化定义如下：
对于两个字符串X={x1,x2,x3&amp;hellip;,xn}和Y={y1,y2,y3,&amp;hellip;,ym}，x的长度是n，y的长度是m，则定义D(i,j)为子字符串X{x1,x2,&amp;hellip;,xi}于Y{y1,y2,&amp;hellip;,yj}间的最小编辑距离。我们的目标是求出D(n,m)。
则可以推出如下关系:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_3.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;根据这个递推关系，我们计算所有的i和j的取值填入一个矩阵中：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_4.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;最终可以求得右上角的D(n,m)。&lt;/p&gt;

&lt;h2 id=&#34;1-3-回溯法对齐字符串:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.3 回溯法对齐字符串&lt;/h2&gt;

&lt;p&gt;通过上面的动态规划方法虽然可以求出来两个字符串之间的最小编辑距离，但是我们并不知道源字符串被编辑的情况，即哪些字符被删除，增加或者替换了。
我们只要能够将源字符串与目标字符串相应字符对齐，即可以得出具体的编辑情况。这里只需要对上面的动态规划方法稍加改进即可实现。
我们只需要记住当前位置是从之前哪一个位置计算而来即可，然后从D(n,m)逐个向前回溯就能够将两个字符串对齐，如图所示：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_5.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;其递推公式为：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_6.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;1-4-加权最小编辑距离:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.4 加权最小编辑距离&lt;/h2&gt;

&lt;p&gt;在基本的编辑距离基础上，结合实际应用，往往需要做一些变形或改进，如某些拼写错误相对其他更容易发生，同义词替换、虚词或修饰成分被删除（或插入）应该得到较小的惩罚，等等。
下图是一个统计好的拼写错误的转移矩阵：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_8.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;加权递推公式如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_9.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;1-5-最小编辑距离的应用:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.5 最小编辑距离的应用&lt;/h2&gt;

&lt;p&gt;最小编辑距离通常作为一种相似度计算函数被用于多种实际应用中，详细如下： （特别的，对于中文自然语言处理，一般以词为基本处理单元）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;拼写纠错（Spell Correction）：又拼写检查（Spell Checker），将每个词与词典中的词条比较，英文单词往往需要做词干提取等规范化处理，如果一个词在词典中不存在，就被认为是一个错误，然后试图提示N个最可能要输入的词——拼写建议。常用的提示单词的算法就是列出词典中与原词具有最小编辑距离的词条。&lt;br /&gt;
这里肯定有人有疑问：对每个不在词典中的词（假如长度为len）都与词典中的词条计算最小编辑距离，时间复杂度是不是太高了？的确，所以一般需要加一些剪枝策略，如：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;因为一般拼写检查应用只需要给出Top-N的纠正建议即可（N一般取10），那么我们可以从词典中按照长度依次为len、len-1、len+1、len-2、len-3、&amp;hellip;的词条比较；&lt;/li&gt;
&lt;li&gt;限定拼写建议词条与当前词条的最小编辑距离不能大于某个阈值；&lt;/li&gt;
&lt;li&gt;如果最小编辑距离为1的候选词条超过N后，终止处理；&lt;/li&gt;
&lt;li&gt;缓存常见的拼写错误和建议，提高性能。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;DNA分析：基因学的一个主要主题就是比较 DNA 序列并尝试找出两个序列的公共部分。如果两个 DNA 序列有类似的公共子序列，那么这些两个序列很可能是同源的。在比对两个序列时，不仅要考虑完全匹配的字符，还要考虑一个序列中的空格或间隙（或者，相反地，要考虑另一个序列中的插入部分）和不匹配，这两个方面都可能意味着突变（mutation）。在序列比对中，需要找到最优的比对（最优比对大致是指要将匹配的数量最大化，将空格和不匹配的数量最小化）。如果要更正式些，可以确定一个分数，为匹配的字符添加分数、为空格和不匹配的字符减去分数。&lt;br /&gt;
全局序列比对尝试找到两个完整的序列 S1和 S2之间的最佳比对。以下面两个 DNA 序列为例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S1= GCCCTAGCG
S2= GCGCAATG
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果为每个匹配字符一分，一个空格扣两分，一个不匹配字符扣一分，那么下面的比对就是全局最优比对：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S1&#39;= GCCCTAGCG
S2&#39;= GCGC-AATG
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;连字符（-）代表空格。在 S2&amp;rsquo;中有五个匹配字符，一个空格（或者反过来说，在 S1&amp;rsquo;中有一个插入项），有三个不匹配字符。这样得到的分数是 (5 * 1) + (1 * -2) + (3 * -1) = 0，这是能够实现的最佳结果。&lt;br /&gt;
使用局部序列比对，不必对两个完整的序列进行比对，可以在每个序列中使用某些部分来获得最大得分。使用同样的序列 S1和 S2，以及同样的得分方案，可以得到以下局部最优比对 S1&amp;rdquo;和 S2&amp;rdquo;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S1      = GCCCTAGCG
S1&#39;&#39;=           GCG
S2&#39;&#39;=           GCG
S2      =       GCGCAATG
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个局部比对的得分是 (3 * 1) + (0 * -2) + (0 * -1) = 3。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命名实体抽取（Named Entity Extraction）：由于实体的命名往往没有规律，如品牌名，且可能存在多种变形、拼写形式，如“IBM”和“IBM Inc.”，这样导致基于词典完全匹配的命名实体识别方法召回率较低，为此，我们可以使用编辑距离由完全匹配泛化到模糊匹配，先抽取实体名候选词。&lt;br /&gt;
具体的，可以将候选文本串与词典中的每个实体名进行编辑距离计算，当发现文本中的某一字符串的编辑距离值小于给定阈值时，将其作为实体名候选词；获取实体名候选词后，根据所处上下文使用启发式规则或者分类的方法判定候选词是否的确为实体名。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;实体共指（Entity Coreference）：通过计算任意两个实体名之间的最小编辑距离判定是否存在共指关系？如“IBM”和“IBM Inc.”, &amp;ldquo;Stanford President John Hennessy &amp;ldquo;和&amp;rdquo;Stanford University President John Hennessy&amp;rdquo;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;机器翻译（Machine Translation）：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;识别平行网页对：由于平行网页通常有相同或类似的界面结构，因此平行网页在HTML结构上应该有很大近似度。首先将网页的HTML标签抽取出来，连接成一个字符串，然后用最小编辑距离考察两个字符串的近似度。实际中，此策略一般与文档长度比例、句对齐翻译模型等方法结合使用，以识别最终的平行网页对。&lt;/li&gt;
&lt;li&gt;自动评测：首先存储机器翻译原文和不同质量级别的多个参考译文，评测时把自动翻译的译文对应到与其编辑距离最小的参考译文上，间接估算自动译文的质量，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_10.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;字符串核函数（String Kernel）：最小编辑距离作为字符串之间的相似度计算函数，用作核函数，集成在SVM中使用。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;二-参考资料:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;二、参考资料&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Lecture Slides：&lt;a href=&#34;http://spark-public.s3.amazonaws.com/nlp/slides/med.pptx&#34;&gt;Minimum Edit Distance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org&#34;&gt;http://en.wikipedia.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.google.com.hk/url?sa=t&amp;amp;rct=j&amp;amp;q=%E6%9C%80%E5%B0%8F%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB+%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91&amp;amp;source=web&amp;amp;cd=7&amp;amp;ved=0CG0QFjAG&amp;amp;url=http%3A%2F%2Fwww.ecice06.com%2FCN%2Farticle%2FdownloadArticleFile.do%3FattachType%3DPDF%26id%3D10174&amp;amp;ei=GNinT4iaKKaViQfDpbnKAw&amp;amp;usg=AFQjCNGbWIzPc5GpPghvdYQAnbQ8iIPMIw&#34;&gt;双语平行网页挖掘系统的设计与实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wenku.baidu.com/view/a50949dc7f1922791688e81d.html&#34;&gt;机器翻译系统评测规范&lt;/a&gt; .&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wenku.baidu.com/view/a50949dc7f1922791688e81d.html&#34;&gt;matrix67-漫话中文分词算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ir.hit.edu.cn/ir_papers/Vol_3/Improved-Edit-Distance%20Kernel%20for%20Chinese%20Relation%20Extraction.pdf&#34;&gt;Improved-Edit-Distance Kernel for Chinese Relation Extraction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>【自然语言处理之二】文本处理基础（Basic Text Processing）</title>
      <link>http://blog.songru.org/posts/nlp/nlp_2_BasicTextProcessing/</link>
      <pubDate>Fri, 11 Sep 2015 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/nlp/nlp_2_BasicTextProcessing/</guid>
      <description>

&lt;h1 id=&#34;一-文本处理基础:2e62519647c0e571a1761fc424fb31ad&#34;&gt;一、文本处理基础&lt;/h1&gt;

&lt;h2 id=&#34;1-1-正则表达式:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.1 正则表达式&lt;/h2&gt;

&lt;p&gt;自然语言处理过程中面临大量的文本处理工作，如词干提取、网页正文抽取、分词、断句、文本过滤、模式匹配等任务，而正则表达式往往是首选的文本预处理工具。
现在主流的编程语言对正则表达式都有较好的支持，如Grep、Awk、Sed、Python、Perl、Java、C/C++(推荐re2)等。
&lt;em&gt;注：课程中给出的正则表达式语法和示例在此略去&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-2-分词:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.2 分词&lt;/h2&gt;

&lt;p&gt;分词的操作就是将一句话中的词语全部切分开来。&lt;/p&gt;

&lt;h3 id=&#34;1-2-1-词典规模:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.2.1 词典规模&lt;/h3&gt;

&lt;p&gt;同一词条可能存在不同的时态、变形，那么给定文本语料库，如何确定词典规模呢？
首先定义两个变量Type和Token：
    &lt;strong&gt;Type&lt;/strong&gt;：词典中的元素，即独立词条
    &lt;strong&gt;Token&lt;/strong&gt;：词典中独立词条在文本中的每次出现
如果定义 N = number of tokens 和 V = vocabulary = set of types，|V| is the size of the vocabulary，那么根据Church and Gale (1990)的研究工作可知: |V| &amp;gt; O(N½) ，验证如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_2_1.jpg&#34; alt=&#34;词典规模&#34; /&gt;
&lt;/p&gt;

&lt;h3 id=&#34;1-2-2-分词算法:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.2.2 分词算法&lt;/h3&gt;

&lt;p&gt;任务：统计给定文本文件（如shake.txt）中词频分布，子任务：分词，频率统计，实现如下：&lt;/p&gt;

&lt;p&gt;英文分词非常简单，因为英文中每个单词之间（少数特殊写法额外考虑，We&amp;rsquo;re,isn&amp;rsquo;t&amp;hellip;)都由空格分割开来，所以分词只需要按空格将它们切分，然后对一些特殊的写法进一步处理即可。&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_2_2.jpg&#34; alt=&#34;分词算法&#34; /&gt;
&lt;br /&gt;
以上实现将非字母字符作为token分隔符作为简单的分词器实现，但是，这难免存在许多不合理之处，如下面的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    - Finland’s capital  -&amp;gt;  Finland Finlands Finland’s  ?
    - what’re, I’m, isn’t  -&amp;gt;  What are, I am, is not
    - Hewlett-Packard   -&amp;gt;  Hewlett Packard ?
    - state-of-the-art     -&amp;gt;   state of the art ?
    - Lowercase  -&amp;gt;  lower-case lowercase lower case ?
    - San Francisco -&amp;gt;   one token or two?
    - m.p.h., PhD.  -&amp;gt;  ??
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的方法对英语这种包含固定分隔符的语言行之有效，对于汉语、日语、德语等文本则不再适用，所以就需要专门的分词技术。其中，最简单、最常用，甚至是最有效的方法就是最大匹配法（Maximum Matching），它是一种基于贪心思想的切词策略，主要包括正向最大匹配法（Forward Maximum Matching，FMM）、逆向最大匹配法（Backward Maximum Matching）与双向最大匹配法（Bi-direction Maximum Matching，BMM）。&lt;/p&gt;

&lt;p&gt;以FMM中文分词为例，步骤如下：
1.  选取包含N(N通常取6~8)个汉字的字符串作为最大字符串；
2. 把最大字符串与词典中的单词条目相匹配（词典通常使用Double Array Trie组织）；
3. 如果不能匹配，就去除最后一个汉字继续匹配，直到在词典中找到相应的词条为止。
例如：句子“莎拉波娃现在居住在美国东南部的佛罗里达“的分词结果是”莎拉波娃/   现在/   居住/   在/  美国/   东南部/     的/  佛罗里达/”。
另外，使用较多的分词方法有最少分词法、最短路径法、最大概率法（或词网格法，大规模语料库+HMM/HHMM）、字标注法等。&lt;/p&gt;

&lt;h3 id=&#34;1-2-3-分词难点:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.2.3 分词难点&lt;/h3&gt;

&lt;p&gt;-切分歧义：主要包括交集型歧义和覆盖型歧义，在汉语书面文本中占比并不大，而且一般都可以通过规则或建立词表解决。&lt;/p&gt;

&lt;p&gt;-未登录词：就是未在词典中记录的人名（中、外）、地名、机构名、新词、缩略语等，构成了中文自然语言处理永恒的难点。常见的解决方法有互信息、语言模型，以及基于最大熵或隐马尔科夫模型的统计分类方法。&lt;/p&gt;

&lt;h2 id=&#34;1-3-文本归一化-标准化:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.3 文本归一化(标准化)&lt;/h2&gt;

&lt;p&gt;文本归一化主要包括大小写转换、词干提取、繁简转换等问题。&lt;/p&gt;

&lt;h2 id=&#34;1-4-断句:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.4 断句&lt;/h2&gt;

&lt;p&gt;句子一般分为大句和小句，大句一般由“！”，“。”，“；”，“\“”、“？”等分割，可以表达完整的含义，小句一般由“，”分割，起停顿作用，需要上下文搭配表达特定的语义。
中文断句通常使用正则表达式将文本按照有分割意义的标点符号(如句号)分开即可，而对于英文文本，由于英文句号”.“在多种场景下被使用，如缩写“Inc.”、“Dr.”、“.02%”、“4.3”等，无法通过简单的正则表达式处理，为了识别英文句子边界，课程中给出了一种基于决策树（Decision Tree）的分类方法，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_2_3.jpg&#34; alt=&#34;断句&#34; /&gt;
&lt;br /&gt;
此方法的核心就是如何选取有效的特征？如句号前后的单词是否大写开头、是否为缩略词、前后是否存在数字、句号前的单词长度、句号前后的单词在语料库中作为句子边界的概率等等。当然，你也可以基于上述特征采用其他分类器解决断句问题，如罗辑回归（Logistic regression）、支持向量机（Support Vector Machine）、神经网络（Neural Nets）等。&lt;/p&gt;

&lt;h1 id=&#34;二-参考资料:2e62519647c0e571a1761fc424fb31ad&#34;&gt;二、参考资料&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Lecture Slides：&lt;a href=&#34;http://spark-public.s3.amazonaws.com/nlp/slides/textprocessingboth.pptx&#34;&gt;Basic Text Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org&#34;&gt;http://en.wikipedia.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;关毅，统计自然语言处理基础 课程PPT&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=4&amp;amp;ved=0CHAQFjAD&amp;amp;url=http%3A%2F%2Fwww2.denizyuret.com%2Fref%2Fchurch%2Fpublished_1990_darpa.ps.gz&amp;amp;ei=guWlT4rCFOWZ2QW3mbymAg&amp;amp;usg=AFQjCNFcEeYyaP8TQUYJxNVUkdoHZl98hg&amp;amp;sig2=17cCPZhMQzpWCHeWm-knag&#34;&gt;Gale, W. A. and K. W. Church (1990) “Estimation Procedures for Language Context: Poor Estimates are Worse than None,” Proceedings in Computational Statistics, 1990, p.69-74, Physica-Verlag, Heidelberg&lt;/a&gt; .&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.52nlp.cn/matrix67-%E6%BC%AB%E8%AF%9D%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95&#34;&gt;matrix67-漫话中文分词算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%951&#34;&gt;中文分词入门之字标注法&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>