<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Leon&#39;s Blog</title>
    <link>http://blog.songru.org/posts/</link>
    <description>Recent content in Posts on Leon&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Sat, 18 Mar 2017 09:40:10 +0800</lastBuildDate>
    <atom:link href="http://blog.songru.org/posts/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>放松地方</title>
      <link>http://blog.songru.org/posts/linux/%E6%94%BE%E6%9D%BE%E5%9C%B0%E6%96%B9/</link>
      <pubDate>Sat, 18 Mar 2017 09:40:10 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/linux/%E6%94%BE%E6%9D%BE%E5%9C%B0%E6%96%B9/</guid>
      <description>&lt;pre&gt;&lt;code&gt;yum install gcc kernel-devel make ncurses-devel
yum install git cmake ruby zlib-devel openssl-devel libevent-devel ncurses-devel
yum group install &amp;quot;Development Tools&amp;quot;
yum install libssh libssh-devel

# Install msgpack
yum install msgpack msgpack-devel python-msgpack
# If not work, then install with source
git clone https://github.com/msgpack/msgpack-c.git
cd msgpack-c
cmake .
make
sudo make install

# Install tmate
./autogen.sh &amp;amp;&amp;amp; \
./configure  &amp;amp;&amp;amp; \
make         &amp;amp;&amp;amp; \
make install

# If not work, install libevent2 with source
wget https://github.com/downloads/libevent/libevent/libevent-2.0.21-stable.tar.gz
# cd to libevent2 src
./configure --prefix=/usr/local
make &amp;amp;&amp;amp; make install

# Install tmate-slave
git clone https://github.com/tmate-io/tmate-slave.git &amp;amp;&amp;amp; cd tmate-slave
./create_keys.sh # This will generate SSH keys, remember the keys fingerprints.
./autogen.sh &amp;amp;&amp;amp; ./configure &amp;amp;&amp;amp; make

# Start
sudo ./tmate-slave -p 222
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Ubuntu 14.04 笔记本双显卡cuda 安装</title>
      <link>http://blog.songru.org/posts/linux/NoteBook-Ubuntu-Install/</link>
      <pubDate>Sat, 18 Mar 2017 09:36:53 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/linux/NoteBook-Ubuntu-Install/</guid>
      <description>

&lt;h1 id=&#34;ubuntu-14-04-笔记本双显卡cuda-安装:b5033d739bcdb1d7b241dd7a0f168519&#34;&gt;Ubuntu 14.04 笔记本双显卡cuda 安装&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;安装ubuntu系统&lt;/li&gt;
&lt;li&gt;在系统设置 -&amp;gt;  软件和更新 -&amp;gt; 附加驱动 中如果有nvidia驱动， 则表示该系统支持cuda， 可以继续后面步骤。&lt;/li&gt;
&lt;li&gt;下载新版的NVIDIA显卡驱动， CUDA的run file里面的驱动或者是deb中的都太老了, 不适合新版tensorflow。&lt;/li&gt;
&lt;li&gt;安装新版的NVIDIA驱动：&lt;br /&gt;
&lt;code&gt;sh NVIDIA-Linux-x86_64-375.26.run -no-x-check -no-nouveau-check -no-opengl-files&lt;/code&gt;
–no-x-check 安装驱动时关闭X服务
–no-nouveau-check 安装驱动时禁用nouveau
–no-opengl-files 只安装驱动文件，不安装OpenGL文件&lt;/li&gt;
&lt;li&gt;通过run文件安装cuda，选择不安装显卡驱动&lt;/li&gt;
&lt;li&gt;成功&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Sequence to Sequence Learning with Neural Networks</title>
      <link>http://blog.songru.org/posts/notebook/Sequence-to-Sequence-Learning-with-Neural-Networks/</link>
      <pubDate>Fri, 03 Jun 2016 14:55:16 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/notebook/Sequence-to-Sequence-Learning-with-Neural-Networks/</guid>
      <description>&lt;p&gt;该论文使用Encoder-Decoder模型, 进行end-to-end的训练来进行机器翻译. 该论文与&lt;a href=&#34;http://blog.songru.org/posts/notebook/Neural_Machine_Translation_By_Jointly_Learning_To_Align_And_Translate_NOTE/&#34;&gt;Neural Machine Translation By Jointly Learning To Align And Translate&lt;/a&gt;的主要区别在于:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;该论文中不是使用Bidirectional RNN, 而是使用了multi-layer RNN, 发现比shallow RNN效果好(可能因为deep结构包含更多的隐藏状态).&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;该论文将输入序列进行反向, 再一次输入到Encoder的RNN中.&lt;br /&gt;
正常顺序输入的Encoder序列与Decoder序列之间有一个比较大的&amp;rdquo;minimal time lag&amp;rdquo;, 将输入序列方向之后, 虽然输入序列与输出序列对应词语之间的平均距离没有改变, 但是输入序列最前面的一些词语与输出序列的对应词语更加近了, 也就是说&amp;rdquo;minimal time lag&amp;rdquo;减小了.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;该论文没有使用attention.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;该论文在输出翻译句子时使用了&lt;a href=&#34;https://en.wikipedia.org/wiki/Beam_search&#34;&gt;beam search&lt;/a&gt;方法, 而不是传统的greedy search方法.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;下面介绍一下Sequence to Sequence Learning当中的&lt;strong&gt;beam search&lt;/strong&gt;:&lt;br /&gt;
在训练好模型之后, 预测阶段我们需要逐个词语的进行预测, 通常的做法(greedy search)是从Decoder的第一个时刻开始选取概率最大的词作为下一个时刻的输入, 这样依次预测得到最终的结果. 但是这里存在一个问题, 就是&lt;strong&gt;最可能的预测结果序列可能并不是从选取的最可能的那个词语开始的.&lt;/strong&gt;为了找到概率最大的预测结果, 可以简单的采用列举所有可能的输出序列, 然后选取最大概率的一个, 但是计算的复杂度与句子长度呈指数级增长, 效率太低.&lt;/p&gt;

&lt;p&gt;Beam search的思想是首先指定一个数$b$, 称$b$为beam size或beam width. 接下来要做的不是找到最有可能的第一个词, 而是第一个词中最可能的前$b$个(这$b$个候选词就成为beam); 接下来由第一个词预测第二个词, 依次使用选出的$b$个候选词进行预测, 并计算所有这些长度为2的序列的概率(一共$b*n$个这样的序列, $n$为词典大小), 从中选出概率最大的$b$个. 接下来使用前面选定的b个最大概率序列(长度为2)来计算概率最大的长度为3的前$n$个序列, 以此类推.&lt;br /&gt;
Beam search的计算量是greedy search的$b$倍, $b$一般不会取太大(2-5貌似).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;So the RNN estimates the joint distribution:&lt;br /&gt;
$p(X_1, X_2, X_3, \cdots, X_N)$ over a set of random variables&lt;br /&gt;
What we really want is the mode of this distribution, that is the point with the highest probability. One way to get this is through sampling from the joint distribution and taking the highest probability sample, but this is slow.&lt;br /&gt;
The RNN factors the joint distribution for 3 random variables in this way:&lt;br /&gt;
$$p(x_1, x_2, x_3) = p(x_3 \mid x_1,x_2) * p(x_2 \mid x_1) * p(x_1)$$&lt;br /&gt;
Beam search uses a heuristic that assumes that chains of random variables with high probability have fairly high probability conditionals. Basically you take the k highest probability solutions for $p(x_1)$, then for each of those take the k highest probability solutions for $p(x_2 \mid x_1)$. You then take the k of those with the highest value for $p(x_2 \mid x_1) * p(x_1)$ and repeat.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Opinion Mining with Deep Recurrent Neural Networks笔记</title>
      <link>http://blog.songru.org/posts/notebook/Opinion_Mining_with_Deep_Recurrent_Neural_Networks_NOTE/</link>
      <pubDate>Tue, 24 May 2016 21:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/notebook/Opinion_Mining_with_Deep_Recurrent_Neural_Networks_NOTE/</guid>
      <description>&lt;p&gt;本文对传统的RNN进行改进，结合BidirectionalRNN，提出了Deep bidirectional RNN。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.songru.org/img/1464096878586.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;传统的RNN&lt;/strong&gt;（图a）信息传播方向是从前往后一个方向，某个时刻t的隐藏状态仅仅包含它之前词语的语义信息：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464097111614.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;br /&gt;
$f$为非线性激活函数（比如sigmoid），$g$为输出的计算函数（比如softmax）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bidirectional RNN&lt;/strong&gt;（图b）包含前向和后向RNN两个部分，分别从相反的反向进行信息的传播，再将每个时刻的两个方向的隐藏状态联合起来计算输出值：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464097064201.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;可以看到，前向RNN和反向RNN的参数是互相独立的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep  RNN&lt;/strong&gt;（图c）是将多个传统RNN进行叠加而来，即每一层计算隐藏状态的输入都为上一层的输出。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep Bidirectional  RNN&lt;/strong&gt;（图d）则是将Bidirectional RNN与Deep RNN结合起来：&lt;br /&gt;
第$i$层（$i&amp;gt;1$）第$t$个时刻的前向隐藏状态$\overrightarrow{{h_t}^{(i)}}$依赖于三个输入：第$i-1$层$t$时刻的前向隐藏状态$\overrightarrow{{h_t}^{(i-1)}}$和后向隐藏状态$\overleftarrow{{h_t}^{(i-1)}}$以及第$i$层$t-1$时刻的前向隐藏状态.&lt;br /&gt;
第$i$层（$i&amp;gt;1$）第$t$个时刻的后向隐藏状态$\overleftarrow{{h_t}^{(i)}}$的计算同理:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464098995066.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;第1层隐藏状态机算比较特殊,也比较简单:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464099041075.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;输出的计算有两种选择:一是使用所有时刻的隐藏状态计算输出,或者仅使用最后一个时刻的隐藏状态.这里使用第二种方案:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464097751597.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;该文是通过堆叠（stack）RNN的方式达到Deep RNN的结构，&lt;a href=&#34;http://arxiv.org/abs/1312.6026&#34;&gt;Pascanu等&lt;/a&gt;的论文采取了另一种思路进行Deep RNN的扩展。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ubuntu下matplotlib绘图中文乱码</title>
      <link>http://blog.songru.org/posts/linux/Ubuntu_Matplotlib_fix/</link>
      <pubDate>Sat, 23 Apr 2016 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/linux/Ubuntu_Matplotlib_fix/</guid>
      <description>

&lt;p&gt;原因：在Ubuntu下安装了各种中文字体，但是修改matplotlibrc文件后，均提示找不到该字体，猜测可能matplotlib字体列表与系统字体列表不同。&lt;/p&gt;

&lt;h2 id=&#34;方法一-持久性修改:c4a9591d3c6525e63df3a6e304bb9db6&#34;&gt;方法一（持久性修改）&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;首先查看matplotlib支持的中文字体&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# -*- coding: utf-8 -*-
from matplotlib.font_manager import FontManager
import subprocess

fm = FontManager()
mat_fonts = set(f.name for f in fm.ttflist)

output = subprocess.check_output(
    &#39;fc-list :lang=zh -f &amp;quot;%{family}\n&amp;quot;&#39;, shell=True)
# print &#39;*&#39; * 10, &#39;系统可用的中文字体&#39;, &#39;*&#39; * 10
# print output
zh_fonts = set(f.split(&#39;,&#39;, 1)[0] for f in output.split(&#39;\n&#39;))
available = mat_fonts &amp;amp; zh_fonts

print &#39;*&#39; * 10, &#39;可用的字体&#39;, &#39;*&#39; * 10
for f in available:
    print f
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出为：
Droid Sans Fallback
YaHei Consolas Hybrid
就是求出系统字体列表与matplotlib字体列表的交集&lt;/p&gt;

&lt;p&gt;2.修改matplotlibrc文件&lt;br /&gt;
Ubuntu默认对应的是/etc/matplotlibrc，可以复制到～/.matplotlibrc/matplotlibrc，然后配置后者即可
修改&lt;strong&gt;font.sans-serif&lt;/strong&gt;为上面的一个输出结果即可, 还需要修改&lt;strong&gt;axes.unicode_minus&lt;/strong&gt;为&lt;strong&gt;False&lt;/strong&gt;,否则图像是负号&amp;rsquo;-&amp;lsquo;会显示为方块.&lt;/p&gt;

&lt;h2 id=&#34;方法二-临时性修改:c4a9591d3c6525e63df3a6e304bb9db6&#34;&gt;方法二（临时性修改）&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# -*- coding: utf-8 -*-
 import matplotlib as mpl
 import matplotlib.pyplot as plt
 
 mpl.rcParams[&#39;font.sans-serif&#39;] = [&#39;Droid Sans Fallback&#39;] # 指定字体名字
 mpl.rcParams[&#39;axes.unicode_minus&#39;] = False #解决保存图像是负号&#39;-&#39;显示为方块的问题
 plt.figure()
 plt.xlabel(u&#39;性别&#39;)
 plt.ylabel(u&#39;人数&#39;)
 plt.xticks((0,1),(u&#39;男&#39;,u&#39;女&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# -*- coding: utf-8 -*-
 import matplotlib.pyplot as plt
 from matplotlib import font_manager

 zh_font = font_manager.FontProperties(fname=r&#39;/home/lsr/Documents/simsun.ttf&#39;, size=14) # 指定字体文件
 
 plt.figure()
 plt.xlabel(u&#39;性别&#39;, fontproperties=zh_font) # 使用字体配置
 plt.ylabel(u&#39;人数&#39;,fontproperties=zh_font)
 plt.xticks((0,1),(u&#39;男&#39;,u&#39;女&#39;)) # 没有使用字体配置，乱码
 plt.bar(left = (0,1),height = (1,0.5),width = 0.35)
 plt.show()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Neural Machine Translation By Jointly Learning To Align And Translate笔记</title>
      <link>http://blog.songru.org/posts/notebook/Neural_Machine_Translation_By_Jointly_Learning_To_Align_And_Translate_NOTE/</link>
      <pubDate>Sun, 27 Mar 2016 22:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/notebook/Neural_Machine_Translation_By_Jointly_Learning_To_Align_And_Translate_NOTE/</guid>
      <description>&lt;p&gt;本文主要创新在传统的神经机器翻译上进行改进，确切的说是改进了基本的RNN Encoder-Decoder模型,提出了Alignment model，即实现了Attention model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;传统的encoder-decoder模型如下图所示:
&lt;img src=&#34;http://blog.songru.org/img/1458651310553.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
该模型通过神经网络将所有的输入信息压缩为一个固定长度的向量$w$,然后通过decoder的神经网络解码这个$w$，最后得出翻译结果.&lt;br /&gt;
使用固定长度的向量是该模型的一个缺点,因为该向量很难将所有需要的信息编码到其中，对于长度大的句子,该模型的效果会显著下降.&lt;/p&gt;

&lt;p&gt;本文作者提出的改进模型结构为:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458651809449.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
主要有4个方面改进:&lt;br /&gt;
1. &lt;strong&gt;GRU 和 Bidirectional RNN&lt;/strong&gt;&lt;br /&gt;
RNN网络的结果采用的是GRU单元，双向神经网络(Bidirectional).一个BiRNN由前向(forward)和后向(backward)RNN组成,前向RNN &amp;amp;(\stackrel{\rightarrow}{\mbox{f}})&amp;amp; 按顺序读取输入序列(从&amp;amp;(x_f)&amp;amp;到&amp;amp;(x_{T_x})&amp;amp;)，计算得出前向RNN的隐含状态序列&amp;amp;((\vec{h_1},\cdots,\vec{h_{T_x}} ))&amp;amp; 。反向RNN$\stackrel{\leftarrow}{\mbox{f}}$则逆序读取输入序列(从$x_{T_x}$到$x_1$)，计算得出$(\stackrel{\leftarrow}{h_1},\cdots,\stackrel{\leftarrow}{h_{T_x}})$ .&lt;br /&gt;
对于一个词$x_j$直接concatenating前向$\vec{h_j}$与后向$\stackrel{\leftarrow}{h_j}$，即$h_j=\left[\begin{array}{c}\stackrel{\rightarrow}{h_j}  \\  \stackrel{\leftarrow}{h_j}  \end{array} \right]$，计算时词向量矩阵$E$是前向与后向网络共享的，其他参数则不是.&lt;br /&gt;
2. &lt;strong&gt;对齐模型(alignment model)&lt;/strong&gt;&lt;br /&gt;
该模型也可以说是实现了注意力模型(Attention model).&lt;br /&gt;
通过该模型计算得到输入时刻$j$在预测输出时刻$i$时所占的权重$\alpha _{ij}$.比如说翻译&lt;strong&gt;&amp;ldquo;我喜欢飞机&amp;rdquo;&lt;/strong&gt;到&lt;strong&gt;&amp;ldquo;I like airplane&amp;rdquo;&lt;/strong&gt;,翻译输出&lt;strong&gt;&amp;ldquo;I&amp;rdquo;&lt;/strong&gt;时，&lt;strong&gt;&amp;ldquo;我&amp;rdquo;&lt;/strong&gt;字所占的权重会比较大.&lt;br /&gt;
权重$\alpha _{ij}$是通过一个单层的多层感知机计算得到,该模型与算法中其他部分同时训练:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458736905308.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
3. &lt;strong&gt;Encoder 和 Decoder&lt;/strong&gt;&lt;br /&gt;
Encoder的计算如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458737365201.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
$E$表示词向量的矩阵，$x_i$表示$i$时刻的词,是一个$k$维的向量(词典大小维度的向量,应该是one-hot的)，反向过程的计算与上面类似。&lt;/p&gt;

&lt;p&gt;Decoder的计算如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458737469906.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
初始的隐藏状态为：$$s_0 = tanh(W_s\stackrel{\leftarrow}{h_1})$$&lt;br /&gt;
每一步的context向量都需要通过Alignment model重新计算:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738176634.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738239955.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
最后计算$y_i$的概率:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738415785.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738447372.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738462004.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
使用了maxout（第二个公式），取相邻两个数中较大的一个。需要计算词典中所有词出现的概率，最后取最大的那一个？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;具体的实现细节见论文的附录&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation笔记</title>
      <link>http://blog.songru.org/posts/notebook/Learning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation_NOTE/</link>
      <pubDate>Sun, 27 Mar 2016 21:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/notebook/Learning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation_NOTE/</guid>
      <description>&lt;p&gt;本文主要创新在于提出一个新的神经网络模型RNN Encoder-Decoder，并提出GRU单元.将训练的模型作为standard phrase-based statistical machine translation system的一部分，用于计算phrase table中的每一个phrase的得分。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;1. RNN Encoder-Decoder&lt;/strong&gt;
该模型由两个RNN组成，分别作为Encoder和Decoder，Encoder的作用是读取一个变长的序列数据，将其编码为一个固定长度的向量，再通过Decoder将这个向量解码为一个变长的序列数据.&lt;/p&gt;

&lt;p&gt;Encoder为一个由GRU单元组成的RNN网络:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458734687528.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
$e(x_t)$代表t时刻输入词的向量表示，初始的隐藏状态$h^{(0)}$固定为$0$，最后计算得出一个固定长度的编码结果：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458735958249.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
Decoder也为一个由GRU单元组成的RNN网络:
它首先初始化隐藏状态：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458736097645.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
其网络的计算公式为：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458736032536.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
这里每一个时刻的计算都用到了Encoder传递过来的编码向量$c$，并且它的值是不变的。&lt;br /&gt;
最后通过Softmax计算概率（没看懂），还有maxout。。。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theano 中使用pydot报错</title>
      <link>http://blog.songru.org/posts/machine-learning/theano%20pydot/</link>
      <pubDate>Sun, 03 Jan 2016 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/machine-learning/theano%20pydot/</guid>
      <description>&lt;p&gt;运行:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 切换为使用cpu,有这句var_with_name_simple=True的时候会报错...,换成gpu就不报错...
theano.printing.pydotprint(forward_prop, var_with_name_simple=True, compact=True, outfile=&#39;img/nn-theano-forward_prop.png&#39;, format=&#39;png&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;报错:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pywintypes.error(2,&#39;RegOpenKeyEx&#39;,&#39;\xcf\xb5\xcd\xb3\xd5\xd2\xb2\xbb\xb5\xb...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因是未安装好graphviz,于是下载&lt;a href=&#34;http://www.graphviz.org/Download_windows.php&#34;&gt;安装包&lt;/a&gt;,并把安装目录的bin加入到path中(重要!)&lt;/p&gt;

&lt;p&gt;后来又报错:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RuntimeError: Failed to import pydot. You must install pydot for `pydotprint` to work.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因是使用的pyparsing版本太高(我的是2.0.3),使用低版本的即可:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    pip install pyparsing==1.5.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我还安装了pydot2,不知有什么用:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    pip install pydot2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我之后做了一个实验,再把pyparsing升级到2.0.3,报了一个不一样的错:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;InvocationException: Program terminated with status: -1073741819. stderr follows: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将pyparsing再降级即可&amp;hellip;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Window7安装theano、anaconda、CUDA</title>
      <link>http://blog.songru.org/posts/machine-learning/theano%20cuda%20install/</link>
      <pubDate>Sat, 02 Jan 2016 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/machine-learning/theano%20cuda%20install/</guid>
      <description>

&lt;p&gt;最开始我使用的是anaconda3，但是一直没有成功，并且很多库对python2的支持更好，所以最好尝试使用anaconda2.
因为要使用mingw，但是新版的anaconda都没有自带mingw，所以下载了老版本anaconda1.9.2.&lt;/p&gt;

&lt;h1 id=&#34;1-安装cuda:10082a9e681b37c239307c871d1bb53d&#34;&gt;1.安装CUDA&lt;/h1&gt;

&lt;p&gt;我的gpu是GTX 750ti|，使用的是CUDA7，安装简单（已久忘了细节，传说中使用默认路径安装好些，笔记本上是GTX 950M，CUDA7.5）。安装好CUDA后，跑程序可能遇到cuda is installed,but unavailale的错误，这个我查的结果说是显卡驱动太低，然后我就更新为最新的显卡驱动！！
安装完之后在cmd下执行nvcc -V查看版本成功的话，则表示小成。
要的话，进到sample目录，用相应的vs打开解决方案，然后生成解决方案，中途可能会有无法打开”d3dx9.h”、”d3dx10.h”、”d3dx11.h”头文件，可以&lt;a href=&#34;http://www.microsoft.com/en-us/download/details.aspx?id=6812&#34;&gt;下载DXSDK_Jun10.exe&lt;/a&gt;，然后安装到默认目录下；再编译工程即可；然后到bin目录跑生成的程序（deviceQuery.exe还有一些图形程序等），没问题的话则大成了。&lt;br /&gt;
下面是一个详细的步骤（copy来的）:&lt;br /&gt;
　　　1. 查看本机配置，查看显卡类型是否支持NVIDIA GPU，选中计算机&amp;ndash;&amp;gt; 右键属性 &amp;ndash;&amp;gt; 设备管理器 &amp;ndash;&amp;gt; 显示适配器：NVIDIA GeForce GT 610，在&lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;这里&lt;/a&gt;可以查到相应显卡的compute capability；&lt;br /&gt;
　　　2. 在&lt;a href=&#34;http://www.nvidia.cn/Download/index.aspx?lang=cn&#34;&gt;这里&lt;/a&gt;下载合适驱动347.88-desktop-win8-win7-winvista-64bit-international-whql.exe 并安装；&lt;br /&gt;
　　　3. 从&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;https://developer.nvidia.com/cuda-toolkit&lt;/a&gt;   根据本机类型下载相应的最新版本CUDA7.0安装；&lt;br /&gt;
　　　4. 按照&lt;a href=&#34;http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-microsoft-windows/index.html#axzz3W8BU10Ol&#34;&gt;官方文档&lt;/a&gt;步骤，验证是否安装正确：&lt;br /&gt;
　　　　　(1) 打开C:\ProgramData\NVIDIACorporation\CUDA Samples\v7.0目录下的Samples_vs2010.sln工程，分别在Debug、Release x64下编译整个工程；&lt;br /&gt;
　　　　　(2) 编译过程中，会提示找不到”d3dx9.h”、”d3dx10.h”、”d3dx11.h”头文件，可以&lt;a href=&#34;http://www.microsoft.com/en-us/download/details.aspx?id=6812&#34;&gt;下载DXSDK_Jun10.exe&lt;/a&gt;，然后安装到默认目录下；再编译工程即可；&lt;br /&gt;
　　　　　(3) 打开C:\ProgramData\NVIDIACorporation\CUDA Samples\v7.0\bin\win64\Release目录，打开cmd命令行，将deviceQuery.exe直接拖到cmd中，回车，会显示GPU显卡、CUDA版本等相关信息，最后一行显示：Result = PASS；&lt;br /&gt;
　　　　　(4) 将bandwidthTest.exe拖到cmd中，回车，会显示Device0: GeForce GT 610等相关信息，后面也会有一行显示：Result = PASS；&lt;/p&gt;

&lt;h1 id=&#34;2-安装anaconda:10082a9e681b37c239307c871d1bb53d&#34;&gt;2.安装anaconda&lt;/h1&gt;

&lt;p&gt;开始使用的是anacond192版本，最后都配置成功了，但是这个版本的ipython notebook等等东西版本太低，我就用conda update了一下，导致一切都玩完了，theano又跑不起来了。所以后来就换成了anaconda2.4.1，是anaconda2的最新版本，但是没有mingw（这时我已经知道怎么配了，所以没有也不怕，我自己安）。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;安装anaconda很简单，傻瓜式的&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;装mingw，运行conda install mingw即可&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;装theano，因为conda库中没有theano，所以使用pip安装，执行pip install theano即可&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;配置.theanorc.txt&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在home目录下(我的是c:/uesrs/lisongru)创建.theanorc.txt,输入一下内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[blas]
ldflags =
[gcc]
cxxflags = -IE:\Anaconda2\MinGW    #安装的mingw目录
[nvcc]
fastmath = True
flags=-LE:\Anaconda2\libs
compiler-bindir=C:\Program Files (x86)\Microsoft Visual Studio 10.0\VC\bin   #vs的目录,不知是否可以不要,因为下面path也配了
flags =  -arch=sm_30   #这句没有也行,好像,未测试
base_compiledir=path_to_a_directory_without_such_characters   #这句没有也行,好像,未测试
[global]
openmp = False
floatX = float32
device = gpu   #cpu则使用cpu
allow_input_downcast=True
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;3-配置环境变量:10082a9e681b37c239307c871d1bb53d&#34;&gt;3.配置环境变量&lt;/h1&gt;

&lt;p&gt;这一步非常关键,开始我的电脑有cygwin，并且也在path中，然后运行import theano总是报错，原因是cygwin被用来编译了，但我们要用的是。。。（我也不清楚),后来删除它的path,加入vs_bin的path,就好了.
下面是一些相关的path&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;VS10_VC_BIN%        #vs的bin目录,和上面的compiler-bindir一样,这必须有,没有则包找不到cl.exe...
%CUDA_PATH%\bin     #安装cuda好像默认会有,没有自己加,有它才能在cmd中nvcc -V查看
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5\libnvvp;        #cuda安装自带
C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common      #自带
%ANACONDA2_HOME%;%ANACONDA2_SCRIPTS%;%ANACONDA2_BIN%        #anaconda自带
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;4-遇到的问题:10082a9e681b37c239307c871d1bb53d&#34;&gt;4.遇到的问题&lt;/h1&gt;

&lt;p&gt;在笔记本上安装好一切后，报collect2: ld returned 1 exit status错，查的结果说是python 32位与64位冲突，那时笔记本上正好有32位的一个python，就卸载了，但还是不行。
再查，说是少了libpython包,安装之:conda install libpython，然后就好了。。。（这里有点奇怪，在台式机上没有这个包也成功了）。&lt;br /&gt;
&amp;gt; Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux命令（目前使用过的）</title>
      <link>http://blog.songru.org/posts/linux/linuxComands/</link>
      <pubDate>Tue, 22 Sep 2015 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/linux/linuxComands/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;移动文件（夹）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mv originalDir/source.txt targetDir/target.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;删除文件（夹）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rm dir/target.file   //删除文件
rm -r dir //删除文件夹
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;利用软件源下载安装软件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install package 安装包
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;解压文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf XXX压缩包
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建文件夹&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir dir    // -p参数可以创建子文件夹
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;touch dir/a.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;解压文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf file.tar.gz    //解压到当前目录
tar -zxvf file.tar.gz -C dir    //解压到制定目录
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是zip格式的话：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;unzip  abc.zip -d /home/test/
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;复制文件（夹）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp target.file copy.file    //复制文件
cp -r targetDir copyDir //复制文件夹，使用-r递归复制
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;打印当前路径&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pwd
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;管理员权限&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo otherComand...     //使用root权限执行后续命令,一次性的
sudo su    //登录root,以后的命令都是使用root权限
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看文件系统使用情况信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    df -Tha    //T显示分区类型，h易读格式，a显示所有   
    fdisk -l /dev/sda    // 查看指定磁盘分区情况
    parted /dev/sda    // parted为分区工具，p可查看分区情况，quit退出
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看文件(夹)大小&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;du filename/dir    //-s易于阅读,-h显示总览
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看内存使用情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;free -m    //-m表示单位为m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;任务管理器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;top     //退出按 q
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看系统信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;uname -a    //-a查看所有信息
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看网络接口信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ifconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;MD5校验&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo md5sum filename
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看文件(夹)权限情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ls -l path/filename      //查看path路径下名为filename的文件或文件夹的权限
ls -ls path    //查看path路径下的所有文件的权限
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改文件(夹)权限(change mode)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo chmod -（代表类型）×××（所有者）×××（组用户）×××（其他用户）    //其中×××指文件名（也可以是文件夹名，不过要在chmod后加-ld）,使用 -R 参数来递归修改所有子文件   


//三位数的每一位都表示一个用户类型的权限设置。取值是0～7，即二进制的[000]~[111],这个三位的二进制数的每一位分别表示读、写、执行权限。      
//如000表示三项权限均无，而100表示只读。这样，我们就有了下面的对应：
0 [000] 无任何权限
4 [100] 只读权限
6 [110] 读写权限
7 [111] 读写执行权限


//常用的
sudo chmod 600 ××× （只有所有者有读和写的权限）
sudo chmod 644 ××× （所有者有读和写的权限，组用户只有读的权限）
sudo chmod 700 ××× （只有所有者有读和写以及执行的权限）
sudo chmod 666 ××× （每个人都有读和写的权限）
sudo chmod 777 ××× （每个人都有读和写以及执行的权限）
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改文件所有者(change owner)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo chown -R lsr software/    // -R 递归修改 software文件夹所有者为lsr
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;读取文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat a.txt   # 读取a.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;转换文件编码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;iconv -f gbk -t utf-8   # 将文件编码从gbk转到utf8
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查找行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep -E &amp;quot;&amp;lt;content&amp;gt;|&amp;lt;contenttitle&amp;gt;&amp;quot;  # 保留含有&amp;lt;content&amp;gt;或者&amp;lt;contenttitle&amp;gt;的行,-E表示使用正则
grep -v &amp;quot;hehe&amp;quot; # -v 排除匹配的行
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;删除文本中指定内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tr -d &amp;quot;&amp;lt;content&amp;gt;|&amp;lt;contenttitle&amp;gt;|&amp;lt;/content&amp;gt;|&amp;lt;/contenttitle&amp;gt;|&amp;gt;&amp;quot;   # -d表示删除,删除引号内的字符
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多个命令连续执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat a.txt | iconv -f gbk -t utf-8 | grep -E &amp;quot;&amp;lt;content&amp;gt;|&amp;lt;contenttitle&amp;gt;&amp;quot; | tr -d &amp;quot;&amp;lt;content&amp;gt;|&amp;lt;contenttitle&amp;gt;|&amp;lt;/content&amp;gt;|&amp;lt;/contenttitle&amp;gt;|&amp;gt;&amp;quot; &amp;gt; corpus.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查找文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;find SogouCS/ -name &amp;quot;m_*&amp;quot;   # 在SogouCS目录中按文件名-name查找含有 m_*的文件
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命令参数传递&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# xargs将文件名传递过来, -i 表示使用传过来的参数替换 {}
find SogouCS/ -name &amp;quot;m_*&amp;quot; | xargs -i mv {} SogouCS_M/
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;大小写转换&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tr A-Z a-z  # 全部转换为小写
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;文本替换&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 替换’为&#39;，′为&#39;，&#39;为&#39; , -e 表示多个编辑,即多个sed同时, s表示替换,g表示全局替换
sed -e &amp;quot;s/’/&#39;/g&amp;quot; -e &amp;quot;s/′/&#39;/g&amp;quot; -e &amp;quot;s/&#39;&#39;/ /g&amp;quot;


# 替换A-Za-z&#39;_ \n之外的字符为空格,-c 表示排除..之外的都..
tr -c &amp;quot;A-Za-z&#39;_ \n&amp;quot; &amp;quot; &amp;quot;


# 例子             输入文件                                                                    输出到文件
sed -e &amp;quot;s/’/&#39;/g&amp;quot; -e &amp;quot;s/′/&#39;/g&amp;quot; -e &amp;quot;s/&#39;&#39;/ /g&amp;quot; &amp;lt; news.2012.en.shuffled | tr -c &amp;quot;A-Za-z&#39;_ \n&amp;quot; &amp;quot; &amp;quot; &amp;gt; news.2012.en.shuffled-norm0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;从第3000行开始，显示1000行。即显示3000~3999行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat filename | tail -n +3000 | head -n 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;显示1000行到3000行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat filename| head -n 3000 | tail -n +1000
*注意两种方法的顺序
分解：
    tail -n 1000：显示最后1000行
    tail -n +1000：从1000行开始显示，显示1000行以后的
    tail -n -1000：相当于tail -n 1000
    head -n 1000：显示前面1000行
    head -n +1000：显示前面1000行
    head -n -1000：从第1行到倒数1000行
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;用sed命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -n &#39;5,10p&#39; filename # 查看文件的第5行到第10行, p表示打印匹配行
sed -n &#39;5p&#39; filename # 只显示第5行
sed -n &#39;5,$p&#39; filename # 查看文件的第5行到最后一行, $表示最后一行
sed -n &#39;3,/movie/&#39;p temp.txt   # 只在第3行查找movie并打印
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Linux统计文件行数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wc -l file # - c 统计字节数, - l 统计行数, - w 统计字数。


1.统计demo目录下，js文件数量：
find demo/ -name &amp;quot;*.js&amp;quot; | wc -l


2.统计demo目录下所有js文件代码行数：
find demo/ -name &amp;quot;*.js&amp;quot; | xargs cat | wc -l 或 
wc -l `find ./ -name &amp;quot;*.js&amp;quot;` | tail -n 1 # 取最后一行,总行数


3.统计demo目录下所有js文件代码行数，过滤了空行：
find /demo -name &amp;quot;*.js&amp;quot; | xargs cat | grep -v ^$ | wc -l
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;打印进程信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ps aux
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ssh连接主机,并执行命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh username@ip &#39;command&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;shell中的一种for循环&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for((i=1;i&amp;lt;=10;i++))
do
    echo compute-0-$i
    ssh compute-0-$i &amp;quot;nvidia-smi | grep &#39;%&#39; &amp;quot;
 done
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;awk中的while循环&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;awk &#39;BEGIN{s=&amp;quot;&amp;quot;;i=11}{while(i&amp;lt;=NF){ s=s&amp;quot; &amp;quot;&amp;quot;&amp;quot;$i;i++; } print s;}&#39; # 提取出command &amp;quot;&amp;quot;用于拼接字符串, NF表示总的字段数
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;半角字符与全角字符之间的unicode码相差65248,而全角空格为12288,半角为32,需特殊对待.&lt;/p&gt;

&lt;p&gt;java代码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 全角转半角
 * @param str
 * @return
 */
public static String FullWidth2HalfWidth(String str){
    if (null == str || str.length() &amp;lt;= 0) {
        return &amp;quot;&amp;quot;;
    }
    char []cs;
    cs = str.toCharArray();
    int k;
    for(int i = 0; i &amp;lt; cs.length; i++){
        k = (int)cs[i];
        if(k &amp;gt;= 65281 &amp;amp;&amp;amp; k &amp;lt;= 65374){
            cs[i] -= 65248;
        }else if(k == 12288 || k == 58380){
            cs[i] = 32;
        }
    }
    return new String(cs);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写的bat代码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@echo off
echo Deploying updates to GitHub...

:: Build the project.
hugo -t red

:: Add changes to git.
git add -A

:: Commit changes.
set msg=rebuilding site %date% %time%
if not &amp;quot;%1&amp;quot;==&amp;quot;&amp;quot; (
  set msg=%1
)
git commit -m &amp;quot;%msg&amp;quot;

:: Push source and build repos.
git push origin hugo
git subtree push --prefix=public origin master

pause
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对应的shell代码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
echo -e &amp;quot;\033[0;32mDeploying updates to GitHub...\033[0m&amp;quot;

# Build the project.
hugo -t red

# Add changes to git.
git add -A

# Commit changes.
msg=&amp;quot;rebuilding site `date`&amp;quot;
if [ $# -eq 1 ]
  then msg=&amp;quot;$1&amp;quot;
fi
git commit -m &amp;quot;$msg&amp;quot;

# Push source and build repos.
git push origin hugo
git subtree push --prefix=public origin master
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>【自然语言处理之三】最小编辑距离（Minimum Edit Distance）</title>
      <link>http://blog.songru.org/posts/nlp/nlp_3_MinimumEditDistance/</link>
      <pubDate>Sun, 13 Sep 2015 21:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/nlp/nlp_3_MinimumEditDistance/</guid>
      <description>

&lt;h1 id=&#34;一-最小编辑距离:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;一、最小编辑距离&lt;/h1&gt;

&lt;h2 id=&#34;1-1-定义:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.1 定义&lt;/h2&gt;

&lt;p&gt;最小编辑距离（Minimum Edit Distance，MED），又称Levenshtein距离，是指两个字符串之间，由一个转成另一个所需要的最少编辑操作次数。允许的编辑操作包括：将一个字符替换成另一个字符（substitution，s），插入一个字符（insert，i）或者删除一个字符（delete，d），如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_1.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;1-2-求解:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.2 求解&lt;/h2&gt;

&lt;p&gt;在大学算法设计相关课程上，想必大家都已经学习过使用动态规划算法解最小编辑距离，形式化定义如下：
对于两个字符串X={x1,x2,x3&amp;hellip;,xn}和Y={y1,y2,y3,&amp;hellip;,ym}，x的长度是n，y的长度是m，则定义D(i,j)为子字符串X{x1,x2,&amp;hellip;,xi}于Y{y1,y2,&amp;hellip;,yj}间的最小编辑距离。我们的目标是求出D(n,m)。
则可以推出如下关系:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_3.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;根据这个递推关系，我们计算所有的i和j的取值填入一个矩阵中：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_4.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;最终可以求得右上角的D(n,m)。&lt;/p&gt;

&lt;h2 id=&#34;1-3-回溯法对齐字符串:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.3 回溯法对齐字符串&lt;/h2&gt;

&lt;p&gt;通过上面的动态规划方法虽然可以求出来两个字符串之间的最小编辑距离，但是我们并不知道源字符串被编辑的情况，即哪些字符被删除，增加或者替换了。
我们只要能够将源字符串与目标字符串相应字符对齐，即可以得出具体的编辑情况。这里只需要对上面的动态规划方法稍加改进即可实现。
我们只需要记住当前位置是从之前哪一个位置计算而来即可，然后从D(n,m)逐个向前回溯就能够将两个字符串对齐，如图所示：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_5.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;其递推公式为：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_6.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;1-4-加权最小编辑距离:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.4 加权最小编辑距离&lt;/h2&gt;

&lt;p&gt;在基本的编辑距离基础上，结合实际应用，往往需要做一些变形或改进，如某些拼写错误相对其他更容易发生，同义词替换、虚词或修饰成分被删除（或插入）应该得到较小的惩罚，等等。
下图是一个统计好的拼写错误的转移矩阵：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_8.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;加权递推公式如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_9.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;1-5-最小编辑距离的应用:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.5 最小编辑距离的应用&lt;/h2&gt;

&lt;p&gt;最小编辑距离通常作为一种相似度计算函数被用于多种实际应用中，详细如下： （特别的，对于中文自然语言处理，一般以词为基本处理单元）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;拼写纠错（Spell Correction）：又拼写检查（Spell Checker），将每个词与词典中的词条比较，英文单词往往需要做词干提取等规范化处理，如果一个词在词典中不存在，就被认为是一个错误，然后试图提示N个最可能要输入的词——拼写建议。常用的提示单词的算法就是列出词典中与原词具有最小编辑距离的词条。&lt;br /&gt;
这里肯定有人有疑问：对每个不在词典中的词（假如长度为len）都与词典中的词条计算最小编辑距离，时间复杂度是不是太高了？的确，所以一般需要加一些剪枝策略，如：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;因为一般拼写检查应用只需要给出Top-N的纠正建议即可（N一般取10），那么我们可以从词典中按照长度依次为len、len-1、len+1、len-2、len-3、&amp;hellip;的词条比较；&lt;/li&gt;
&lt;li&gt;限定拼写建议词条与当前词条的最小编辑距离不能大于某个阈值；&lt;/li&gt;
&lt;li&gt;如果最小编辑距离为1的候选词条超过N后，终止处理；&lt;/li&gt;
&lt;li&gt;缓存常见的拼写错误和建议，提高性能。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;DNA分析：基因学的一个主要主题就是比较 DNA 序列并尝试找出两个序列的公共部分。如果两个 DNA 序列有类似的公共子序列，那么这些两个序列很可能是同源的。在比对两个序列时，不仅要考虑完全匹配的字符，还要考虑一个序列中的空格或间隙（或者，相反地，要考虑另一个序列中的插入部分）和不匹配，这两个方面都可能意味着突变（mutation）。在序列比对中，需要找到最优的比对（最优比对大致是指要将匹配的数量最大化，将空格和不匹配的数量最小化）。如果要更正式些，可以确定一个分数，为匹配的字符添加分数、为空格和不匹配的字符减去分数。&lt;br /&gt;
全局序列比对尝试找到两个完整的序列 S1和 S2之间的最佳比对。以下面两个 DNA 序列为例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S1= GCCCTAGCG
S2= GCGCAATG
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果为每个匹配字符一分，一个空格扣两分，一个不匹配字符扣一分，那么下面的比对就是全局最优比对：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S1&#39;= GCCCTAGCG
S2&#39;= GCGC-AATG
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;连字符（-）代表空格。在 S2&amp;rsquo;中有五个匹配字符，一个空格（或者反过来说，在 S1&amp;rsquo;中有一个插入项），有三个不匹配字符。这样得到的分数是 (5 * 1) + (1 * -2) + (3 * -1) = 0，这是能够实现的最佳结果。&lt;br /&gt;
使用局部序列比对，不必对两个完整的序列进行比对，可以在每个序列中使用某些部分来获得最大得分。使用同样的序列 S1和 S2，以及同样的得分方案，可以得到以下局部最优比对 S1&amp;rdquo;和 S2&amp;rdquo;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S1      = GCCCTAGCG
S1&#39;&#39;=           GCG
S2&#39;&#39;=           GCG
S2      =       GCGCAATG
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个局部比对的得分是 (3 * 1) + (0 * -2) + (0 * -1) = 3。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命名实体抽取（Named Entity Extraction）：由于实体的命名往往没有规律，如品牌名，且可能存在多种变形、拼写形式，如“IBM”和“IBM Inc.”，这样导致基于词典完全匹配的命名实体识别方法召回率较低，为此，我们可以使用编辑距离由完全匹配泛化到模糊匹配，先抽取实体名候选词。&lt;br /&gt;
具体的，可以将候选文本串与词典中的每个实体名进行编辑距离计算，当发现文本中的某一字符串的编辑距离值小于给定阈值时，将其作为实体名候选词；获取实体名候选词后，根据所处上下文使用启发式规则或者分类的方法判定候选词是否的确为实体名。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;实体共指（Entity Coreference）：通过计算任意两个实体名之间的最小编辑距离判定是否存在共指关系？如“IBM”和“IBM Inc.”, &amp;ldquo;Stanford President John Hennessy &amp;ldquo;和&amp;rdquo;Stanford University President John Hennessy&amp;rdquo;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;机器翻译（Machine Translation）：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;识别平行网页对：由于平行网页通常有相同或类似的界面结构，因此平行网页在HTML结构上应该有很大近似度。首先将网页的HTML标签抽取出来，连接成一个字符串，然后用最小编辑距离考察两个字符串的近似度。实际中，此策略一般与文档长度比例、句对齐翻译模型等方法结合使用，以识别最终的平行网页对。&lt;/li&gt;
&lt;li&gt;自动评测：首先存储机器翻译原文和不同质量级别的多个参考译文，评测时把自动翻译的译文对应到与其编辑距离最小的参考译文上，间接估算自动译文的质量，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_10.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;字符串核函数（String Kernel）：最小编辑距离作为字符串之间的相似度计算函数，用作核函数，集成在SVM中使用。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;二-参考资料:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;二、参考资料&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Lecture Slides：&lt;a href=&#34;http://spark-public.s3.amazonaws.com/nlp/slides/med.pptx&#34;&gt;Minimum Edit Distance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org&#34;&gt;http://en.wikipedia.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.google.com.hk/url?sa=t&amp;amp;rct=j&amp;amp;q=%E6%9C%80%E5%B0%8F%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB+%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91&amp;amp;source=web&amp;amp;cd=7&amp;amp;ved=0CG0QFjAG&amp;amp;url=http%3A%2F%2Fwww.ecice06.com%2FCN%2Farticle%2FdownloadArticleFile.do%3FattachType%3DPDF%26id%3D10174&amp;amp;ei=GNinT4iaKKaViQfDpbnKAw&amp;amp;usg=AFQjCNGbWIzPc5GpPghvdYQAnbQ8iIPMIw&#34;&gt;双语平行网页挖掘系统的设计与实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wenku.baidu.com/view/a50949dc7f1922791688e81d.html&#34;&gt;机器翻译系统评测规范&lt;/a&gt; .&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wenku.baidu.com/view/a50949dc7f1922791688e81d.html&#34;&gt;matrix67-漫话中文分词算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ir.hit.edu.cn/ir_papers/Vol_3/Improved-Edit-Distance%20Kernel%20for%20Chinese%20Relation%20Extraction.pdf&#34;&gt;Improved-Edit-Distance Kernel for Chinese Relation Extraction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>【自然语言处理之二】文本处理基础（Basic Text Processing）</title>
      <link>http://blog.songru.org/posts/nlp/nlp_2_BasicTextProcessing/</link>
      <pubDate>Fri, 11 Sep 2015 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/nlp/nlp_2_BasicTextProcessing/</guid>
      <description>

&lt;h1 id=&#34;一-文本处理基础:2e62519647c0e571a1761fc424fb31ad&#34;&gt;一、文本处理基础&lt;/h1&gt;

&lt;h2 id=&#34;1-1-正则表达式:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.1 正则表达式&lt;/h2&gt;

&lt;p&gt;自然语言处理过程中面临大量的文本处理工作，如词干提取、网页正文抽取、分词、断句、文本过滤、模式匹配等任务，而正则表达式往往是首选的文本预处理工具。
现在主流的编程语言对正则表达式都有较好的支持，如Grep、Awk、Sed、Python、Perl、Java、C/C++(推荐re2)等。
&lt;em&gt;注：课程中给出的正则表达式语法和示例在此略去&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-2-分词:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.2 分词&lt;/h2&gt;

&lt;p&gt;分词的操作就是将一句话中的词语全部切分开来。&lt;/p&gt;

&lt;h3 id=&#34;1-2-1-词典规模:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.2.1 词典规模&lt;/h3&gt;

&lt;p&gt;同一词条可能存在不同的时态、变形，那么给定文本语料库，如何确定词典规模呢？
首先定义两个变量Type和Token：
    &lt;strong&gt;Type&lt;/strong&gt;：词典中的元素，即独立词条
    &lt;strong&gt;Token&lt;/strong&gt;：词典中独立词条在文本中的每次出现
如果定义 N = number of tokens 和 V = vocabulary = set of types，|V| is the size of the vocabulary，那么根据Church and Gale (1990)的研究工作可知: |V| &amp;gt; O(N½) ，验证如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_2_1.jpg&#34; alt=&#34;词典规模&#34; /&gt;
&lt;/p&gt;

&lt;h3 id=&#34;1-2-2-分词算法:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.2.2 分词算法&lt;/h3&gt;

&lt;p&gt;任务：统计给定文本文件（如shake.txt）中词频分布，子任务：分词，频率统计，实现如下：&lt;/p&gt;

&lt;p&gt;英文分词非常简单，因为英文中每个单词之间（少数特殊写法额外考虑，We&amp;rsquo;re,isn&amp;rsquo;t&amp;hellip;)都由空格分割开来，所以分词只需要按空格将它们切分，然后对一些特殊的写法进一步处理即可。&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_2_2.jpg&#34; alt=&#34;分词算法&#34; /&gt;
&lt;br /&gt;
以上实现将非字母字符作为token分隔符作为简单的分词器实现，但是，这难免存在许多不合理之处，如下面的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    - Finland’s capital  -&amp;gt;  Finland Finlands Finland’s  ?
    - what’re, I’m, isn’t  -&amp;gt;  What are, I am, is not
    - Hewlett-Packard   -&amp;gt;  Hewlett Packard ?
    - state-of-the-art     -&amp;gt;   state of the art ?
    - Lowercase  -&amp;gt;  lower-case lowercase lower case ?
    - San Francisco -&amp;gt;   one token or two?
    - m.p.h., PhD.  -&amp;gt;  ??
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的方法对英语这种包含固定分隔符的语言行之有效，对于汉语、日语、德语等文本则不再适用，所以就需要专门的分词技术。其中，最简单、最常用，甚至是最有效的方法就是最大匹配法（Maximum Matching），它是一种基于贪心思想的切词策略，主要包括正向最大匹配法（Forward Maximum Matching，FMM）、逆向最大匹配法（Backward Maximum Matching）与双向最大匹配法（Bi-direction Maximum Matching，BMM）。&lt;/p&gt;

&lt;p&gt;以FMM中文分词为例，步骤如下：
1.  选取包含N(N通常取6~8)个汉字的字符串作为最大字符串；
2. 把最大字符串与词典中的单词条目相匹配（词典通常使用Double Array Trie组织）；
3. 如果不能匹配，就去除最后一个汉字继续匹配，直到在词典中找到相应的词条为止。
例如：句子“莎拉波娃现在居住在美国东南部的佛罗里达“的分词结果是”莎拉波娃/   现在/   居住/   在/  美国/   东南部/     的/  佛罗里达/”。
另外，使用较多的分词方法有最少分词法、最短路径法、最大概率法（或词网格法，大规模语料库+HMM/HHMM）、字标注法等。&lt;/p&gt;

&lt;h3 id=&#34;1-2-3-分词难点:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.2.3 分词难点&lt;/h3&gt;

&lt;p&gt;-切分歧义：主要包括交集型歧义和覆盖型歧义，在汉语书面文本中占比并不大，而且一般都可以通过规则或建立词表解决。&lt;/p&gt;

&lt;p&gt;-未登录词：就是未在词典中记录的人名（中、外）、地名、机构名、新词、缩略语等，构成了中文自然语言处理永恒的难点。常见的解决方法有互信息、语言模型，以及基于最大熵或隐马尔科夫模型的统计分类方法。&lt;/p&gt;

&lt;h2 id=&#34;1-3-文本归一化-标准化:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.3 文本归一化(标准化)&lt;/h2&gt;

&lt;p&gt;文本归一化主要包括大小写转换、词干提取、繁简转换等问题。&lt;/p&gt;

&lt;h2 id=&#34;1-4-断句:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.4 断句&lt;/h2&gt;

&lt;p&gt;句子一般分为大句和小句，大句一般由“！”，“。”，“；”，“\“”、“？”等分割，可以表达完整的含义，小句一般由“，”分割，起停顿作用，需要上下文搭配表达特定的语义。
中文断句通常使用正则表达式将文本按照有分割意义的标点符号(如句号)分开即可，而对于英文文本，由于英文句号”.“在多种场景下被使用，如缩写“Inc.”、“Dr.”、“.02%”、“4.3”等，无法通过简单的正则表达式处理，为了识别英文句子边界，课程中给出了一种基于决策树（Decision Tree）的分类方法，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_2_3.jpg&#34; alt=&#34;断句&#34; /&gt;
&lt;br /&gt;
此方法的核心就是如何选取有效的特征？如句号前后的单词是否大写开头、是否为缩略词、前后是否存在数字、句号前的单词长度、句号前后的单词在语料库中作为句子边界的概率等等。当然，你也可以基于上述特征采用其他分类器解决断句问题，如罗辑回归（Logistic regression）、支持向量机（Support Vector Machine）、神经网络（Neural Nets）等。&lt;/p&gt;

&lt;h1 id=&#34;二-参考资料:2e62519647c0e571a1761fc424fb31ad&#34;&gt;二、参考资料&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Lecture Slides：&lt;a href=&#34;http://spark-public.s3.amazonaws.com/nlp/slides/textprocessingboth.pptx&#34;&gt;Basic Text Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org&#34;&gt;http://en.wikipedia.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;关毅，统计自然语言处理基础 课程PPT&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=4&amp;amp;ved=0CHAQFjAD&amp;amp;url=http%3A%2F%2Fwww2.denizyuret.com%2Fref%2Fchurch%2Fpublished_1990_darpa.ps.gz&amp;amp;ei=guWlT4rCFOWZ2QW3mbymAg&amp;amp;usg=AFQjCNFcEeYyaP8TQUYJxNVUkdoHZl98hg&amp;amp;sig2=17cCPZhMQzpWCHeWm-knag&#34;&gt;Gale, W. A. and K. W. Church (1990) “Estimation Procedures for Language Context: Poor Estimates are Worse than None,” Proceedings in Computational Statistics, 1990, p.69-74, Physica-Verlag, Heidelberg&lt;/a&gt; .&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.52nlp.cn/matrix67-%E6%BC%AB%E8%AF%9D%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95&#34;&gt;matrix67-漫话中文分词算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%951&#34;&gt;中文分词入门之字标注法&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>【自然语言处理之一】介绍</title>
      <link>http://blog.songru.org/posts/nlp/nlp_1_introduction/</link>
      <pubDate>Thu, 16 Jul 2015 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/nlp/nlp_1_introduction/</guid>
      <description>

&lt;h1 id=&#34;一-什么是自然语言处理-nature-language-processing:a1b5d0a3988a3fc353b3287af28cee1e&#34;&gt;一、什么是自然语言处理（Nature Language Processing）？&lt;/h1&gt;

&lt;p&gt;首先看什么是自然语言，自然语言是指一种自然地随文化演化的语言。各个国家地区的语言,英语、汉语、日语等等都属于自然语言，它们都是随这文化发展自然形成的，自然语言是人类交流和思维的主要工具，是人类智慧的结晶。与之相对的是“人造语言”，比如各种编程语言。&lt;/p&gt;

&lt;p&gt;自然语言处理的任务就是让计算机理解人类的自然语言，从而实现人与计算机之间用自然语言进行有效通信。&lt;/p&gt;

&lt;p&gt;自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。&lt;/p&gt;

&lt;h1 id=&#34;二-自然语言处理的实际应用:a1b5d0a3988a3fc353b3287af28cee1e&#34;&gt;二、自然语言处理的实际应用&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;机器翻译&lt;/strong&gt;&lt;br /&gt;
我们最常接触的自然语言处理的应用就是机器翻译，它能够将某种语言 译为你指定的目标语言。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;回答问题&lt;/strong&gt;&lt;br /&gt;
比如Iphone中的Siri能够理解用户所说的话,并能做出回应。&lt;br /&gt;
说到回答问题，不得不提IBM创造的Watson，它是一台懂得人类语言的超级电脑。2011年2月17日,它在美国最受欢迎的智力竞猜电视节目《危险边缘》中击败该节目历史上两位最成功的选手肯-詹宁斯和布拉德-鲁特，成为《危险边缘》节目新的王者。《危险边缘》是哥伦比亚广播公司益智问答游戏节目，已经经历了数十年历史。该节目的比赛以一种独特的问答形式进行，问题设置的涵盖面非常广泛，涉及到各个领域。与一般问答节目相反，《危险边缘》以答案形式提问、提问形式作答，比如这样一个题目：
&lt;code&gt;这种带刺类型的植物有大约50个种类；因此是依据它的多刺的果实来命名的。&lt;/code&gt;
这个题目的答案是仙人掌。显然这需要watson具有理解自然语言的能力才能对这种题目进行作答。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;情感分析&lt;/strong&gt;&lt;br /&gt;
又称倾向性分析和意见挖掘，它是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程，如从大量网页文本中分析用户对“数码相机”的“变焦、价格、大小、重量、闪光、易用性”等属性的情感倾向。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;信息抽取&lt;/strong&gt;&lt;br /&gt;
其目的是将非结构化或半结构化的自然语言描述文本转化结构化的数据，如自动根据邮件内容生成Calendar。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;三-自然语言处理的研究进展:a1b5d0a3988a3fc353b3287af28cee1e&#34;&gt;三、自然语言处理的研究进展&lt;/h1&gt;

&lt;p&gt;自然语言处理也被分为许多小的研究领域，并且在某些方面已经取得了很好成果，但很多方面的研究依然很困难。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;基本解决&lt;/strong&gt;：词性标注、命名实体识别、Spam识别&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;取得长足进展&lt;/strong&gt;：情感分析、共指消解、词义消歧、句法分析、机器翻译、信息抽取&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;挑战&lt;/strong&gt;：自动问答、复述、文摘、会话机器人&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.songru.org/img/nlp_challenge.png&#34; alt=&#34;challenge&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;这里记录几个专业术语：
英文 | 中文
&amp;mdash;- | &amp;mdash;-
Part-of-speech (POS) tagging | 词性标注
Named entity recognition(NER) | 命名实体识别
Sentiment analysis | 情感分析
Coreference resolution | 指代消解
Word sense disambiguation(WSD) | 词义消歧
Paraphrase | 复述
Summarization | 摘要生成
neologisms | 新词
idioms | 成语&lt;/p&gt;

&lt;h1 id=&#34;四-自然语言处理的难点:a1b5d0a3988a3fc353b3287af28cee1e&#34;&gt;四、自然语言处理的难点&lt;/h1&gt;

&lt;p&gt;有多方面原因导致自然语言处理变得相当困难：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;自然语言具有多样性（不同语种、不同地域、不同人群）。让计算机理解一种语言已经足够困难，何况世界上一共有n种语言。&lt;/li&gt;
&lt;li&gt;自然语言具有进化性。网络语言就是随着社会的发展进化而来，它和传统的正式语言又有一定的差别，出现了很多新的表达方式（网络新词等等）。&lt;/li&gt;
&lt;li&gt;自然语言具有模糊性。&lt;/li&gt;
&lt;li&gt;自然语言具有歧义性。处理歧义问题是NLP的核心问题。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;五-自然语言处理的基本方法:a1b5d0a3988a3fc353b3287af28cee1e&#34;&gt;五、自然语言处理的基本方法&lt;/h1&gt;

&lt;p&gt;在NLP领域中，目前使用较多是概率模型（probabilistic model）或称为统计模型（statistical model），或者称为“经验主义模型”，其建模过程基于大规模真实语料库，从中各级语言单位上的统计信息，并且，依据较低级语言单位上的统计信息，运行相关的统计、推理等技术计算较高级语言单位上的统计信息。与其相对的“理想主义模型”，即基于Chomsky形式语言的确定性语言模型，它建立在人脑中先天存在语法规则这一假设基础上，认为语言是人脑语言能力推导出来的，建立语言模型就是通过建立人工编辑的语言规则集来模拟这种先天的语言能力。&lt;/p&gt;

&lt;p&gt;本课程主要侧重于&lt;strong&gt;基于统计的NLP技术，如Viterbi、贝叶斯和最大熵分类器、N-gram语言模型&lt;/strong&gt;等等。
&amp;gt; Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>统计学习方法读书笔记(5)-决策树</title>
      <link>http://blog.songru.org/posts/machine-learning/statical-learning-5-Decition-tree/</link>
      <pubDate>Tue, 07 Jul 2015 10:51:18 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/machine-learning/statical-learning-5-Decition-tree/</guid>
      <description>

&lt;p&gt;决策树(decision tree)是一种基本的分类与回归方法。它可以认为是if-then规则的集合,也可以认为是定义在特征空间与类空间上的条件概率分布。决策树学习通常包括3个步骤:&lt;strong&gt;特征选择、决策树的生成和决策树的修剪&lt;/strong&gt;。&lt;/p&gt;

&lt;h1 id=&#34;决策树模型与学习:0b17248df6960d4fcec23e1367068474&#34;&gt;决策树模型与学习&lt;/h1&gt;

&lt;p&gt;分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点有两种类型:内部结点(internal node)和叶结点(leaf    node)。内部结点表示一个特征或属性,叶结点表示一个类。&lt;/p&gt;

&lt;p&gt;用决策树分类,从根结点开始,对实例的某一特征进行测试,根据测试结果,将实例分配到其子结点;这时,每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配,直至达到叶结点。最后将实例分到叶结点的类中。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;可以将决策树看成一个if-then规则的集合。&lt;/strong&gt;将决策树转换成if-then规则的过程是这样的:由决策树的根结点到叶结点的每一条路径构建一条规则;路径上内部结点的特征对应着规则的条件,而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质:&lt;strong&gt;互斥并且完备&lt;/strong&gt;。这就是说,每一个实例都被一条路径或一条规则所覆盖,而且只被一条路径或一条规则所覆盖。&lt;/p&gt;

&lt;h2 id=&#34;决策树与条件概率分布:0b17248df6960d4fcec23e1367068474&#34;&gt;决策树与条件概率分布&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;决策树还表示给定特征条件下类的条件概率分布。&lt;/strong&gt;这一条件概率分布定义在特征空间的一个划分
(partition)上。将特征空间划分为互不相交的单元(cell)或区域(region),并&lt;strong&gt;在每个单元定义一个类的概率分布就构成了一个条件概率分布&lt;/strong&gt;。&lt;strong&gt;决策树的一条路径对应于划分中的一个单元。&lt;/strong&gt;决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设X为表示特征的随机变量,Y为表示类的随机变量,那么这个条件概率分布可以表示为P(Y|X)。X取值于给定划分下单元的集合,Y取值于类的集合。各叶结点(单元)上的条件概率往往偏向某一个类,即属于某一类的概率较大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。&lt;/p&gt;

&lt;h1 id=&#34;决策树学习:0b17248df6960d4fcec23e1367068474&#34;&gt;决策树学习&lt;/h1&gt;

&lt;p&gt;决策树学习,假设给定训练数据集
&lt;code&gt;$$ D = \{  (x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)  \} $$&lt;/code&gt;
其中&lt;code&gt;$   x_i=(x_i^{(1)}, x_i^{(2)} ,  \cdots, x_i^{(n)} )^T  $&lt;/code&gt;为输入实例(特征向量), &lt;code&gt;$n$&lt;/code&gt;为特征个数, &lt;code&gt;$y_i \in \{1,2, \cdots ,K\}$&lt;/code&gt;为类标记, &lt;code&gt;$i=1,2, \cdots ,N$&lt;/code&gt;, &lt;code&gt;$N$&lt;/code&gt;为样本容量.学习的目标是根据给定的训练数据集构建一个决策树模型,使它能够对实例进行正确的分类。&lt;/p&gt;

&lt;p&gt;决策树学习是由训练数据集估计条件概率模型。基于特征空间划分的类的条件概率模型有无穷多个。我们选择的条件概率模型应该不仅对训练数据有很好的拟合,而且对未知数据有很好的预测。&lt;/p&gt;

&lt;p&gt;决策树学习用损失函数表示这一目标。如下所述,决策树学习的&lt;strong&gt;损失函数通常是正则化的极大似然函数&lt;/strong&gt;。决策树学习的策略是以损失函数为目标函数的最小化。&lt;/p&gt;

&lt;p&gt;当损失函数确定以后,学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策
树中选取最优决策树是&lt;strong&gt;NP完全问题&lt;/strong&gt;,所以现实中决策树学习算法通常采用启发式方法,近似求解这一最优化问题。这样得到的决策树是&lt;strong&gt;次最优(sub-optimal)的&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;决策树学习算法包含&lt;strong&gt;特征选择、决策树的生成与决策树的剪枝过程&lt;/strong&gt;。由于决策树表示一个条件概率分布,所以深浅不同的决策树对应着不同复杂度的概率模型。&lt;strong&gt;决策树的生成对应于模型的局部选择,决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优,相对地,决策树的剪枝则考虑全局最优。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;特征选择:0b17248df6960d4fcec23e1367068474&#34;&gt;特征选择&lt;/h2&gt;

&lt;p&gt;特征选择在于选取对训练数据具有分类能力的特征。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别,则称这个特征是没有分类能力的。&lt;br /&gt;
直观上,如果一个特征具有更好的分类能力,或者说,按照这一特征将训练数据集分割成子集,使得各个子集在当前条件下有最好的分类,那么就更应该选择这个特征。&lt;strong&gt;信息增益(information gain)&lt;/strong&gt;就能够很好地表示这一直观的准则。&lt;/p&gt;

&lt;h3 id=&#34;熵-entropy:0b17248df6960d4fcec23e1367068474&#34;&gt;熵(entropy)&lt;/h3&gt;

&lt;p&gt;在信息论与概率统计中,&lt;strong&gt;熵(entropy)&lt;/strong&gt;是表示随机变量&lt;strong&gt;不确定性&lt;/strong&gt;的度量。设&lt;code&gt;$X$&lt;/code&gt;是一个取有限个值的离散随机变量,其概率分布为
&lt;code&gt;$$ P(X=x_I) = p_i, \quad i=1,2,\cdots, n $$&lt;/code&gt;
则随机变量X的熵定义为
&lt;code&gt;$$ H(X) = - \sum_{i=1}^n  p_i \log p_i $$&lt;/code&gt;
若&lt;code&gt;$p_i=0$&lt;/code&gt;, 则定义&lt;code&gt;$0\log0=0$&lt;/code&gt;。   通常, 对数以2为底或以e为底(自然对数),这时熵的单位分别称作&lt;strong&gt;比特(bit)或纳特(nat)&lt;/strong&gt;。由定义可知,熵只依赖于&lt;code&gt;$X$&lt;/code&gt;的分布,而与&lt;code&gt;$X$&lt;/code&gt;的取值无关,所以也可将&lt;code&gt;$X$&lt;/code&gt;的熵记作&lt;code&gt;$H(p)$&lt;/code&gt;,即
&lt;code&gt;$$ H(p) = - \sum_{i=1}^n  p_i \log p_i $$&lt;/code&gt;
&lt;strong&gt;熵越大,随机变量的不确定性就越大。&lt;/strong&gt;从定义可验证
&lt;code&gt;$$ 0 \le H(p) \le \log n $$&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;条件熵-conditional-entropy:0b17248df6960d4fcec23e1367068474&#34;&gt;条件熵(conditional entropy)&lt;/h3&gt;

&lt;p&gt;设有随机变量&lt;code&gt;$(X,Y)$&lt;/code&gt;,其联合概率分布为
&lt;code&gt;$$ P(X=x_i, Y=y_i) = p_{ij}, \quad i=1,2,\cdots,n ; \quad j=1,2,\cdots,m  $$&lt;/code&gt;
条件熵&lt;code&gt;$H(Y|X)$&lt;/code&gt;表示&lt;strong&gt;在已知随机变量X的条件下随机变量Y的不确定性&lt;/strong&gt;。随机变量X给定的条件下随机变量Y的条件熵(conditional entropy)&lt;code&gt;$H(Y|X)$&lt;/code&gt;, 定义为&lt;strong&gt;在X已经给定的条件下Y的条件概率分布的熵对X的数学期望&lt;/strong&gt;
&lt;code&gt;$$  H(Y \mid X) = \sum_{i=1}^n p_i H(Y \mid X = x_i) $$&lt;/code&gt;
这里, &lt;code&gt;$p_i=P(X=x_i), \ i=1,2,...,n$&lt;/code&gt;。
当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时,所对应的熵与条件熵分别称为&lt;strong&gt;经验熵(empirical  entropy)和经验条件熵(empirical     conditional entropy)&lt;/strong&gt;。此时,如果有0概率,令&lt;code&gt;$0\log0=0$&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&#34;信息增益-information-gain:0b17248df6960d4fcec23e1367068474&#34;&gt;信息增益(information gain)&lt;/h3&gt;

&lt;p&gt;信息增益(information gain)表示&lt;strong&gt;得知特征X的信息而使得类Y的信息的不确定性减少的程度&lt;/strong&gt;。&lt;br /&gt;
特征A对训练数据集D的信息增益&lt;code&gt;$g(D,A)$&lt;/code&gt;,定义为集合D的经验熵&lt;code&gt;$H(D)$&lt;/code&gt;与特征A给定条件下D的经验条件熵&lt;code&gt;$H(D \mid A)$&lt;/code&gt;之差,即
 &lt;code&gt;$$  g(D, A) = H(D) - H(D \mid A) $$&lt;/code&gt;
 熵&lt;code&gt;$H(Y)$&lt;/code&gt;与条件熵&lt;code&gt;$H(Y \mid X)$&lt;/code&gt;之差又称为&lt;strong&gt;互信息(mutual information)&lt;/strong&gt;。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。&lt;/p&gt;

&lt;p&gt;给定训练数据集D和特征A,经验熵&lt;code&gt;$H(D)$&lt;/code&gt;表示对数据集D进行分类的不确定性。而经验条件熵&lt;code&gt;$H(D \mid A)$&lt;/code&gt;表示在特征A给定的条件下对数据集D进行分类的不确定性。那么它们的差,即信息增益,就&lt;strong&gt;表示由于特征A而使得对数据集D的分类的不确定性减少的程度&lt;/strong&gt;。显然,对于数据集D而言,信息增益依赖于特征,不同的特征往往具有不同的信息增益。&lt;strong&gt;信息增益大的特征具有更强的分类能力&lt;/strong&gt;。&lt;br /&gt;
&lt;strong&gt;根据信息增益准则的特征选择方法是:&lt;/strong&gt;对训练数据集(或子集)D,计算其每个特征的信息增益,并比较它们的大小,选择信息增益最大的特征。&lt;/p&gt;

&lt;p&gt;设训练数据集为D, &lt;code&gt;$|D|$&lt;/code&gt; 表示其样本容量,即样本个数。设有K个类&lt;code&gt;$C_k,k=1,2, \cdots ,K$&lt;/code&gt;, &lt;code&gt;$|C_k|$&lt;/code&gt;为属于类&lt;code&gt;$C_k$&lt;/code&gt;的样本个数 。设特征A有n个不同的取值&lt;code&gt;$\{a_1,a_2, \cdots, a_n\}$&lt;/code&gt;, 根据特征A的取值将D划分为n个子集&lt;code&gt;$D_1,D_2, \cdots ,D_n$, $|D_i|$&lt;/code&gt;为&lt;code&gt;$D_i$&lt;/code&gt;的样本个数 。记子集&lt;code&gt;$D_i$&lt;/code&gt;中属于类&lt;code&gt;$C_k$&lt;/code&gt;的样本的集合为&lt;code&gt;$D_{ik}$&lt;/code&gt;, 即&lt;code&gt;$D_{ik} = D_i \cap C_k$&lt;/code&gt;, &lt;code&gt;$|D_{ik}|$&lt;/code&gt;为&lt;code&gt;$D_{ik}$&lt;/code&gt;的样本个数。于是信息增益的算法如下:&lt;/p&gt;

&lt;p&gt;输入:训练数据集D和特征A;&lt;br /&gt;
输出:特征A对训练数据集D的信息增益&lt;code&gt;$g(D,A)$&lt;/code&gt;。&lt;br /&gt;
步骤:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;计算数据集D的经验熵&lt;code&gt;$H(D)$&lt;/code&gt;
&lt;code&gt;$$ H(D) = - \sum_{k=1}^K \frac{\lvert C_k \rvert} { \lvert D \rvert} \log \frac{\lvert C_k \rvert} { \lvert D \rvert}  $$&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;计算特征A对数据集D的经验条件熵&lt;code&gt;$H(D \mid A)$&lt;/code&gt;
&lt;code&gt;$$ 
\begin{align*}
H(D \mid A) &amp;amp;= \sum_{i=1}^n \frac{\lvert D_i\rvert} { \lvert D \rvert}  H(D_i)   \\ 
&amp;amp;=  \sum_{i=1}^n \frac{\lvert D_i\rvert} { \lvert D \rvert}  \sum_{k=1}^K  \frac{\lvert D_{ik}\rvert} { \lvert D_i \rvert}  \log \frac{\lvert D_{ik}\rvert} { \lvert D_i \rvert} 
\end{align*}
$$&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;计算信息增益
&lt;code&gt;$$ g(D,A) = H(D) - H(D \mid A) $$&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;信息增益比:0b17248df6960d4fcec23e1367068474&#34;&gt;信息增益比&lt;/h3&gt;

&lt;p&gt;以信息增益作为划分训练数据集的特征, 存在偏向于选择取值较多的特征的问题。使用信息增益比(information gain ratio)可以对这一问题进行校正。这是特征选择的另一准则。&lt;br /&gt;
 特征A对训练数据集D的信息增益比&lt;code&gt;$g_R(D,A)$&lt;/code&gt;定义为其信息增益&lt;code&gt;$g(D,A)$&lt;/code&gt;与训练数据集D的关于特征&lt;code&gt;$A$&lt;/code&gt;的值的熵&lt;code&gt;$H_A(D)$&lt;/code&gt;之比:
 &lt;code&gt;$$ g_R(D, A) = \frac {g(D, A)} {H_A(D)} $$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这里的分母应该是
&lt;code&gt;$$ H_A(D) = - \sum_{i=1}^n \frac{\lvert D_i \rvert} { \lvert D \rvert} \log \frac{ \lvert D_i \rvert }{ \lvert D \rvert} $$&lt;/code&gt;
分子不再是&lt;code&gt;$C_i$&lt;/code&gt;了. 其中分母又被称为分裂信息量或内在信息（Intrinsic Information），可简单地理解为表示信息分支所需要的信息量, &lt;strong&gt;是将特征A的可选值作为划分(而不是类别)&lt;/strong&gt;，计算节点上样本总的信息熵。&lt;/p&gt;

&lt;h2 id=&#34;决策树的生成:0b17248df6960d4fcec23e1367068474&#34;&gt;决策树的生成&lt;/h2&gt;

&lt;h3 id=&#34;id3算法:0b17248df6960d4fcec23e1367068474&#34;&gt;ID3算法&lt;/h3&gt;

&lt;p&gt;ID3算法的&lt;strong&gt;核心是在决策树各个结点上应用信息增益准则选择特征,递归地构建决策树。&lt;/strong&gt;具体方法是:从根结点(root node)开始,对结点计算所有可能的特征的信息增益,选择信息增益最大的特征作为结点的特征,由该特征的不同取值建立子结点;再对子结点递归地调用以上方法,构建决策树;直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。&lt;strong&gt;ID3相当于用极大似然法进行概率模型的选择&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;输入:训练数据集D,特征集A, 阈值&lt;code&gt;$ \epsilon$&lt;/code&gt;;&lt;br /&gt;
输出:决策树T。&lt;br /&gt;
步骤:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;若D中所有实例属于同一类&lt;code&gt;$C_k$&lt;/code&gt;, 则T为单结点树, 并将类&lt;code&gt;$C_k$&lt;/code&gt;作为该结点的类标记, 返回T;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;若&lt;code&gt;$A= \varnothing $&lt;/code&gt;,则T为单结点树, 并将D中实例数最大的类&lt;code&gt;$C_k$&lt;/code&gt;作为该结点的类标记, 返回T;&lt;/li&gt;
&lt;li&gt;否则, 计算A中各特征对D的信息增益, 选择信息增益最大的特征&lt;code&gt;$A_g$&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;如果&lt;code&gt;$A_g$&lt;/code&gt;的信息增益小于阈值&lt;code&gt;$\epsilon$&lt;/code&gt; , 则置T为单结点树,并将D中实例数最大的类&lt;code&gt;$C_k$&lt;/code&gt; 作为该结点的类标记, 返回T;&lt;/li&gt;
&lt;li&gt;否则, 对&lt;code&gt;$A_g$&lt;/code&gt;的每一可能值&lt;code&gt;$a_i$&lt;/code&gt;, 依&lt;code&gt;$A_g=a_i$&lt;/code&gt;将D分割为若干非空子集&lt;code&gt;$D_i$&lt;/code&gt;, 将&lt;code&gt;$D_i$&lt;/code&gt;中实例数最大的类作为标记,构建子结点,由结点及其子结点构成树T, 返回T;&lt;/li&gt;
&lt;li&gt;对第i个子结点,以&lt;code&gt;$D_i$&lt;/code&gt;为训练集, 以&lt;code&gt;$A-\{A_g\}$&lt;/code&gt;为特征集, 递归地调用步(1)~步(5),得到子树&lt;code&gt;$T_i$&lt;/code&gt;, 返回&lt;code&gt;$T_i$&lt;/code&gt;。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;c4-5算法:0b17248df6960d4fcec23e1367068474&#34;&gt;C4.5算法&lt;/h3&gt;

&lt;p&gt;C4.5克服了ID3的2个缺点：&lt;br /&gt;
1. 用信息增益选择属性时偏向于选择分枝比较多的属性值(即取值多的属性), 所有C4,5采用信息增益比来选择特征。&lt;br /&gt;
2. ID3不能处理连续型的属性特征&lt;/p&gt;

&lt;p&gt;输入: 训练数据集D,特征集A,阈值&lt;code&gt;$\epsilon$&lt;/code&gt; ;&lt;br /&gt;
输出: 决策树T。&lt;br /&gt;
步骤:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果D中所有实例属于同一类&lt;code&gt;$C_k$&lt;/code&gt; , 则置T为单结点树, 并将&lt;code&gt;$C_k$&lt;/code&gt;作为该结点的类, 返回T;&lt;/li&gt;
&lt;li&gt;若&lt;code&gt;$A= \varnothing $&lt;/code&gt;,则T为单结点树, 并将D中实例数最大的类&lt;code&gt;$C_k$&lt;/code&gt;作为该结点的类标记, 返回T;&lt;/li&gt;
&lt;li&gt;否则, 计算A中各特征对D的信息增益, 选择信息增益最大的特征&lt;code&gt;$A_g$&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;如果&lt;code&gt;$A_g$&lt;/code&gt;的信息增益比小于阈值&lt;code&gt;$\epsilon$&lt;/code&gt; , 则置T为单结点树,并将D中实例数最大的类&lt;code&gt;$C_k$&lt;/code&gt; 作为该结点的类标记, 返回T;&lt;/li&gt;
&lt;li&gt;否则, 对&lt;code&gt;$A_g$&lt;/code&gt;的每一可能值&lt;code&gt;$a_i$&lt;/code&gt;, 依&lt;code&gt;$A_g=a_i$&lt;/code&gt;将D分割为若干非空子集$D_i$, 将$D_i$中实例数最大的类作为标记,构建子结点,由结点及其子结点构成树T, 返回T;&lt;/li&gt;
&lt;li&gt;对第i个子结点, 以$D_i$为训练集, 以$A-{A_g}$为特征集, 递归地调用步(1)~步(5),得到子树$T_i$, 返回$T_i$。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;连续型特征的处理:0b17248df6960d4fcec23e1367068474&#34;&gt;连续型特征的处理&lt;/h4&gt;

&lt;p&gt;先把连续属性转换为离散属性再进行处理。虽然本质上属性的取值是连续的，但对于有限的采样数据它是离散的，如果有N条样本，那么我们有N-1种离散化的方法：$&amp;lt;=v_j$的分到左子树，$&amp;gt;v_j$的分到右子树。计算这N-1种情况下最大的信息增益比。&lt;br /&gt;
在离散属性上只需要计算1次信息增益率，而在连续属性上却需要计算N-1次，计算量是相当大的。可以如下减少计算量: 对于连续属性先进行排序，&lt;strong&gt;只在决策属性(即类别)发生改变的地方进行切分&lt;/strong&gt;。&lt;br /&gt;
如果利用增益率来选择连续值属性的分界点，会导致一些副作用。分界点将样本分成两个部分，这两个部分的样本个数之比也会影响增益率。根据增益率公式，我们可以发现，当分界点能够把样本分成数量相等的两个子集时（我们称此时的分界点为等分分界点），增益率的抑制会被最大化(此时分母$H(D)$增大, 增益率减小)，因此等分分界点被过分抑制了。子集样本个数能够影响分界点，显然不合理。&lt;strong&gt;因此在决定分界点是还是采用增益这个指标，而选择属性的时候才使用增益率这个指标。这个改进能够很好得抑制连续值属性的倾向。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;设训练数据集为D, $|D|$ 表示其样本容量,即样本个数。设有K个类$C&lt;em&gt;k,k=1,2, \cdots ,K$, $|C_k|$为属于类$C_k$的样本个数 。设特征A有n个不同的取值${a_1,a_2, \cdots, a_n}$, 根据特征A的取值将D划分为n个子集$D_1,D_2, \cdots ,D_n$, $|D_i|$为$D_i$的样本个数 。记子集$D_i$中属于类$C_k$的样本的集合为$D&lt;/em&gt;{ik}$, 即$D&lt;em&gt;{ik} = D_i \cap C_k$, $|D&lt;/em&gt;{ik}|$为$D_{ik}$的样本个数。于是信息增益的算法如下:&lt;br /&gt;
输入:训练数据集D和特征A;
输出:特征A对训练数据集D的信息增益$g(D,A)$。&lt;/p&gt;

&lt;h4 id=&#34;属性缺失值的处理:0b17248df6960d4fcec23e1367068474&#34;&gt;属性缺失值的处理&lt;/h4&gt;

&lt;p&gt;现实任务中常会遇到不完整的样本, 即样本的某些属性值缺失. 如果简单的放弃不完整样本, 仅使用无缺失值的样本来进行学习, 显然是对数据的极大浪费.&lt;br /&gt;
两个问题:
1. 如何在属性值缺失的情况下进行划分属性的选择?
2. 给定划分属性, 若样本在该属性上的值缺失, 如何对样本进行划分?&lt;/p&gt;

&lt;p&gt;给定训练集$D$和属性$A$, 特征A有V个不同的取值${a&lt;em&gt;1,a_2, \cdots, a_V}$, 令$ \tilde{D} $表示$D$中在属性$A$上没有缺失值的样本子集. 对问题(1), 根据$ \tilde{D} $来判断属性$A$的优劣. 令$  \tilde{D&lt;/em&gt;{v}}  $为$ \tilde{D} $中在属性$A$上取值为$a&lt;em&gt;v$的样本子集,  $  \tilde{D^{k}}  $为$ \tilde{D} $中在属于第$k$类的样本子集. $ \tilde{D}&lt;/em&gt;{ik} $表示在属性$A$上取值为$a&lt;em&gt;v$的, 且属于第$k$类的样本子集, 则有 $  \tilde{D} =  \bigcup _{k=1} ^K \tilde{D^k}$, $  \tilde{D} =  \bigcup _{v=1} ^V \tilde{D_v}$. 假设我们为每个样本$x$ 赋予一个权重$w_x$(一般初始化为1), 并定义
$$
\rho = \frac{ \sum&lt;/em&gt;{x \in  \tilde{D}} w&lt;em&gt;x } { \sum&lt;/em&gt;{x \in  D} w&lt;em&gt;x  }  , \
\tilde{p}_k = \frac{ \sum&lt;/em&gt;{x \in  \tilde{D&lt;em&gt;k}} w_x } { \sum&lt;/em&gt;{x \in  D} w&lt;em&gt;x  }  , \
\tilde{r}_v = \frac{ \sum&lt;/em&gt;{x \in  \tilde{D^v}} w&lt;em&gt;x } { \sum&lt;/em&gt;{x \in  D} w&lt;em&gt;x  }  , \
$$&lt;br /&gt;
对属性$A$, $\rho$表示无缺失值样本所占比例, $\tilde{p}_k$表示无缺失值样本中第$k$类所占比例, $\tilde{r}_v$表示无缺失值样本中在属性$A$上取值$a_v$的样本所占比例.&lt;br /&gt;
基于上述定义, 将信息增益推广为:
 $$  g(D, A) = \rho \ g(\tilde{D}, A) = \rho \ \left(H(\tilde{D}) - H(\tilde{D} \mid A) \right)$$
$$
\begin{align*}
H(\tilde{D} \mid A) &amp;amp;= \sum&lt;/em&gt;{i=1}^V \frac{\lvert \tilde{D}&lt;em&gt;i\rvert} { \lvert \tilde{D} \rvert}  H(\tilde{D}_i)   \
&amp;amp;=  \sum&lt;/em&gt;{i=1}^V \frac{\lvert \tilde{D}&lt;em&gt;i\rvert} { \lvert \tilde{D} \rvert}  \sum&lt;/em&gt;{k=1}^K  \frac{\lvert \tilde{D}&lt;em&gt;{ik}\rvert} { \lvert \tilde{D}_i \rvert}  \log \frac{\lvert \tilde{D}&lt;/em&gt;{ik}\rvert} { \lvert \tilde{D}_i \rvert}
\end{align*}
$$&lt;br /&gt;
对问题(2), 若样本$x$在划分属性$A$上的取值已知, 则将$x$划入与其取值对应的子结点, 且样本权值在子节点中保持为$w_x$. 若样本$x$在划分属性是的取值未知, 则将$x$同时划分到所有子节点, 且样本取值在与属性值$a_v$对应的子节点中调整为$\tilde{r}_v  \cdot w_x$; 直观的看, 就是让同一样本以不同的概率划入不同的子节点中取.&lt;/p&gt;

&lt;h4 id=&#34;决策树的剪枝-后剪枝:0b17248df6960d4fcec23e1367068474&#34;&gt;决策树的剪枝(后剪枝)&lt;/h4&gt;

&lt;p&gt;决策树生成算法递归地产生决策树,直到不能继续下去为止。这样产生的树往往出现&lt;strong&gt;过拟合&lt;/strong&gt;现象。解决这个问题的办法是考虑决策树的复杂度,对已生成的决策树进行简化。&lt;br /&gt;
将已生成的树进行简化的过程称为&lt;strong&gt;剪枝(pruning)&lt;/strong&gt;。具体地,剪枝从已生成的树上裁掉一些子树或叶结点,并将其根结点或父结点作为新的叶结点,从而简化分类树模型。&lt;/p&gt;

&lt;p&gt;决策树的剪枝往往通过&lt;strong&gt;极小化决策树整体的损失函数&lt;/strong&gt;(loss function)或代价函数(cost function)来实现。设树T的叶结点个数为$|T|$, $t$是树$T$的叶结点, 该叶结点有$N&lt;em&gt;t$个样本点, 其中k类的样本点有$N&lt;/em&gt;{tk}$个, $k=1,2, \cdots ,K$, $H&lt;em&gt;t(T)$为叶结点t上的经验熵, $\alpha≥0$为参数,则决策树学习的损失函数可以定义为
$$ C&lt;/em&gt;{\alpha} (T) = \sum&lt;em&gt;{t=1}^{|T|} N_t H_t(T) + \alpha \lvert T \rvert  $$
其中经验熵为
$$ H_t(T) = - \sum&lt;/em&gt;{k=1}^K \frac{N&lt;em&gt;{tk}} {N_t} \log  \frac{N&lt;/em&gt;{tk}} {N&lt;em&gt;t} $$
在损失函数中,将式右端的第1项记作
$$ C(T) =  \sum&lt;/em&gt;{t=1}^{|T|} N&lt;em&gt;t H_t(T) = - \sum&lt;/em&gt;{t=1}^{|T|} \sum&lt;em&gt;{k=1}^K N&lt;/em&gt;{tk} \log \frac{N&lt;em&gt;{tk}} {N_t}   $$
这时有:
$$ C&lt;/em&gt;{\alpha} (T) = C(T) + \alpha  \lvert T \rvert  $$
$C(T)$表示模型对训练数据的预测误差, 即模型与训练数据的拟合程度, $|T|$表示模型复杂度(正则项), 参数$\alpha≥0$控制两者之间的影响。较大的$\alpha$促使选择较简单的模型(树), 较小的$\alpha$促使选择较复杂的模型(树)。$\alpha=0$意味着只考虑模型与训练数据的拟合程度, 不考虑模型的复杂度。&lt;br /&gt;
剪枝,就是当$\alpha$确定时,选择损失函数最小的模型,即损失函数最小的子树。可以看出,决策树生成只考虑了通过提高信息增益(或信息增益比)对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。&lt;strong&gt;决策树生成学习局部的模型,而决策树剪枝学习整体的模型。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;损失函数的极小化等价于正则化的极大似然估计&lt;/strong&gt;。所以,利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;树的剪枝算法(最小误差剪枝)&lt;/strong&gt;&lt;br /&gt;
输入: 生成算法产生的整个树$T$, 参数$\alpha$;
输出: 修剪后的子树$T&lt;em&gt;{\alpha}$。
1. 计算每个结点的经验熵。
2. 递归地从树的叶结点向上回缩。
设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$, 其对应的损失函数值分别$C&lt;/em&gt;{\alpha}(T&lt;em&gt;B)$与$C&lt;/em&gt;{\alpha}(T&lt;em&gt;A)$, 如果
$$ C&lt;/em&gt;{\alpha}(T&lt;em&gt;B) \ge C&lt;/em&gt;{\alpha}(T&lt;em&gt;A) $$
则进行剪枝,即将父结点变为新的叶结点。&lt;br /&gt;
3. 返回(2), 直至不能继续为止, 得到损失函数最小的子树$T&lt;/em&gt;{\alpha}$。&lt;br /&gt;
上式只需考虑两个树的损失函数的差,其计算可以在局部进行。所以,决策树的剪枝算法可以由一种动态规划的算法实现。&lt;/p&gt;

&lt;h4 id=&#34;决策树的剪枝-预剪枝:0b17248df6960d4fcec23e1367068474&#34;&gt;决策树的剪枝(预剪枝)&lt;/h4&gt;

&lt;p&gt;预剪枝是指在决策树生成过程中, 对每个节点在划分前先进行估计, 若当前节点的划分不能带来&lt;strong&gt;决策树泛化能力&lt;/strong&gt;的提升, 则停止划分并将当前节点标记为叶节点.&lt;br /&gt;
&lt;strong&gt;通过在验证集上的准确率来进行判断是否剪枝.&lt;/strong&gt; 分别计算划分前后的准确率, 若划分后准确率下降, 则不进行该次划分, 直接标记该节点为叶节点, 所属类别为节点中实例数最大的类.&lt;br /&gt;
预剪枝使得很多分支没有&amp;rdquo;展开&amp;rdquo;, 这不仅降低了过拟合的风险, 还显著降低了决策树的训练时间和预测时间开销. 但另一方面, 有些分支的当前划分虽不能提升泛化性能,甚至可能导致泛化性能暂时下降, 但在其基础上进行的后续划分却有可能导致性能显著提高; 预剪纸基于&amp;rdquo;贪心&amp;rdquo;本质禁止这些分支展开, 给决策树带来了欠拟合的风险.&lt;/p&gt;

&lt;p&gt;上面的后剪纸也可以采用这种通过验证集准确率决定是否剪纸的策略.&lt;/p&gt;

&lt;h4 id=&#34;cart算法:0b17248df6960d4fcec23e1367068474&#34;&gt;CART算法&lt;/h4&gt;

&lt;p&gt;CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。&lt;strong&gt;CART假设决策树是二叉树&lt;/strong&gt;, &lt;strong&gt;内部结点特征的取值为“是”和“否&lt;/strong&gt;”,左分支是取值为“是”的分支,右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征,将输入空间即特征空间划分为有限个单元,并在这些单元上确定预测的概率分布,也就是在输入给定的条件下输出的条件概率分布。&lt;/p&gt;

&lt;p&gt;CART算法由以下两步组成:
(1)决策树生成:基于训练数据集生成决策树,&lt;strong&gt;生成的决策树要尽量大&lt;/strong&gt;;
(2)决策树剪枝:用&lt;strong&gt;验证数据集&lt;/strong&gt;对已生成的树进行剪枝并选择最优子树,这时用损失函数最小作为剪枝
的标准。&lt;/p&gt;

&lt;h4 id=&#34;cart的生成:0b17248df6960d4fcec23e1367068474&#34;&gt;CART的生成&lt;/h4&gt;

&lt;p&gt;决策树的生成就是递归地构建&lt;strong&gt;二叉决策树&lt;/strong&gt;的过程。对&lt;strong&gt;回归树用平方误差最小化准则&lt;/strong&gt;, 对&lt;strong&gt;分类树用基尼指数(Gini  index)最小化准则&lt;/strong&gt;,进行特征选择,生成二叉树。&lt;/p&gt;

&lt;h4 id=&#34;回归树的生成:0b17248df6960d4fcec23e1367068474&#34;&gt;回归树的生成&lt;/h4&gt;

&lt;p&gt;假设$X$与$Y$分别为输入和输出变量, 并且$Y$是&lt;strong&gt;连续变量&lt;/strong&gt;,给定训练数据集考虑如何生成回归树.
$$ D = {  (x&lt;em&gt;1, y_1), (x_2, y_2), \cdots, (x_N, y_N)  } $$&lt;br /&gt;
&lt;strong&gt;一个回归树对应着输入空间(即特征空间)的一个划分以及在划分的单元上的输出值&lt;/strong&gt;。假设已将输入空间划分为$M$个单元$R_1,R2, \cdots ,R_M$, 并且在每个单元$R_m$上有一个固定的输出值$c_m$,于是回归树模型可表示为
$$ f(x) = \sum&lt;/em&gt;{m=1}^M c&lt;em&gt;m I(x \in R_m) $$&lt;br /&gt;
当输入空间的划分确定时,可以用&lt;strong&gt;平方误差&lt;/strong&gt;$ \sum&lt;/em&gt;{x&lt;em&gt;i \in R_m} (y_i - f(x_i))^2 $来表示回归树对于训练数据的预测误差, 用&lt;strong&gt;平方误差最小的准则求解每个单元上的最优输出值&lt;/strong&gt;。易知,单元$R_m$上的$c_m$的最优值$\hat{c}_m$是$R_m$上的所有输入实例$x_i$对应的输出$y_i$的均值,即
$$ \hat{c}_m = avg (y_i \mid x_i \in R_m) $$&lt;br /&gt;
采用启发式的方法样对输入空间进行划分, 选择第$j$个变量$x^{(j)}$和它取的值$s$作为切分变量(splitting variable)和切分点(splitting  point), 并定义两个区域:
$$ R_1(j, s) = { x \mid x^{(j)} \le s }  \quad  和   \quad R_2(j, s) = { x \mid x^{(j)} &amp;gt; s }  $$
然后寻找最优切分变量j和最优切分点s。具体地,求解&lt;br /&gt;
$$  \min \limits&lt;/em&gt;{j, s} \left[ \min \limits&lt;em&gt;{c_1} \sum&lt;/em&gt;{x&lt;em&gt;i \in R_1(j, s)} (y_i - c_1)^2 + \min \limits&lt;/em&gt;{c&lt;em&gt;2} \sum&lt;/em&gt;{x&lt;em&gt;i \in R_2(j, s)} (y_i - c_2)^2  \right] $$&lt;br /&gt;
对固定输入变量$j$可以找到最优切分点$s$。
$$  \hat{c}_1 = avg( y_i \mid x_i \in R_1(j, s) )  \quad 和 \quad   \hat{c}_2 = avg( y_i \mid x_i \in R_2(j, s) )$$&lt;br /&gt;
遍历所有输入变量, 找到最优的切分变量$j$, 构成一个对$(j,s)$。依此将输入空间划分为两个区域。接着,对每个区域重复上述划分过程, 直到满足停止条件为止。这样就生成一棵回归树。这样的回归树通常称为&lt;strong&gt;最小二乘回归树(least   squares regression tree)&lt;/strong&gt;,现将算法叙述如下:&lt;br /&gt;
输入: 训练数据集$D$;&lt;br /&gt;
输出: 回归树$f(x)$。&lt;br /&gt;
在训练数据集所在的输入空间中, 递归地将每个区域划分为两个子区域并决定每个子区域上的输出值,构建二叉决策树:&lt;br /&gt;
1. 选择最优切分变量$j$与切分点$s$, 求解&lt;br /&gt;
$$  \min \limits&lt;/em&gt;{j, s} \left[ \min \limits&lt;em&gt;{c_1} \sum&lt;/em&gt;{x&lt;em&gt;i \in R_1(j, s)} (y_i - c_1)^2 + \min \limits&lt;/em&gt;{c&lt;em&gt;2} \sum&lt;/em&gt;{x&lt;em&gt;i \in R_2(j, s)} (y_i - c_2)^2  \right] $$&lt;br /&gt;
遍历变量$j$, 对固定的切分变量$j$扫描切分点$s$, 选择使上式达到最小值的对$(j,s)$。&lt;br /&gt;
2. 用选定的对$(j,s)$划分区域并决定相应的输出值:&lt;br /&gt;
$$ R_1(j, s) = { x \mid x^{(j)} \le s }  \  ,   \quad R_2(j, s) = { x \mid x^{(j)} &amp;gt; s }   \
\hat{c}_m = \frac{1} {N_m} \sum&lt;/em&gt;{x&lt;em&gt;i \in R_m(j, s)} y_i \ , \quad x \in R_m, \ m=1,2
$$&lt;br /&gt;
3. 继续对两个子区域调用步骤(1),(2),直至满足停止条件。&lt;br /&gt;
4. 将输入空间划分为$M$个区域$R_1,R_2, \cdots, R_m$,生成决策树:&lt;br /&gt;
$$ f(x) = \sum&lt;/em&gt;{m=1}^M \hat{c}_m I(x \in R_m) $$&lt;/p&gt;

&lt;h4 id=&#34;分类树的生成:0b17248df6960d4fcec23e1367068474&#34;&gt;分类树的生成&lt;/h4&gt;

&lt;p&gt;分类树用基尼指数选择最优特征,同时决定该特征的最优二值切分点。&lt;/p&gt;

&lt;h4 id=&#34;基尼指数:0b17248df6960d4fcec23e1367068474&#34;&gt;基尼指数&lt;/h4&gt;

&lt;p&gt;分类问题中,假设有$K$个类, 样本点属于第$k$类的概率为$p$ , 则概率分布的基尼指数$k$定义为&lt;br /&gt;
$$ Gini(p) = 1 -  \sum_{k=1}^K \left(  \frac{ \lvert C_k \rvert  } { \lvert  D \rvert  }  \right) $$&lt;br /&gt;
这里$C_k$是$D$中属于第$k$类的样本子集, $K$是类的个数.&lt;br /&gt;
如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分,即&lt;br /&gt;
$$ D_1 = {  (x, y) \in D \mid A(x) = a } \ , \quad D_2 = D - D_1  $$&lt;br /&gt;
则在特征A的条件下, 集合$D$的基尼指数定义为&lt;br /&gt;
$$  Gini(D, A) =  \frac{ \lvert D_1 \rvert  } { \lvert  D \rvert  } Gini(D_1) + \frac{ \lvert D_2 \rvert  } { \lvert  D \rvert  } Gini(D_2)$$&lt;br /&gt;
基尼指数$Gini(D)$表示集合$D$的不确定性, 基尼指数$Gini(D,A)$表示经$A=a$分割后集合$D$的不确定性。&lt;strong&gt;基尼指数值越大,样本集合的不确定性也就越大,这一点与熵相似。&lt;/strong&gt;&lt;br /&gt;
二类分类问题中基尼指数$Gini(p)$、熵(单位比特)之半$\frac{1}{2}H(p)$和分类误差率的关系&lt;br /&gt;
&lt;img src=&#34;./1467279391154.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;br /&gt;
基尼指数和熵之半的曲线很接近,都可以近似地代表分类误差率。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CART生成算法&lt;/strong&gt;
输入: 训练数据集$D$, 停止计算的条件;&lt;br /&gt;
输出: CART决策树。&lt;br /&gt;
根据训练数据集,从根结点开始,递归地对每个结点进行以下操作,构建二叉决策树:&lt;br /&gt;
1. 设结点的训练数据集为$D$, 计算现有特征对该数据集的基尼指数。此时,对每一个特征$A$,对其可能取的每个值$a$, 根据样本点对$A=a$的测试为“是”或“否”将$D$分割成$D_1$和$D_2$两部分, 计算$A=a$时的基尼指数$Gini(D, A)$。&lt;br /&gt;
2. 在所有可能的特征$A$以及它们所有可能的切分点$a$中, &lt;strong&gt;选择基尼指数最小的特征及其对应的切分点&lt;/strong&gt;作为最优特征与最优切分点. 依最优特征与最优切分点,从现结点生成两个子结点,将训练数据集依特征分配到两个子结点中去。&lt;br /&gt;
3. 对两个子结点递归地调用(1),(2),直至满足停止条件。
4. 生成CART决策树。&lt;br /&gt;
&lt;strong&gt;算法停止计算的条件&lt;/strong&gt;是结点中的样本个数小于预定阈值, 或样本集的基尼指数小于预定阈值(样本基本属于同一类),或者没有更多特征。&lt;/p&gt;

&lt;h4 id=&#34;cart剪枝:0b17248df6960d4fcec23e1367068474&#34;&gt;CART剪枝&lt;/h4&gt;

&lt;p&gt;CART剪枝算法由两步组成:首先从生成算法产生的决策树$T_0$底端开始不断剪枝, 直到$T_0$的根结点,形成一个子树序列${T_0,T_1, \cdots ,T_n}$; 然后通过交叉验证法在独立的验证数据集上对子树序列进行测试,从中选择最优子树。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;剪枝,形成一个子树序列
在剪枝过程中,计算子树的损失函数:
$$ C&lt;em&gt;\alpha (T) = C(T) + \alpha \lvert T \rvert $$&lt;br /&gt;
其中, $T$为任意子树, $C(T)$为对训练数据的预测误差(如基尼指数, 平方误差), $ \lvert T \rvert  $为子树的叶子点个数, $\alpha \ge 0$为参数, $C&lt;/em&gt;\alpha (T)  $为参数是$\alpha$时的子树$T$的整体损失。参数$\alpha$权衡训练数据的拟合程度与模型的复杂度。&lt;br /&gt;
对固定的$\alpha$, 一定存在使损失函数$C&lt;em&gt;\alpha (T)  $最小的子树, 将其表示为$T&lt;/em&gt;\alpha$。$T&lt;em&gt;\alpha$在损失函数$C&lt;/em&gt;\alpha (T)  $最小的意义下是最优的.&lt;br /&gt;
从整体树$T&lt;em&gt;0$开始剪枝。对$T_0$的任意内部结点$t$, 以$t$为单结点树的损失函数是
$$ C&lt;/em&gt;\alpha (T) = C(t)  + \alpha \ , \ (\lvert t \rvert  = 1) $$&lt;br /&gt;
以$t$为根结点的子树$T&lt;em&gt;t$的损失函数是
$$ C&lt;/em&gt;\alpha (T&lt;em&gt;t) = C(T_t)  + \alpha \lvert T_t \rvert$$&lt;br /&gt;
当$\alpha=0$及$\alpha$充分小时, 有不等式
$$ C&lt;/em&gt;\alpha (T&lt;em&gt;t)  &amp;lt; C&lt;/em&gt;\alpha (t)   $$&lt;br /&gt;
当$\alpha$增大时, 在某一$\alpha$有&lt;br /&gt;
$$ C&lt;em&gt;\alpha (T_t)  =  C&lt;/em&gt;\alpha (t)   $$&lt;br /&gt;
当$\alpha$再增大时,不等式反向。只要$ \alpha = \frac {C(t) - C(T_t)} {\lvert T_t \rvert -1}$, $T_t$与$t$有相同的损失函数值,  而$t$的结点少, 因此$t$比$T_t$更可取, 对$T_t$进行剪枝。&lt;br /&gt;
为此, 对$T_0$的每一内部结点t, 计算
$$ g(t) = \frac {C(t) - C(T_t)} {\lvert T_t \rvert -1}$$&lt;br /&gt;
&lt;strong&gt;它表示剪枝后整体损失函数减少的程度&lt;/strong&gt;。在$T _0$中剪去$g(t)$最小的$T_t$ , 将得到的子树作为$T_1$ ,同时将最小的$g(t)$设为$\alpha_1$。$T_1$为区间$[\alpha_1,\alpha_2)$的最优子树。&lt;br /&gt;
如此剪枝下去,直至得到根结点。在这一过程中,不断地增加$\alpha$的值,产生新的区间。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在剪枝得到的子树序列$T&lt;em&gt;0,T_1,\cdots,T_n$中通过交叉验证选取最优子树$T_a$&lt;br /&gt;
具体地, &lt;strong&gt;利用独立的验证数据集&lt;/strong&gt;, 测试子树序列$T_0,T_1,\cdots,T_n$中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中,每棵子树$T_0,T_1,\cdots,T_n$都对应于一个参数$\alpha_1, \alpha_2,\cdots, \alpha_n$。所以, 当最优子树$T_k$确定时, 对应的$\alpha_k$也确定了, 即得到最优决策树$T&lt;/em&gt;\alpha$。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;CART剪枝算法:&lt;/strong&gt;&lt;br /&gt;
输入: CART算法生成的决策树$T&lt;em&gt;0$;&lt;br /&gt;
输出: 最优决策树$T&lt;/em&gt;\alpha$&lt;br /&gt;
1. 设$k=0,\ T=T&lt;em&gt;0$.
2. 设$ \alpha = + \infty$
3. 自下而上地对各内部结点$t$计算$C(T_t)$, $ \lvert T_t \rvert $以及
$$ g(t) = \frac {C(t) - C(T_t)} {\lvert T_t \rvert -1} \
\alpha = \min \left(\alpha, g(t) \right)
$$&lt;br /&gt;
这里,$T_t$表示以$t$为根结点的子树, $C(T_t)$是对训练数据的预测误差, $\lvert T_t \rvert $是$T_t$的叶结点个数。&lt;br /&gt;
4. 对$g(t)=\alpha$的内部节点$t$进行剪枝, 并对叶结点$t$以多数表决法决定其类,得到树$T$。&lt;br /&gt;
5. 设$k=k+1, \alpha_k=\alpha, T_k=T$。&lt;br /&gt;
6. 如果$T_k$不是由根结点及两个叶结点构成的树, 则回到步骤(3), 否则令$T_k = T_n$。
7. 采用交叉验证法在子树序列$T_0,T_1,\cdots,T_n$中选取最优子树$T&lt;/em&gt;\alpha$。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>统计学习方法读书笔记(4)-朴素贝叶斯算法</title>
      <link>http://blog.songru.org/posts/machine-learning/statical-learning-4-Naive-bayes/</link>
      <pubDate>Tue, 07 Jul 2015 10:34:24 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/machine-learning/statical-learning-4-Naive-bayes/</guid>
      <description>

&lt;p&gt;朴素贝叶斯(naïve Bayes)法是基于&lt;strong&gt;贝叶斯定理与特征条件独立假设&lt;/strong&gt;的分类方法。对于给定的训练数据集,首先基于特征条件独立假设学习输入/输出的&lt;strong&gt;联合概率分布&lt;/strong&gt;;然后基于此模型,对给定的输入x,利用贝叶斯定理求出后验概率最大的输出y。&lt;/p&gt;

&lt;h1 id=&#34;定义:099c9edb24a218621028d97385cd3c12&#34;&gt;定义:&lt;/h1&gt;

&lt;p&gt;设输入空间&lt;code&gt;$\mathcal{X}⊆R^n$&lt;/code&gt;为n维向量的集合,输出空间为类标记集合&lt;code&gt;$ \mathcal{Y} = \{c_1,c_2, \cdots ,c_K\}$&lt;/code&gt;。输入为特征向量&lt;code&gt;$x \in \mathcal{X}$&lt;/code&gt;,输出为类标记(class label)&lt;code&gt;$y \in \mathcal{Y}$&lt;/code&gt;。&lt;code&gt;$X$&lt;/code&gt;是定义在输入空间&lt;code&gt;$\mathcal{X}$&lt;/code&gt;上的随机向量,&lt;code&gt;$Y$&lt;/code&gt;是定义在输出空间&lt;code&gt;$\mathcal{Y}$&lt;/code&gt;上的随机变量. &lt;code&gt;$P(X,Y)$&lt;/code&gt;是&lt;code&gt;$X$&lt;/code&gt;和&lt;code&gt;$Y$&lt;/code&gt;的联合概率分布。&lt;/p&gt;

&lt;p&gt;训练数据集&lt;code&gt;$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$&lt;/code&gt;,由&lt;code&gt;$P(X,Y)$&lt;/code&gt;独立同分布产生。&lt;br /&gt;
其中&lt;code&gt;$x_i = (x_i^{(1)}, x_i^{(2)}, \cdots , x_i^{(n)})^T$&lt;/code&gt;, &lt;code&gt;$x_i^{(j)}$&lt;/code&gt;是第i个样本的第j个特征, &lt;code&gt;$x_i^{(j)} \in \{a_{j1},a_{j2}, \cdots ,a_{jS_j} \}$&lt;/code&gt;, &lt;code&gt;$a_{jl}$&lt;/code&gt;是第j个特征可能取的第l个值, &lt;code&gt;$j=1,2, \cdots ,n$&lt;/code&gt; , &lt;code&gt;$l=1,2, \cdots ,S_j $&lt;/code&gt;, &lt;code&gt;$y_i \in \{c_1,c_2, \cdots ,c_K\} $&lt;/code&gt;;&lt;/p&gt;

&lt;p&gt;朴素贝叶斯法通过训练数据集学习联合概率分布&lt;code&gt;$P(X,Y)$&lt;/code&gt;, 再根据贝叶斯公式
&lt;code&gt;$$ P(Y \mid X) = \frac{P(X, Y)}{P(X)}  = \frac {P(X \mid Y) P(Y) }  {\sum \limits_Y P(X \mid Y) P(Y)}$$&lt;/code&gt;
求出后验概率。&lt;/p&gt;

&lt;p&gt;朴素贝叶斯法分类时,对给定的输入x,通过学习到的模型计算后验概率分布&lt;code&gt;$P(Y=c_k|X=x)$&lt;/code&gt;,将后验概率最大的类作为x的类输出。后验概率计算根据贝叶斯公式进行:
&lt;code&gt;$$ 
\begin{align}
P(Y=c_K \mid X = x) &amp;amp;= \frac{P(X = x, Y = c_k)}{P(X = x)}  \\ 
&amp;amp;= \frac {P(X = x \mid Y = c_k) P(Y = c_k) }  {\sum_{k=1}^K P(X = x \mid Y = c_k) P(Y = c_k)}
\end{align}
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;具体地,需要学习以下先验概率分布&lt;code&gt;$P(Y=c_k)$&lt;/code&gt;及条件概率分布&lt;code&gt;$P(X = x \mid Y = c_k)$&lt;/code&gt;。&lt;/p&gt;

&lt;h1 id=&#34;求解方法-极大似然估计方法:099c9edb24a218621028d97385cd3c12&#34;&gt;求解方法, 极大似然估计方法:&lt;/h1&gt;

&lt;p&gt;先验概率分布
&lt;code&gt;$$ P(Y=c_K) = \frac{\sum_{i=1}^{N} I(y_i = c_k)} {N} , \quad k=1,2,\cdots,K $$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;条件概率分布:
&lt;code&gt;$$  P(X=x \mid Y = c_k) = P(X^{(1)} = x_{(1)}, \cdots, X^{(n)} = x_{(n)} \mid Y=c_k) \quad  k=1,2,\cdots,K $$&lt;/code&gt;
首先朴素贝叶斯法对条件概率分布作了&lt;strong&gt;条件独立性的假设&lt;/strong&gt;。由于这是一个较强的假设,朴素贝叶斯法也由此得名。具体地,条件独立性假设是
&lt;code&gt;$$
\begin{align}  
  P(X=x \mid Y=c_k)  &amp;amp;= P(X^{(1)} = x_{(1)}, \cdots, X^{(n)} = x_{(n)} \mid Y=c_k)    \\  
             &amp;amp;= \prod_{j=1} ^n P( X^{(j)} = x^{(j)} \mid Y = c_k )  \\
  P(X^{(j)} = a_{jl} \mid Y = c_k) &amp;amp;=  \frac {\sum_{i=1}^N I(x_i^{(j)} = a_{jl}, y_i = c_k) } {\sum_{i=1}^N I(y_i = c_k)}
\end{align} 
$$&lt;/code&gt;
&lt;strong&gt;条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的&lt;/strong&gt;。这一假设使朴素贝叶斯法变得简单,但有时会牺牲一定的分类准确率。&lt;/p&gt;

&lt;p&gt;最终后验概率为:
&lt;code&gt;$$ P(Y = c_k \mid X = x)  =  \frac{P(Y = c_k) \prod_{j=1}^n P(X^{(j)} = x^{(j)} \mid Y = c_k)}  { \sum_{k=1}^K \left( P(Y = c_k) \prod_{j=1}^n P(X^{(j)} = x^{(j)} \mid Y = c_k) \right) }  \quad \text{(K为类别个数, n为特征个数)}$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;朴素贝叶斯分类器可表示为
&lt;code&gt;$$ y = f(x) = \arg \max_{c_k} \frac{P(Y = c_k) \prod_{j=1}^n P(X^{(j)} = x^{(j)} \mid Y = c_k)}  { \sum_{k=1}^K \left( P(Y = c_k) \prod_{j=1}^n P(X^{(j)} = x^{(j)} \mid Y = c_k) \right) } $$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;其中分母对所有$c_k$都是相同的,所以
&lt;code&gt;$$ y = f(x) = \arg \max_{c_k} P(Y = c_k) \prod_{j=1}^n P(X^{(j)} = x^{(j)} \mid Y = c_k) $$&lt;/code&gt;
朴素贝叶斯法实际上学习到生成数据的机制,所以&lt;strong&gt;属于生成模型&lt;/strong&gt;。&lt;/p&gt;

&lt;h1 id=&#34;贝叶斯估计-平滑:099c9edb24a218621028d97385cd3c12&#34;&gt;贝叶斯估计(平滑)&lt;/h1&gt;

&lt;p&gt;用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果,使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。具体地,条件概率的贝叶斯估计是
&lt;code&gt;$$  P(X^{(j)} = a_{jl} \mid Y = c_k) =  \frac {\sum_{i=1}^N I(x_i^{(j)} = a_{jl}, y_i = c_k) + \lambda} {\sum_{i=1}^N I(y_i = c_k) + S_j \lambda}  $$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;式中&lt;code&gt;$ \lambda ≥0$&lt;/code&gt;。等价于在随机变量各个取值的频数上赋予一个正数&lt;code&gt;$ \lambda&amp;gt;0$&lt;/code&gt;。当&lt;code&gt;$ \lambda=0$&lt;/code&gt;时就是极大似然估计。常取&lt;code&gt;$ \lambda = 1$&lt;/code&gt;, 这时称为拉普拉斯平滑(Laplace smoothing)。&lt;/p&gt;

&lt;h1 id=&#34;后验概率最大化的含义-不是很懂:099c9edb24a218621028d97385cd3c12&#34;&gt;后验概率最大化的含义(不是很懂)&lt;/h1&gt;

&lt;p&gt;朴素贝叶斯法将实例分到后验概率最大的类中。这&lt;strong&gt;等价于期望风险最小化&lt;/strong&gt;。假设选择0-1损失函数:
&lt;code&gt;$$ L(Y, f(X)) = \begin{cases}
                1, &amp;amp; Y \ne f(X) \\
                0, &amp;amp; Y = f(X)
                \end{cases}
$$&lt;/code&gt;
式中&lt;code&gt;$f(X)$&lt;/code&gt;是分类决策函数。这时,期望风险函数为
&lt;code&gt;$$ R_{exp}(f) = E [L(Y, f(X))] $$&lt;/code&gt;
期望是对联合分布$P(X,Y)$取的。由此取条件期望
&lt;code&gt;$$ R_{exp}(f) = E_X \sum_{k=1}^K[ L(c_k, f(X)) ] P(c_k \mid X)$$&lt;/code&gt;
为了使期望风险最小化,只需对&lt;code&gt;$X=x$&lt;/code&gt;逐个极小化,由此得到:
&lt;code&gt;$$
\begin{align}  
  f(x) &amp;amp;= \arg \max_{y \in \mathcal{Y}} \sum_{k=1}^K L(c_k, y) P(c_k \mid X = x)  \\
  &amp;amp;= \arg \max_{y \in \mathcal{Y}} \sum_{k=1}^K P(y \ne  c_k \mid X =x ) \\
  &amp;amp;= \arg \min_{y \in \mathcal{Y}} (1 - P(y = c_k \mid X =x)) \\
  &amp;amp;= \arg \max_{y \in \mathcal{Y}} P(y = c_k \mid X = x)
\end{align} 
$$&lt;/code&gt;
这样一来,根据期望风险最小化准则就得到了后验概率最大化准则:
&lt;code&gt;$$ f(x) = \arg \max_{c_k} P( y = c_k \mid X = x)  $$&lt;/code&gt;
即朴素贝叶斯法所采用的原理。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>