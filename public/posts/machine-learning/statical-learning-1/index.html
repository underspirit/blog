<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <title>
    统计学习方法读书笔记(1)-统计学习方法概论 | Leon&#39;s Blog
  </title>

  <link href="http://gmpg.org/xfn/11" rel="profile">
<meta http-equiv="content-type" content="text/html; charset=utf-8">


<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="">
<meta name="generator" content="Hugo 0.14" />

  <meta property="og:title" content="统计学习方法读书笔记(1)-统计学习方法概论" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:locale" content="en_US" />
<meta property="og:url" content="http://blog.songru.org/posts/machine-learning/statical-learning-1/" />


  
  <link rel="stylesheet" href="http://blog.songru.org//css/base-min.css">
  <link rel="stylesheet" href="http://blog.songru.org//css/pure-min.css">
  
  
    <link rel="stylesheet" href="http://blog.songru.org//css/grids-responsive-min.css">
  
  

  <link rel="stylesheet" href="http://blog.songru.org//css/redlounge.css">
  <link href="http://blog.songru.org//css/font-awesome.min.css" rel="stylesheet">
  <link href="http://blog.songru.org//css/googlefonts_raleway.css" rel='stylesheet' type='text/css'>
  <link href="http://blog.songru.org//css/googlefonts_Libre.css" rel='stylesheet' type='text/css'>

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="Leon&#39;s Blog" />

    
  
  <link rel="stylesheet" href="http://blog.songru.org//css/tomorrow-night-bright.min.css">
  
  <script src="http://blog.songru.org//js/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>


  

  

    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)'],['&(',')&']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {font: inherit;
              font-size: 100%;
              background: inherit;
              border: inherit;
              color: #515151;}
</style>


  
</head>

<body>
	

	<div id="layout" class="pure-g">
    <div class="sidebar pure-u-1 pure-u-md-1-4">
  <div class="header">
    

    <h1 class="brand-title">Leon&#39;s Blog</h1>
    <h2 class="brand-tagline">Is super awesome</h2>

    <nav class="nav">
      <ul class="nav-list">
        <li class="nav-item"><span class="nav-item-separator">//</span><a href="http://blog.songru.org/">Home</a></li>
        
          <li class="nav-item"><span class="nav-item-separator">//</span><a href="/posts/">Blog</a></li>
        
          <li class="nav-item"><span class="nav-item-separator">//</span><a href="/categories/">Categories</a></li>
        
          <li class="nav-item"><span class="nav-item-separator">//</span><a href="/tags/">Tags</a></li>
        
      </ul>
    </nav>

    
    <div class="social-buttons">
      
        
        <a href="https://github.com/underspirit" target="_blank"><i class='fa fa-github' style='font-size:22px;'></i></a>
        
      
      
    </div>
    

  </div>
</div>

	
	

    <div class="content pure-u-1 pure-u-md-3-4">
		<a name="top"></a>
		

		
			
		    <div id="toc" class="pure-u-1 pure-u-md-1-4">
				<small class="toc-label">Contents</small>
		   	 	<nav id="TableOfContents">
<ul>
<li><a href="#1-模型:b06856972157b699cfa45f18de0ae7a3">1. 模型</a></li>
<li><a href="#2-策略:b06856972157b699cfa45f18de0ae7a3">2. 策略</a>
<ul>
<li><a href="#损失函数和风险函数:b06856972157b699cfa45f18de0ae7a3">损失函数和风险函数</a></li>
<li><a href="#经验风险最小化与结构风险最小化:b06856972157b699cfa45f18de0ae7a3">经验风险最小化与结构风险最小化</a></li>
</ul></li>
<li><a href="#3-算法:b06856972157b699cfa45f18de0ae7a3">3. 算法</a>
<ul>
<li><a href="#正则化:b06856972157b699cfa45f18de0ae7a3">正则化</a></li>
</ul></li>
<li><a href="#4-生成模型与判别模型:b06856972157b699cfa45f18de0ae7a3">4. 生成模型与判别模型</a></li>
</ul>
</nav>
		    </div>
		    
	    
  		<section class="post">
            <h1 class="post-title">
              <a href="/posts/machine-learning/statical-learning-1/">统计学习方法读书笔记(1)-统计学习方法概论</a>
            </h1>
            <h3 class="post-subtitle">
            	
            </h3>
            
            	<span class="post-date">
                	<span class="post-date-day"><sup>3</sup></span><span class="post-date-separator">/</span><span class="post-date-month">Jul</span> <span class="post-date-year">2015</span>
            	</span>
            	
            
            	
            

			
            
			
				<div class="post-categories" style="margin-bottom: 25px;">
					<span class="dark-red">Keywords</span><span class="decorative-marker">：</span>
				
					<a class="post-category post-category-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" href="http://blog.songru.org//tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>
				
				</div>
			

			

			

            

<p>统计学习方法都是由模型、策略和算法这三个要素构成。</p>

<h1 id="1-模型:b06856972157b699cfa45f18de0ae7a3">1. 模型</h1>

<p>在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间(hypothesis    space)包含所有可能的条件概率分布或决策函数。例如,假设决策函数是输入变量的线性函数,那么模型的假设空间就是所有这些线性函数构成的函数集合。假设空间中的模型一般有无穷多个。<br />
假设空间用<code>$\mathcal{F}$</code>表示。假设空间可以定义为决策函数的集合：
<code>$$\mathcal{F}=\{f \mid Y=f(x)\}$$</code>
其中, <code>$X$</code>和<code>$Y$</code>是定义在输入空间<code>$\mathcal{X}$</code>和输出空间<code>$\mathcal{Y}$</code>上的变量。这时$\mathcal{F}$通常是由一个参数向量决定的函数族:<br />
<code>$$\mathcal{F}=\{f \mid Y=f_{\theta}(x), \theta \in R^n \}$$</code>
参数向量<code>$\theta$</code>取值于n维欧氏空间<code>$R^n$</code>,称为参数空间(parameter space)。假设空间也可以定义为条件概率的集合:<br />
<code>$$\mathcal{F}=\{P \mid P(Y \mid X) \}$$</code><br />
这时<code>$\mathcal{F}$</code>通常是由一个参数向量决定的条件概率分布族:<br />
<code>$$\mathcal{F}=\{P \mid P_{\theta}(Y \mid X) , \theta \in R^n\}$$</code><br />
参数向量<code>$\theta$</code>取值于n维欧氏空间<code>$R^n$</code>,也称为参数空间。<br />
称由决策函数表示的模型为非概率模型,由条件概率表示的模型为概率模型。为了简便起见,当论及模型时,有时只用其中一种模型。</p>

<h1 id="2-策略:b06856972157b699cfa45f18de0ae7a3">2. 策略</h1>

<p>有了模型的假设空间,统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。统计学习的
目标在于从假设空间中选取最优模型。<br />
首先引入损失函数与风险函数的概念。损失函数度量模型一次预测的好坏,风险函数度量平均意义下模型
预测的好坏。</p>

<h2 id="损失函数和风险函数:b06856972157b699cfa45f18de0ae7a3">损失函数和风险函数</h2>

<p>监督学习问题是在假设空间<code>$\mathcal{F}$</code>中选取模型f作为决策函数,对于给定的输入X,由<code>$f(X)$</code>给出相应的输出Y,这个输出的预测值<code>$f(X)$</code>与真实值Y可能一致也可能不一致,用一个损失函数(loss function)或代价函数(cost
function)来度量预测错误的程度。损失函数是<code>$f(X)$</code>和Y的非负实值函数,记作<code>$L(Y,f(X))$</code>。
统计学习常用的损失函数有以下几种:</p>

<ol>
<li>0-1损失函数(0-1  loss function)
<code>$$  L(Y, f(X))=\begin{cases}
    1, &amp;Y \neq f(X)\\
    0, &amp;Y = f(X)
    \end{cases}  $$</code></li>
<li>平方损失函数(quadratic loss function)
<code>$$  L(Y, f(X))=(Y - f(X))^2 $$</code></li>
<li>绝对值损失函数(absolute loss function)
<code>$$   L(Y, f(X))= \lvert Y - f(X) \rvert $$</code></li>
<li>对数损失函数(logarithmic loss function)或对数似然损失函数(loglikelihood loss    function)
<code>$$ L(Y, P(Y \mid X))=  - \log P(Y \mid X)$$</code></li>
</ol>

<p>损失函数值越小,模型就越好。由于模型的输入、输出<code>$(X,Y)$</code>是随机变量,遵循联合分布<code>$P(X,Y)$</code>,所以损失函数的期望是<br />
<code>$$ R_{exp} (f) = E_p \left[  L(Y, f(X)) \right] =  \int_{X \times Y}  L(y, f(x)) P(x, y) \ dxdy  $$</code>
这是理论上模型<code>$f(X)$</code>关于联合分布<code>$P(X,Y)$</code>的平均意义下的损失,称为风险函数(risk    function)或期望损失
(expected   loss)。<br />
学习的目标就是选择期望风险最小的模型。由于联合分布<code>$P(X,Y)$</code>是未知的,<code>$R_{exp}(f)$</code>不能直接计算。实际上,如果知道联合分布<code>$P(X,Y)$</code>,可以从联合分布直接求出条件概率分布<code>$P(Y|X)$</code>,也就不需要学习了。正因为不知道联合概率分布,所以才需要进行学习。这样一来,一方面根据期望风险最小学习模型要用到联合分布,另一方面联合分布又是未知的,所以监督学习就成为一个病态问题(ill-formed    problem)。<br />
给定一个训练数据集
<code>$$  T = \{  (x_1, y_1), (x_2, y_2), \cdots , (x_N, y_N) \} $$</code><br />
模型$f(X)$关于训练数据集的平均损失称为经验风险(empirical risk)或经验损失(empirical   loss),记作<code>$R_{emp}$</code>:
<code>$$ R_{emp} (f) =  \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i)) $$</code></p>

<p>期望风险<code>$R_{exp}(f)$</code>是模型关于联合分布的期望损失,经验风险<code>$R_{emp}(f)$</code>是模型关于训练样本集的平均损失。根据大数定律,当样本容量N趋于无穷时,经验风险<code>$R_{emp}(f)$</code>趋于期望风险<code>$R_{exp}(f)$</code>。所以一个很自然的想法是用经验风险估计期望风险。但是,由于现实中训练样本数目有限,甚至很小,所以用经验风险估计期望风险常常并不理想,要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略:<strong>经验风险最小化和结构风险最小化</strong>。</p>

<h2 id="经验风险最小化与结构风险最小化:b06856972157b699cfa45f18de0ae7a3">经验风险最小化与结构风险最小化</h2>

<p>经验风险最小化(empirical   risk minimization,ERM)的策略认为,经验风险最小的模型是最优的模型。根据这一策略,按照经验风险最小化求最优模型就是求解最优化问题:
<code>$$ \min \limits_{f \in F} \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i)) $$</code><br />
其中，<code>$F$</code>是假设空间。
当样本容量足够大时,经验风险最小化能保证有很好的学习效果,在现实中被广泛采用。比如,极大似然估计(maximum likelihood estimation)就是经验风险最小化的一个例子。<strong>当模型是条件概率分布,损失函数是对数损失函数时,经验风险最小化就等价于极大似然估计</strong>。<br />
但是,当样本容量很小时,经验风险最小化学习的效果就未必很好,会产生“过拟合(over-fitting)”现象。<br />
结构风险最小化(structural risk minimization,SRM)是为了防止过拟合而提出来的策略。<strong>结构风险最小化
等价于正则化(regularization)</strong>。结构风险在经验风险上加上表示模型复杂度的正则化项(regularizer)或罚项(penalty term)。在假设空间、损失函数以及训练数据集确定的情况下,结构风险的定义是
<code>$$ R_{SRM} (f) = \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i)) + \lambda J(f) $$</code><br />
其中<code>$J(f)$</code>为模型的复杂度,是定义在假设空间 上的泛函。模型f越复杂,复杂度<code>$J(f)$</code>就越大;反之,模型f越简
单,复杂度<code>$J(f)$</code>就越小。也就是说,复杂度表示了对复杂模型的惩罚。 <code>$\lambda \ge 0$</code>是系数,用以权衡经验风险和模型复杂度。<strong>结构风险小需要经验风险与模型复杂度同时小</strong>。<br />
贝叶斯估计中的最大后验概率估计(maximum posterior probability   estimation,MAP)就是结构风险最小化的一个例子。<strong>当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时,结构风险最小化就等价于最大后验概率估计</strong>。</p>

<h1 id="3-算法:b06856972157b699cfa45f18de0ae7a3">3. 算法</h1>

<p>算法是指学习模型的具体计算方法。统计学习基于训练数据集,根据学习策略,从假设空间中选择最优模
型,最后需要考虑用什么样的计算方法求解最优模型。</p>

<h2 id="正则化:b06856972157b699cfa45f18de0ae7a3">正则化</h2>

<p>模型选择的典型方法是正则化(regularization)。正则化是结构风险最小化策略的实现,是在经验风险上
加一个正则化项(regularizer)或罚项(penalty term)。正则化项一般是模型复杂度的单调递增函数,模型越复
杂,正则化值就越大。比如,<strong>正则化项可以是模型参数向量的范数</strong>。正则化一般具有如下形式:
<code>$$ \min \limits_{f \in F} \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i))  + \lambda J(f)$$</code><br />
其中,第1项是经验风险,第2项是正则化项.<br />
正则化项可以取不同的形式。例如<strong>,回归问题中,损失函数是平方损失,正则化项可以是参数向量的L2范
数</strong>:
<code>$$ \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i))  + \frac{\lambda}{2} \| w \|^2  \quad (\text{L2范数之后再平方})  $$</code><br />
这里,<code>$\| w \|$</code>表示参数向量w的L2范数。<br />
正则化项也可以是参数向量的L1范数:
<code>$$ \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i))  + \frac{\lambda}{2} \| w \|_1  \quad (\text{L1范数是绝对值之和})    $$</code></p>

<p>正则化符合奥卡姆剃刀(Occam&rsquo;s  razor)原理。奥卡姆剃刀原理应用于模型选择时变为以下想法:在所有可能选择的模型中,能够很好地解释已知数据并且十分简单才是最好的模型,也就是应该选择的模型。从贝叶斯估计的角度来看,正则化项对应于模型的先验概率。可以假设复杂的模型有较小的先验概率,简单的模型有较大的先验概率。</p>

<h1 id="4-生成模型与判别模型:b06856972157b699cfa45f18de0ae7a3">4. 生成模型与判别模型</h1>

<p>监督学习的任务就是学习一个模型,应用这一模型,对给定的输入预测相应的输出。这个模型的一般形式
为决策函数:
<code>$$ Y=f(X) $$</code>
或者条件概率分布：
<code>$$ P(Y \mid X) $$</code>
监督学习方法又可以分为生成方法(generative approach)和判别方法(discriminative approach)。所学到的模型分别称为生成模型(generative    model)和判别模型(discriminative model)。<br />
<strong>生成方法由数据学习联合概率分布</strong><code>$P(X,Y)$</code>,然后求出条件概率分布<code>$P(Y|X)$</code>作为预测的模型,即生成模型:
<code>$$ P(Y \mid X) = \frac{P(X, Y)} {P(X)} $$</code>
<strong>这样的方法之所以称为生成方法,是因为模型表示了给定输入X产生输出Y的生成关系</strong>。典型的生成模型有:朴素贝叶斯法和隐马尔可夫模型.<br />
判别方法由数据直接学习决策函数<code>$f(X)$</code>或者条件概率分布<code>$P(Y \mid X)$</code>作为预测的模型,即判别模型。判别方法关心的是对给定的输入X,应该预测什么样的输出Y。典型的判别模型包括:k近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等.<br />
<strong>生成方法的特点</strong>:生成方法可以还原出联合概率分布<code>$P(X,Y)$</code>,而判别方法则不能;生成方法的学习收敛速
度更快,即当样本容量增加的时候,学到的模型可以更快地收敛于真实模型;当存在隐变量时,仍可以用生成
方法学习,此时判别方法就不能用。<br />
<strong>判别方法的特点</strong>:判别方法直接学习的是条件概率<code>$P(Y \mid X)$</code>或决策函数<code>$f(X)$</code>,直接面对预测,往往学习的准确率更高;由于直接学习<code>$P(Y \mid X)$</code>或<code>$f(X)$</code>,可以对数据进行各种程度上的抽象、定义特征并使用特征,因此可以简化学习问题。</p>

	
			

			
				<div class="tags-list">
					<span class="dark-red">Categories</span><span class="decorative-marker">：</span>
					
	                <a class="post-tag post-tag-machine-learning" href="http://blog.songru.org//tags/machine-learning">Machine Learning</a>,
	                
	                <a class="post-tag post-tag-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95" href="http://blog.songru.org//tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95">统计学习方法</a>,
	                
				</div>
			


			
				<div class="paging">
					<div class="paging-label">More Reading</div>
					
					<span class="paging-newer">
						<span class="dark-red">下一篇</span><span class="decorative-marker">//</span>
						<a class="paging-link" href="/posts/machine-learning/statical-learning-2-Inception/">统计学习方法读书笔记(2)-感知机</a>
		            </span>
		            

					
	            </div>
            
          </section>
          
			<div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
    
    
    if (window.location.hostname == "localhost")
        return;

    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    var disqus_shortname = 'songru';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
          
        
      <div class="footer">
	<hr class="thin" />
	<div class="pure-menu pure-menu-horizontal pure-menu-open">
		<ul class="footer-menu">
		
		</ul>
	</div>

	<p>&copy; 2017. All rights reserved.</p>
</div>

    </div>
  </div>
	

	

  <script src="http://blog.songru.org//js/jquery-1.10.2.min.js" type="text/javascript"></script>
<script type="text/javascript">

$(function(){
    $('pre code').each(function(){
        $(this).parent().addClass('codePre');
    });
});
</script>
</body>
</html>
