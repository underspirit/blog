+++
date = "2015-07-07T10:51:18+08:00"
description = ""
categories = ["Machine Learning", "统计学习方法"]
tags = ["决策树", "Decition Tree"]
title = "统计学习方法读书笔记(5)-决策树"

+++

决策树(decision tree)是一种基本的分类与回归方法。它可以认为是if-then规则的集合,也可以认为是定义在特征空间与类空间上的条件概率分布。决策树学习通常包括3个步骤:**特征选择、决策树的生成和决策树的修剪**。    

# 决策树模型与学习

 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点有两种类型:内部结点(internal node)和叶结点(leaf	node)。内部结点表示一个特征或属性,叶结点表示一个类。   
 
用决策树分类,从根结点开始,对实例的某一特征进行测试,根据测试结果,将实例分配到其子结点;这时,每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配,直至达到叶结点。最后将实例分到叶结点的类中。     

**可以将决策树看成一个if-then规则的集合。**将决策树转换成if-then规则的过程是这样的:由决策树的根结点到叶结点的每一条路径构建一条规则;路径上内部结点的特征对应着规则的条件,而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质:**互斥并且完备**。这就是说,每一个实例都被一条路径或一条规则所覆盖,而且只被一条路径或一条规则所覆盖。   

## 决策树与条件概率分布
**决策树还表示给定特征条件下类的条件概率分布。**这一条件概率分布定义在特征空间的一个划分
(partition)上。将特征空间划分为互不相交的单元(cell)或区域(region),并**在每个单元定义一个类的概率分布就构成了一个条件概率分布**。**决策树的一条路径对应于划分中的一个单元。**决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设X为表示特征的随机变量,Y为表示类的随机变量,那么这个条件概率分布可以表示为P(Y|X)。X取值于给定划分下单元的集合,Y取值于类的集合。各叶结点(单元)上的条件概率往往偏向某一个类,即属于某一类的概率较大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。   

# 决策树学习
决策树学习,假设给定训练数据集
`$$ D = \{  (x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)  \} $$`
其中`$   x_i=(x_i^{(1)}, x_i^{(2)} ,  \cdots, x_i^{(n)} )^T  $`为输入实例(特征向量), `$n$`为特征个数, `$y_i \in \{1,2, \cdots ,K\}$`为类标记, `$i=1,2, \cdots ,N$`, `$N$`为样本容量.学习的目标是根据给定的训练数据集构建一个决策树模型,使它能够对实例进行正确的分类。     

决策树学习是由训练数据集估计条件概率模型。基于特征空间划分的类的条件概率模型有无穷多个。我们选择的条件概率模型应该不仅对训练数据有很好的拟合,而且对未知数据有很好的预测。    

决策树学习用损失函数表示这一目标。如下所述,决策树学习的**损失函数通常是正则化的极大似然函数**。决策树学习的策略是以损失函数为目标函数的最小化。      

当损失函数确定以后,学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策
树中选取最优决策树是**NP完全问题**,所以现实中决策树学习算法通常采用启发式方法,近似求解这一最优化问题。这样得到的决策树是**次最优(sub-optimal)的**。   

决策树学习算法包含**特征选择、决策树的生成与决策树的剪枝过程**。由于决策树表示一个条件概率分布,所以深浅不同的决策树对应着不同复杂度的概率模型。**决策树的生成对应于模型的局部选择,决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优,相对地,决策树的剪枝则考虑全局最优。**

   
## 特征选择
特征选择在于选取对训练数据具有分类能力的特征。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别,则称这个特征是没有分类能力的。     
直观上,如果一个特征具有更好的分类能力,或者说,按照这一特征将训练数据集分割成子集,使得各个子集在当前条件下有最好的分类,那么就更应该选择这个特征。**信息增益(information gain)**就能够很好地表示这一直观的准则。    

### 熵(entropy)
在信息论与概率统计中,**熵(entropy)**是表示随机变量**不确定性**的度量。设`$X$`是一个取有限个值的离散随机变量,其概率分布为
`$$ P(X=x_I) = p_i, \quad i=1,2,\cdots, n $$`
则随机变量X的熵定义为
`$$ H(X) = - \sum_{i=1}^n  p_i \log p_i $$`
若`$p_i=0$`, 则定义`$0\log0=0$`。   通常, 对数以2为底或以e为底(自然对数),这时熵的单位分别称作**比特(bit)或纳特(nat)**。由定义可知,熵只依赖于`$X$`的分布,而与`$X$`的取值无关,所以也可将`$X$`的熵记作`$H(p)$`,即
`$$ H(p) = - \sum_{i=1}^n  p_i \log p_i $$`
**熵越大,随机变量的不确定性就越大。**从定义可验证
`$$ 0 \le H(p) \le \log n $$`

### 条件熵(conditional entropy)
设有随机变量`$(X,Y)$`,其联合概率分布为
`$$ P(X=x_i, Y=y_i) = p_{ij}, \quad i=1,2,\cdots,n ; \quad j=1,2,\cdots,m  $$`
条件熵`$H(Y|X)$`表示**在已知随机变量X的条件下随机变量Y的不确定性**。随机变量X给定的条件下随机变量Y的条件熵(conditional entropy)`$H(Y|X)$`, 定义为**在X已经给定的条件下Y的条件概率分布的熵对X的数学期望**
`$$  H(Y \mid X) = \sum_{i=1}^n p_i H(Y \mid X = x_i) $$`
这里, `$p_i=P(X=x_i), \ i=1,2,...,n$`。
当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时,所对应的熵与条件熵分别称为**经验熵(empirical 	entropy)和经验条件熵(empirical	 conditional entropy)**。此时,如果有0概率,令`$0\log0=0$`。   

### 信息增益(information gain)
信息增益(information gain)表示**得知特征X的信息而使得类Y的信息的不确定性减少的程度**。    
特征A对训练数据集D的信息增益`$g(D,A)$`,定义为集合D的经验熵`$H(D)$`与特征A给定条件下D的经验条件熵`$H(D \mid A)$`之差,即
 `$$  g(D, A) = H(D) - H(D \mid A) $$`
 熵`$H(Y)$`与条件熵`$H(Y \mid X)$`之差又称为**互信息(mutual information)**。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。   

给定训练数据集D和特征A,经验熵`$H(D)$`表示对数据集D进行分类的不确定性。而经验条件熵`$H(D \mid A)$`表示在特征A给定的条件下对数据集D进行分类的不确定性。那么它们的差,即信息增益,就**表示由于特征A而使得对数据集D的分类的不确定性减少的程度**。显然,对于数据集D而言,信息增益依赖于特征,不同的特征往往具有不同的信息增益。**信息增益大的特征具有更强的分类能力**。   
**根据信息增益准则的特征选择方法是:**对训练数据集(或子集)D,计算其每个特征的信息增益,并比较它们的大小,选择信息增益最大的特征。       

设训练数据集为D, `$|D|$` 表示其样本容量,即样本个数。设有K个类`$C_k,k=1,2, \cdots ,K$`, `$|C_k|$`为属于类`$C_k$`的样本个数 。设特征A有n个不同的取值`$\{a_1,a_2, \cdots, a_n\}$`, 根据特征A的取值将D划分为n个子集`$D_1,D_2, \cdots ,D_n$, $|D_i|$`为`$D_i$`的样本个数 。记子集`$D_i$`中属于类`$C_k$`的样本的集合为`$D_{ik}$`, 即`$D_{ik} = D_i \cap C_k$`, `$|D_{ik}|$`为`$D_{ik}$`的样本个数。于是信息增益的算法如下:   

输入:训练数据集D和特征A;   
输出:特征A对训练数据集D的信息增益`$g(D,A)$`。    
步骤:    

1. 计算数据集D的经验熵`$H(D)$`
`$$ H(D) = - \sum_{k=1}^K \frac{\lvert C_k \rvert} { \lvert D \rvert} \log \frac{\lvert C_k \rvert} { \lvert D \rvert}  $$`
2. 计算特征A对数据集D的经验条件熵`$H(D \mid A)$`
`$$ 
\begin{align*}
H(D \mid A) &= \sum_{i=1}^n \frac{\lvert D_i\rvert} { \lvert D \rvert}  H(D_i)   \\ 
&=  \sum_{i=1}^n \frac{\lvert D_i\rvert} { \lvert D \rvert}  \sum_{k=1}^K  \frac{\lvert D_{ik}\rvert} { \lvert D_i \rvert}  \log \frac{\lvert D_{ik}\rvert} { \lvert D_i \rvert} 
\end{align*}
$$`

3. 计算信息增益
`$$ g(D,A) = H(D) - H(D \mid A) $$`

### 信息增益比
以信息增益作为划分训练数据集的特征, 存在偏向于选择取值较多的特征的问题。使用信息增益比(information gain ratio)可以对这一问题进行校正。这是特征选择的另一准则。     
 特征A对训练数据集D的信息增益比`$g_R(D,A)$`定义为其信息增益`$g(D,A)$`与训练数据集D的关于特征`$A$`的值的熵`$H_A(D)$`之比:
 `$$ g_R(D, A) = \frac {g(D, A)} {H_A(D)} $$`

这里的分母应该是
`$$ H_A(D) = - \sum_{i=1}^n \frac{\lvert D_i \rvert} { \lvert D \rvert} \log \frac{ \lvert D_i \rvert }{ \lvert D \rvert} $$`
分子不再是`$C_i$`了. 其中分母又被称为分裂信息量或内在信息（Intrinsic Information），可简单地理解为表示信息分支所需要的信息量, **是将特征A的可选值作为划分(而不是类别)**，计算节点上样本总的信息熵。

## 决策树的生成

### ID3算法
ID3算法的**核心是在决策树各个结点上应用信息增益准则选择特征,递归地构建决策树。**具体方法是:从根结点(root node)开始,对结点计算所有可能的特征的信息增益,选择信息增益最大的特征作为结点的特征,由该特征的不同取值建立子结点;再对子结点递归地调用以上方法,构建决策树;直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。**ID3相当于用极大似然法进行概率模型的选择**。   

输入:训练数据集D,特征集A, 阈值`$ \epsilon$`;     
输出:决策树T。   
步骤:    

1. 若D中所有实例属于同一类`$C_k$`, 则T为单结点树, 并将类`$C_k$`作为该结点的类标记, 返回T;   
2. 若`$A= \varnothing $`,则T为单结点树, 并将D中实例数最大的类`$C_k$`作为该结点的类标记, 返回T;
3. 否则, 计算A中各特征对D的信息增益, 选择信息增益最大的特征`$A_g$`;
4. 如果`$A_g$`的信息增益小于阈值`$\epsilon$` , 则置T为单结点树,并将D中实例数最大的类`$C_k$` 作为该结点的类标记, 返回T;
5. 否则, 对`$A_g$`的每一可能值`$a_i$`, 依`$A_g=a_i$`将D分割为若干非空子集`$D_i$`, 将`$D_i$`中实例数最大的类作为标记,构建子结点,由结点及其子结点构成树T, 返回T;
6. 对第i个子结点,以`$D_i$`为训练集, 以`$A-\{A_g\}$`为特征集, 递归地调用步(1)~步(5),得到子树`$T_i$`, 返回`$T_i$`。

### C4.5算法

C4.5克服了ID3的2个缺点：   
1. 用信息增益选择属性时偏向于选择分枝比较多的属性值(即取值多的属性), 所有C4,5采用信息增益比来选择特征。  
2. ID3不能处理连续型的属性特征

输入: 训练数据集D,特征集A,阈值`$\epsilon$` ;    
输出: 决策树T。  
步骤:   

1. 如果D中所有实例属于同一类`$C_k$` , 则置T为单结点树, 并将`$C_k$`作为该结点的类, 返回T;
2. 若`$A= \varnothing $`,则T为单结点树, 并将D中实例数最大的类`$C_k$`作为该结点的类标记, 返回T;
3. 否则, 计算A中各特征对D的信息增益, 选择信息增益最大的特征`$A_g$`;
4. 如果`$A_g$`的信息增益比小于阈值`$\epsilon$` , 则置T为单结点树,并将D中实例数最大的类`$C_k$` 作为该结点的类标记, 返回T;
5. 否则, 对`$A_g$`的每一可能值`$a_i$`, 依`$A_g=a_i$`将D分割为若干非空子集$D_i$, 将$D_i$中实例数最大的类作为标记,构建子结点,由结点及其子结点构成树T, 返回T;
6. 对第i个子结点, 以$D_i$为训练集, 以$A-\{A_g\}$为特征集, 递归地调用步(1)~步(5),得到子树$T_i$, 返回$T_i$。

#### 连续型特征的处理
先把连续属性转换为离散属性再进行处理。虽然本质上属性的取值是连续的，但对于有限的采样数据它是离散的，如果有N条样本，那么我们有N-1种离散化的方法：$<=v_j$的分到左子树，$>v_j$的分到右子树。计算这N-1种情况下最大的信息增益比。   
在离散属性上只需要计算1次信息增益率，而在连续属性上却需要计算N-1次，计算量是相当大的。可以如下减少计算量: 对于连续属性先进行排序，**只在决策属性(即类别)发生改变的地方进行切分**。   
如果利用增益率来选择连续值属性的分界点，会导致一些副作用。分界点将样本分成两个部分，这两个部分的样本个数之比也会影响增益率。根据增益率公式，我们可以发现，当分界点能够把样本分成数量相等的两个子集时（我们称此时的分界点为等分分界点），增益率的抑制会被最大化(此时分母$H(D)$增大, 增益率减小)，因此等分分界点被过分抑制了。子集样本个数能够影响分界点，显然不合理。**因此在决定分界点是还是采用增益这个指标，而选择属性的时候才使用增益率这个指标。这个改进能够很好得抑制连续值属性的倾向。** 

设训练数据集为D, $|D|$ 表示其样本容量,即样本个数。设有K个类`$C_k,k=1,2, \cdots ,K$`, `$|C_k|$`为属于类`$C_k$`的样本个数 。设特征A有n个不同的取值`$\{a_1,a_2, \cdots, a_n\}$`, 根据特征A的取值将D划分为n个子集`$D_1,D_2, \cdots ,D_n$`, `$|D_i|$`为`$D_i$`的样本个数 。记子集`$D_i$`中属于类`$C_k$`的样本的集合为`$D_{ik}$`, 即`$D_{ik} = D_i \cap C_k$`, `$|D_{ik}|$`为`$D_{ik}$`的样本个数。于是信息增益的算法如下:   
输入:训练数据集D和特征A;
输出:特征A对训练数据集D的信息增益`$g(D,A)$`。
#### 属性缺失值的处理
现实任务中常会遇到不完整的样本, 即样本的某些属性值缺失. 如果简单的放弃不完整样本, 仅使用无缺失值的样本来进行学习, 显然是对数据的极大浪费.       
两个问题: 
1. 如何在属性值缺失的情况下进行划分属性的选择?
2. 给定划分属性, 若样本在该属性上的值缺失, 如何对样本进行划分? 

给定训练集`$D$`和属性`$A$`, 特征A有V个不同的取值`$\{a_1,a_2, \cdots, a_V\}$`, 令`$ \tilde{D} $`表示`$D$`中在属性`$A$`上没有缺失值的样本子集. 对问题(1), 根据`$ \tilde{D} $`来判断属性`$A$`的优劣. 令`$  \tilde{D_{v}}  $`为`$ \tilde{D} $`中在属性`$A$`上取值为`$a_v$`的样本子集,  `$  \tilde{D^{k}}  $`为`$ \tilde{D} $`中在属于第`$k$`类的样本子集. `$ \tilde{D}_{ik} $`表示在属性`$A$`上取值为`$a_v$`的, 且属于第`$k$`类的样本子集, 则有 `$  \tilde{D} =  \bigcup _{k=1} ^K \tilde{D^k}$`, `$  \tilde{D} =
\bigcup _{v=1} ^V \tilde{D_v}$`. 假设我们为每个样本`$x$` 赋予一个权重`$w_x$`(一般初始化为1), 并定义
`$$
\rho = \frac{ \sum_{x \in  \tilde{D}} w_x } { \sum_{x \in  D} w_x  }  , \\
\tilde{p}_k = \frac{ \sum_{x \in  \tilde{D_k}} w_x } { \sum_{x \in  D} w_x  }  , \\
\tilde{r}_v = \frac{ \sum_{x \in  \tilde{D^v}} w_x } { \sum_{x \in  D} w_x  }  , \\
$$`    
对属性`$A$`, `$\rho$`表示无缺失值样本所占比例, `$\tilde{p}_k$`表示无缺失值样本中第`$k$`类所占比例, `$\tilde{r}_v$`表示无缺失值样本中在属性`$A$`上取值`$a_v$`的样本所占比例.    
基于上述定义, 将信息增益推广为:
 `$$  g(D, A) = \rho \ g(\tilde{D}, A) = \rho \ \left(H(\tilde{D}) - H(\tilde{D} \mid A) \right)$$`
`$$ 
\begin{align*}
H(\tilde{D} \mid A) &= \sum_{i=1}^V \frac{\lvert \tilde{D}_i\rvert} { \lvert \tilde{D} \rvert}  H(\tilde{D}_i)   \\ 
&=  \sum_{i=1}^V \frac{\lvert \tilde{D}_i\rvert} { \lvert \tilde{D} \rvert}  \sum_{k=1}^K  \frac{\lvert \tilde{D}_{ik}\rvert} { \lvert \tilde{D}_i \rvert}  \log \frac{\lvert \tilde{D}_{ik}\rvert} { \lvert \tilde{D}_i \rvert} 
\end{align*}
$$`    
对问题(2), 若样本`$x$`在划分属性`$A$`上的取值已知, 则将`$x$`划入与其取值对应的子结点, 且样本权值在子节点中保持为`$w_x$`. 若样本`$x$`在划分属性是的取值未知, 则将`$x$`同时划分到所有子节点, 且样本取值在与属性值`$a_v$`对应的子节点中调整为`$\tilde{r}_v  \cdot w_x$`; 直观的看, 就是让同一样本以不同的概率划入不同的子节点中取. 
   
#### 决策树的剪枝(后剪枝)
决策树生成算法递归地产生决策树,直到不能继续下去为止。这样产生的树往往出现**过拟合**现象。解决这个问题的办法是考虑决策树的复杂度,对已生成的决策树进行简化。   
将已生成的树进行简化的过程称为**剪枝(pruning)**。具体地,剪枝从已生成的树上裁掉一些子树或叶结点,并将其根结点或父结点作为新的叶结点,从而简化分类树模型。   

决策树的剪枝往往通过**极小化决策树整体的损失函数**(loss function)或代价函数(cost function)来实现。设树T的叶结点个数为`$|T|$`, `$t$`是树`$T$`的叶结点, 该叶结点有`$N_t$`个样本点, 其中k类的样本点有`$N_{tk}$`个, `$k=1,2, \cdots ,K$`, `$H_t(T)$`为叶结点t上的经验熵, `$\alpha≥0$`为参数,则决策树学习的损失函数可以定义为
`$$ C_{\alpha} (T) = \sum_{t=1}^{|T|} N_t H_t(T) + \alpha \lvert T \rvert  $$`
其中经验熵为
`$$ H_t(T) = - \sum_{k=1}^K \frac{N_{tk}} {N_t} \log  \frac{N_{tk}} {N_t} $$` 
在损失函数中,将式右端的第1项记作
`$$ C(T) =  \sum_{t=1}^{|T|} N_t H_t(T) = - \sum_{t=1}^{|T|} \sum_{k=1}^K N_{tk} \log \frac{N_{tk}} {N_t}   $$`
这时有:
`$$ C_{\alpha} (T) = C(T) + \alpha  \lvert T \rvert  $$`
`$C(T)$`表示模型对训练数据的预测误差, 即模型与训练数据的拟合程度, `$|T|$`表示模型复杂度(正则项), 参数`$\alpha≥0$`控制两者之间的影响。较大的`$\alpha$`促使选择较简单的模型(树), 较小的`$\alpha$`促使选择较复杂的模型(树)。`$\alpha=0$`意味着只考虑模型与训练数据的拟合程度, 不考虑模型的复杂度。    
剪枝,就是当`$\alpha$`确定时,选择损失函数最小的模型,即损失函数最小的子树。可以看出,决策树生成只考虑了通过提高信息增益(或信息增益比)对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。**决策树生成学习局部的模型,而决策树剪枝学习整体的模型。**     

**损失函数的极小化等价于正则化的极大似然估计**。所以,利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。   

**树的剪枝算法(最小误差剪枝)**   
输入: 生成算法产生的整个树`$T$`, 参数`$\alpha$`;
输出: 修剪后的子树`$T_{\alpha}$`。
1. 计算每个结点的经验熵。
2. 递归地从树的叶结点向上回缩。
设一组叶结点回缩到其父结点之前与之后的整体树分别为`$T_B$`与`$T_A$`, 其对应的损失函数值分别`$C_{\alpha}(T_B)$`与`$C_{\alpha}(T_A)$`, 如果
`$$ C_{\alpha}(T_B) \ge C_{\alpha}(T_A) $$`
则进行剪枝,即将父结点变为新的叶结点。    
3. 返回(2), 直至不能继续为止, 得到损失函数最小的子树`$T_{\alpha}$`。    
上式只需考虑两个树的损失函数的差,其计算可以在局部进行。所以,决策树的剪枝算法可以由一种动态规划的算法实现。    


#### 决策树的剪枝(预剪枝)
预剪枝是指在决策树生成过程中, 对每个节点在划分前先进行估计, 若当前节点的划分不能带来**决策树泛化能力**的提升, 则停止划分并将当前节点标记为叶节点.     
**通过在验证集上的准确率来进行判断是否剪枝.** 分别计算划分前后的准确率, 若划分后准确率下降, 则不进行该次划分, 直接标记该节点为叶节点, 所属类别为节点中实例数最大的类.    
预剪枝使得很多分支没有"展开", 这不仅降低了过拟合的风险, 还显著降低了决策树的训练时间和预测时间开销. 但另一方面, 有些分支的当前划分虽不能提升泛化性能,甚至可能导致泛化性能暂时下降, 但在其基础上进行的后续划分却有可能导致性能显著提高; 预剪纸基于"贪心"本质禁止这些分支展开, 给决策树带来了欠拟合的风险. 

上面的后剪纸也可以采用这种通过验证集准确率决定是否剪纸的策略.


#### CART算法
CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。**CART假设决策树是二叉树**, **内部结点特征的取值为“是”和“否**”,左分支是取值为“是”的分支,右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征,将输入空间即特征空间划分为有限个单元,并在这些单元上确定预测的概率分布,也就是在输入给定的条件下输出的条件概率分布。     

CART算法由以下两步组成:
(1)决策树生成:基于训练数据集生成决策树,**生成的决策树要尽量大**;
(2)决策树剪枝:用**验证数据集**对已生成的树进行剪枝并选择最优子树,这时用损失函数最小作为剪枝
的标准。   

#### CART的生成
决策树的生成就是递归地构建**二叉决策树**的过程。对**回归树用平方误差最小化准则**, 对**分类树用基尼指数(Gini	index)最小化准则**,进行特征选择,生成二叉树。    

#### 回归树的生成
假设`$X$`与`$Y$`分别为输入和输出变量, 并且`$Y$`是**连续变量**,给定训练数据集考虑如何生成回归树.
`$$ D = \{  (x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)  \} $$`   
**一个回归树对应着输入空间(即特征空间)的一个划分以及在划分的单元上的输出值**。假设已将输入空间划分为`$M$`个单元`$R_1,R2, \cdots ,R_M$`, 并且在每个单元`$R_m$`上有一个固定的输出值`$c_m$`,于是回归树模型可表示为
`$$ f(x) = \sum_{m=1}^M c_m I(x \in R_m) $$`   
当输入空间的划分确定时,可以用**平方误差**`$ \sum_{x_i \in R_m} (y_i - f(x_i))^2 $`来表示回归树对于训练数据的预测误差, 用**平方误差最小的准则求解每个单元上的最优输出值**。易知,单元`$R_m$`上的`$c_m$`的最优值`$\hat{c}_m$`是`$R_m$`上的所有输入实例`$x_i$`对应的输出`$y_i$`的均值,即
`$$ \hat{c}_m = avg (y_i \mid x_i \in R_m) $$`   
采用启发式的方法样对输入空间进行划分, 选择第`$j$`个变量`$x^{(j)}$`和它取的值`$s$`作为切分变量(splitting variable)和切分点(splitting	point), 并定义两个区域:
`$$ R_1(j, s) = \{ x \mid x^{(j)} \le s \}  \quad  和   \quad R_2(j, s) = \{ x \mid x^{(j)} > s \}  $$`
然后寻找最优切分变量j和最优切分点s。具体地,求解   
`$$  \min \limits_{j, s} \left[ \min \limits_{c_1} \sum_{x_i \in R_1(j, s)} (y_i - c_1)^2 + \min \limits_{c_2} \sum_{x_i \in R_2(j, s)} (y_i - c_2)^2  \right] $$`   
对固定输入变量`$j$`可以找到最优切分点`$s$`。
`$$  \hat{c}_1 = avg( y_i \mid x_i \in R_1(j, s) )  \quad 和 \quad   \hat{c}_2 = avg( y_i \mid x_i \in R_2(j, s) )$$`   
遍历所有输入变量, 找到最优的切分变量`$j$`, 构成一个对`$(j,s)$`。依此将输入空间划分为两个区域。接着,对每个区域重复上述划分过程, 直到满足停止条件为止。这样就生成一棵回归树。这样的回归树通常称为**最小二乘回归树(least	squares regression tree)**,现将算法叙述如下:    
输入: 训练数据集`$D$`;    
输出: 回归树`$f(x)$`。   
在训练数据集所在的输入空间中, 递归地将每个区域划分为两个子区域并决定每个子区域上的输出值,构建二叉决策树:    
1. 选择最优切分变量`$j$`与切分点`$s$`, 求解    
`$$  \min \limits_{j, s} \left[ \min \limits_{c_1} \sum_{x_i \in R_1(j, s)} (y_i - c_1)^2 + \min \limits_{c_2} \sum_{x_i \in R_2(j, s)} (y_i - c_2)^2  \right] $$`      
遍历变量`$j$`, 对固定的切分变量$j$扫描切分点`$s$`, 选择使上式达到最小值的对`$(j,s)$`。    
2. 用选定的对`$(j,s)$`划分区域并决定相应的输出值:   
`$$ R_1(j, s) = \{ x \mid x^{(j)} \le s \}  \  ,   \quad R_2(j, s) = \{ x \mid x^{(j)} > s \}   \\
\hat{c}_m = \frac{1} {N_m} \sum_{x_i \in R_m(j, s)} y_i \ , \quad x \in R_m, \ m=1,2
$$`   
3. 继续对两个子区域调用步骤(1),(2),直至满足停止条件。   
4. 将输入空间划分为`$M$`个区域`$R_1,R_2, \cdots, R_m$`,生成决策树:   
`$$ f(x) = \sum_{m=1}^M \hat{c}_m I(x \in R_m) $$`       

#### 分类树的生成
分类树用基尼指数选择最优特征,同时决定该特征的最优二值切分点。

#### 基尼指数
分类问题中,假设有`$K$`个类, 样本点属于第`$k$`类的概率为`$p$` , 则概率分布的基尼指数`$k$`定义为   
`$$ Gini(p) = 1 -  \sum_{k=1}^K \left(  \frac{ \lvert C_k \rvert  } { \lvert  D \rvert  }  \right) $$`    
这里`$C_k$`是`$D$`中属于第`$k$`类的样本子集, `$K$`是类的个数.      
如果样本集合`$D$`根据特征`$A$`是否取某一可能值`$a$`被分割成`$D_1$`和`$D_2$`两部分,即    
`$$ D_1 = \{  (x, y) \in D \mid A(x) = a \} \ , \quad D_2 = D - D_1  $$`    
则在特征A的条件下, 集合`$D$`的基尼指数定义为   
`$$  Gini(D, A) =  \frac{ \lvert D_1 \rvert  } { \lvert  D \rvert  } Gini(D_1) + \frac{ \lvert D_2 \rvert  } { \lvert  D \rvert  } Gini(D_2)$$`     
基尼指数`$Gini(D)$`表示集合`$D$`的不确定性, 基尼指数`$Gini(D,A)$`表示经`$A=a$`分割后集合`$D$`的不确定性。**基尼指数值越大,样本集合的不确定性也就越大,这一点与熵相似。**      
二类分类问题中基尼指数`$Gini(p)$`、熵(单位比特)之半`$\frac{1}{2}H(p)$`和分类误差率的关系    
![Alt text](/img/1467279391154.png)    
基尼指数和熵之半的曲线很接近,都可以近似地代表分类误差率。

**CART生成算法**
输入: 训练数据集`$D$`, 停止计算的条件;   
输出: CART决策树。  
根据训练数据集,从根结点开始,递归地对每个结点进行以下操作,构建二叉决策树:    
1. 设结点的训练数据集为`$D$`, 计算现有特征对该数据集的基尼指数。此时,对每一个特征`$A$`,对其可能取的每个值`$a$`, 根据样本点对`$A=a$`的测试为“是”或“否”将`$D$`分割成`$D_1$`和`$D_2$`两部分, 计算`$A=a$`时的基尼指数`$Gini(D, A)$`。   
2. 在所有可能的特征`$A$`以及它们所有可能的切分点`$a$`中, **选择基尼指数最小的特征及其对应的切分点**作为最优特征与最优切分点. 依最优特征与最优切分点,从现结点生成两个子结点,将训练数据集依特征分配到两个子结点中去。   
3. 对两个子结点递归地调用(1),(2),直至满足停止条件。
4. 生成CART决策树。   
**算法停止计算的条件**是结点中的样本个数小于预定阈值, 或样本集的基尼指数小于预定阈值(样本基本属于同一类),或者没有更多特征。   

#### CART剪枝
CART剪枝算法由两步组成:首先从生成算法产生的决策树`$T_0$`底端开始不断剪枝, 直到`$T_0$`的根结点,形成一个子树序列`$\{T_0,T_1, \cdots ,T_n\}$`; 然后通过交叉验证法在独立的验证数据集上对子树序列进行测试,从中选择最优子树。   

1. 剪枝,形成一个子树序列
在剪枝过程中,计算子树的损失函数:
`$$ C_\alpha (T) = C(T) + \alpha \lvert T \rvert $$`    
其中, `$T$`为任意子树, `$C(T)$`为对训练数据的预测误差(如基尼指数, 平方误差), `$ \lvert T \rvert  $`为子树的叶子点个数, `$\alpha \ge 0$`为参数, `$C_\alpha (T)  $`为参数是`$\alpha$`时的子树`$T$`的整体损失。参数`$\alpha$`权衡训练数据的拟合程度与模型的复杂度。    
对固定的`$\alpha$`, 一定存在使损失函数`$C_\alpha (T)  $`最小的子树, 将其表示为`$T_\alpha$`。`$T_\alpha$`在损失函数`$C_\alpha (T)  $`最小的意义下是最优的.   
从整体树`$T_0$`开始剪枝。对`$T_0$`的任意内部结点`$t$`, 以`$t$`为单结点树的损失函数是
`$$ C_\alpha (T) = C(t)  + \alpha \ , \ (\lvert t \rvert  = 1) $$`    
以`$t$`为根结点的子树`$T_t$`的损失函数是
`$$ C_\alpha (T_t) = C(T_t)  + \alpha \lvert T_t \rvert$$`      
当`$\alpha=0$及$\alpha$`充分小时, 有不等式
`$$ C_\alpha (T_t)  < C_\alpha (t)   $$`    
当`$\alpha$`增大时, 在某一`$\alpha$`有   
`$$ C_\alpha (T_t)  =  C_\alpha (t)   $$`    
当`$\alpha$`再增大时,不等式反向。只要`$ \alpha = \frac {C(t) - C(T_t)} {\lvert T_t \rvert -1}$`, `$T_t$`与`$t$`有相同的损失函数值,  而`$t$`的结点少, 因此`$t$`比`$T_t$`更可取, 对`$T_t$`进行剪枝。   
为此, 对`$T_0$`的每一内部结点t, 计算
`$$ g(t) = \frac {C(t) - C(T_t)} {\lvert T_t \rvert -1}$$`   
**它表示剪枝后整体损失函数减少的程度**。在`$T _0$`中剪去`$g(t)$`最小的`$T_t$` , 将得到的子树作为`$T_1$` ,同时将最小的`$g(t)$`设为`$\alpha_1$`。`$T_1$`为区间`$[\alpha_1,\alpha_2)$`的最优子树。   
如此剪枝下去,直至得到根结点。在这一过程中,不断地增加`$\alpha$`的值,产生新的区间。   

2. 在剪枝得到的子树序列`$T_0,T_1,\cdots,T_n$`中通过交叉验证选取最优子树`$T_a$`    
具体地, **利用独立的验证数据集**, 测试子树序列`$T_0,T_1,\cdots,T_n$`中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中,每棵子树`$T_0,T_1,\cdots,T_n$`都对应于一个参数`$\alpha_1, \alpha_2,\cdots, \alpha_n$`。所以, 当最优子树`$T_k$`确定时, 对应的`$\alpha_k$`也确定了, 即得到最优决策树`$T_\alpha$`。    

**CART剪枝算法:**    
输入: CART算法生成的决策树`$T_0$`;   
输出: 最优决策树`$T_\alpha$`   
1. 设`$k=0,\ T=T_0$`.
2. 设`$ \alpha = + \infty$`
3. 自下而上地对各内部结点`$t$`计算`$C(T_t)$`, `$ \lvert T_t \rvert $`以及
`$$ g(t) = \frac {C(t) - C(T_t)} {\lvert T_t \rvert -1} \\ 
\alpha = \min \left(\alpha, g(t) \right)
$$`     
这里,`$T_t$`表示以`$t$`为根结点的子树, `$C(T_t)$`是对训练数据的预测误差, `$\lvert T_t \rvert $`是`$T_t$`的叶结点个数。   
4. 对`$g(t)=\alpha$`的内部节点`$t$`进行剪枝, 并对叶结点`$t$`以多数表决法决定其类,得到树`$T$`。   
5. 设`$k=k+1, \alpha_k=\alpha, T_k=T$`。  
6. 如果`$T_k$`不是由根结点及两个叶结点构成的树, 则回到步骤(3), 否则令`$T_k = T_n$`。
7. 采用交叉验证法在子树序列`$T_0,T_1,\cdots,T_n$`中选取最优子树`$T_\alpha$`。
