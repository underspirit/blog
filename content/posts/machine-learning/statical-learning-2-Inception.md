+++
date = "2015-07-03T16:32:27+08:00"
description = ""
tags = ["ML"]
title = "统计学习方法读书笔记(2)-感知机"

+++


感知机(perceptron)是二类分类的线性分类模型,其输入为实例的特征向量,输出为实例的类别,取+1和–1二值。感知机对应于输入空间(特征空间)中将实例划分为正负两类的分离超平面,**属于判别模型**。感知机学习旨在求出将训练数据进行线性划分的分离超平面,为此,导入基于误分类的损失函数,利用梯度下降法对损失函数进行极小化,求得感知机模型。   
<!--more-->    
   

# 感知机的定义
**感知机的定义**:  假设输入空间(特征空间)是`$\mathcal{X} \subseteq R^n$`,输出空间是 `$ \mathcal{Y}=\{+1,-1\}$`。输入`$x \in \mathcal{X}$`表示实例的特征向量,对应于输入空间(特征空间)的点;输出`$y \in \mathcal{Y}$`表示实例的类别。由输入空间到输出空间的如下函数:
`$$  f(x) = sign(w \cdot x + b) $$`
其中, `$w$`和`$b$`为感知机模型参数, `$w\in R^n$`叫作权值(weight)或权值向量(weightvector),`$b \in R$`叫作偏置(bias),`$w \cdot x$`表示w和x的内积。sign是符号函数,即
	`$$  sign(x)=\begin{cases}
		+1, &x \geqslant 0\\
		-1, & x < 0
		\end{cases}  $$`   
感知机是一种线性分类模型,属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类
模型(linear classification	model)或线性分类器(linear	 classifier),即函数集合`$\{f \mid f(x)=w \cdot x+b\}$`。   

感知机有如下几何解释:线性方程 `$$w \cdot x + b =0$$`
对应于特征空间`$R^n$`中的一个超平面S,其中w是超平面的法向量,b是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点(特征向量)分别被分为正、负两类。因此,超平面S称为分离超平面(separating	hyperplane).   

# 感知机学习策略

为了找出一个能够将训练集正实例点和负实例点完全正确分开的分离超平面,即确定感知机模型参数`$w,b$`,需要确定一个学习策略,即定义(经验)损失函数并将损失函数极小化。   
损失函数的一个自然选择是误分类点的总数。但是,这样的损失函数不是参数w,b的连续可导函数,不易优化。**损失函数的另一个选择是误分类点到超平面S的总距离**,这是感知机所采用的。为此,首先写出输入空间`$R^n$`中任一点`$x_0$`到超平面S的距离:
`$$ \frac{1}{\| w \|} | w \cdot x_0 + b |  $$`
 这里,`$||w||$`是w的`$L_2$`范数。   
 对于任意误分类的数据`$(x_i ,y_i )$`来说,
 `$$ -y_i (w \cdot x_i +b) > 0 $$`
 成立.因此,误分类点`$x_i$`到超平面S的距离是
`$$ -\frac{1}{\| w \|} y_i( w \cdot x_i + b )  $$`
这样,假设超平面S的误分类点集合为M,那么所有误分类点到超平面S的总距离为
`$$ -\frac{1}{\| w \|}  \sum_{x_i \in M} y_i( w \cdot x_i + b )  $$`
不考虑 `$ \frac{1}{\| w \|}$`, 给定训练数据集`$T=\{ (x_1, y_1),(x_2, y_2), \cdots,(x_N, y_N) \}$`,其中,`$x_i \in \mathcal{X}=R^n$`, `$ y_i \in \mathcal{Y} = \{ +1, -1\}, i = 1,2,\cdots,N $`.感知机`$sign(w\cdot x + b)$`学习的损失函数定义为:
`$$  L(w, b) = -\sum_{x_i \in M} y_i (w \cdot x_i +b) $$`
其中M为误分类点的集合,。这个损失函数就是感知机学习的经验风险函数。   
显然,损失函数`$L(w,b)$`是非负的。如果没有误分类点,损失函数值是0。而且,误分类点越少,误分类点离超平面越近,损失函数值就越小。一个特定的样本点的损失函数:在误分类时是参数`$w,b$`的线性函数,在正确分类时是0。因此,给定训练数据集T,损失函数`$L(w,b)$`是`$w,b$`的连续可导函数。


# 感知机学习算法
感知机学习算法是误分类驱动的,具体采用随机梯度下降法(stochastic	gradient descent)。首先,任意选取一个超平面`$w_0,b_0$`,然后用梯度下降法不断地极小化目标函数。**极小化过程中不是一次使M中所有误分类点的梯度下降,而是一次随机选取一个误分类点使其梯度下降**。   
假设误分类点集合M是固定的,那么损失函数`$L(w,b)$`的梯度为:
`$$ \nabla_w L(w,b) = -\sum_{x_i \in M} y_i x_i \\ 
\nabla_b L(w,b) = -\sum_{x_i \in M} y_i $$`
随机选取一个误分类点`$(x_i,y_i)$`,对`$w,b$`进行更新:
`$$ w \leftarrow w + \eta y_i x_i \\
b \leftarrow b + \eta y_i$$`
`$\eta(0<\eta\le1)$`是学习率.   

## 感知机学习算法的原始形式(算法)

输入:训练数据集`$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$`,   
其中,`$x_i \in \mathcal{X}=R^n$`, `$ y_i \in \mathcal{Y} = \{ +1, -1\}, i = 1,2,\cdots,N $学习率$\eta(0<\eta\le1)$`.    
输出:`$w,b$`;感知机模型`$f(x)=sign(w \cdot x+b)$`。   
步骤:   

1. 选取初值`$w_0,b_0$`   
2. 在训练集中选取数据`$(x_i,y_i)$`   
3. 如果`$y_i(w \cdot x_i+b)≤0$`   
`$$ w \leftarrow w + \eta y_i x_i \\
b \leftarrow b + \eta y_i$$`
4. 转至(2),直至训练集中没有误分类点。   
这种学习算法直观上有如下解释:当一个实例点被误分类,即位于分离超平面的错误一侧时,则调整`$w,b$`的值,使分离超平面向该误分类点的一侧移动,以减少该误分类点与超平面间的距离,直至超平面越过该误分类
点使其被正确分类。


## 感知机算法的收敛性
为了便于叙述与推导,将偏置b并入权重向量w,记作`$\hat{w} =(w^T,b)^T$`,同样也将输入向量加以扩充,加进常数1,记作 `$\hat{x}=(x^T,1)^T$`。这样, `$\hat{w} , \hat{x} \in R^{N+1}$`。显然, `$\hat{w} \hat{x}=w·x+b$`。   
**定理2.1(Novikoff)** 设训练数据集`$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$`是线性可分的,其中`$x_i \in \mathcal{X}=R^n$`, `$ y_i \in \mathcal{Y} = \{ +1, -1\}, i = 1,2,\cdots,N $`,则   

1. 存在满足条件`$|| \hat{w}_{opt}||=1$`的超平面 `$\hat{w}_{opt} \cdot  \hat{x} =w_{opt}·x+b_{opt}=0$`将训练数据集完全正确分开;且存在`$ \gamma>0$`,对所有`$i=1,2,...,N$`有
`$$ y_i( \hat{w}_{opt} \cdot  \hat{x}_i ) = y_i( w_{opt} \cdot x_i + b_{opt}) \ge \gamma $$`
2. 令$R=\max \limits_{1 \le i \le N} || \hat{x}_i ||$` ,则感知机算法在训练数据集上的误分类次数k满足不等式:
`$$ k \le \left(\frac{R} {\gamma} \right)^2  $$`

## 感知机学习算法的对偶形式
对偶形式的基本想法是,将`$w$`和`$b$`表示为实例`$x_i$`和标记`$y_i$`的线性组合的形式,通过求解其系数而求得`$w$`和`$b$`。    
不失一般性,可假设初始值$w_0,b_0$均为0。对误分类点`$(x_i,y_i)$`通过
`$$ w \leftarrow w + \eta y_i x_i \\
b \leftarrow b + \eta y_i$$`
逐步修改`$w,b$`,设修改`$n_i$`次,则`$w,b$`关于样本`$(x_i,y_i)$`的增量分别是`$\alpha_i y_i x_i$`和`$\alpha_i y_i$`,这里`$\alpha_i=n_i \eta$`看出,最后学习到的`$w,b$`可以分别表示为
`$$ w = \sum_{i=1}^N \alpha_i y_i x_i \\
	 b = \sum_{i=1}^N \alpha_i y_i $$`
这里, `$\alpha_i \ge 0 ,i = 1,2, \cdots, N$`,当`$\eta =1$`时,表示第i个实例点由于误分而进行更新的次数。实例点更新次数越多,意味着它距离分离超平面越近,也就越难正确分类。换句话说,这样的实例对学习结果影响最大。

## 感知机学习算法的对偶形式(算法)
输入:训练数据集`$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$`,   
其中,`$x_i \in \mathcal{X}=R^n$`, `$ y_i \in \mathcal{Y} = \{ +1, -1\}, i = 1,2,\cdots,N $`学习率`$\eta(0<\eta\le1)$`.    
输出: `$\alpha, b$`: 感知机模型`$f(x) = sign\left(  \sum_{j=1}^N \alpha_j y_j x_j \cdot x + b \right)$`,其中`$\alpha = (\alpha_1, \alpha_2, \cdots, \alpha_N)^T$`   
步骤:   

1. 初始化`$\alpha \leftarrow 0, b \leftarrow 0$`
2. 在训练集中选取数据`$(x_i,y_i)$`
3. 如果`$ y_i \left( \sum_{j=1}^N \alpha_j y_j x_j \cdot x_i + b  \right) \le 0 $`
`$$ \alpha_i \leftarrow \alpha_i + \eta \\  
	b \leftarrow b + \eta y_i  $$`
4. 转至(2)直到没有误分类数据。   

对偶形式中训练实例仅以内积的形式出现。为了方便,可以预先将训练集中实例间的内积计算出来并以矩
阵的形式存储,这个矩阵就是所谓的Gram矩阵(Gram	matrix)
`$$ G=[x_i \cdot x_j]_{N \times N} $$`
