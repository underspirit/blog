<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Leon&#39;s Blog</title>
    <link>http://blog.songru.org/</link>
    <description>Recent content on Leon&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Fri, 03 Jun 2016 14:55:16 +0800</lastBuildDate>
    <atom:link href="http://blog.songru.org/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Sequence to Sequence Learning with Neural Networks</title>
      <link>http://blog.songru.org/posts/notebook/Sequence-to-Sequence-Learning-with-Neural-Networks/</link>
      <pubDate>Fri, 03 Jun 2016 14:55:16 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/notebook/Sequence-to-Sequence-Learning-with-Neural-Networks/</guid>
      <description>&lt;p&gt;该论文使用Encoder-Decoder模型, 进行end-to-end的训练来进行机器翻译. 该论文与&lt;a href=&#34;http://blog.songru.org/posts/notebook/Neural_Machine_Translation_By_Jointly_Learning_To_Align_And_Translate_NOTE/&#34;&gt;Neural Machine Translation By Jointly Learning To Align And Translate&lt;/a&gt;的主要区别在于:&lt;br /&gt;
1. 该论文中不是使用Bidirectional RNN, 而是使用了multi-layer RNN, 发现比shallow RNN效果好(可能因为deep结构包含更多的隐藏状态).&lt;br /&gt;
2. 该论文将输入序列进行反向, 再一次输入到Encoder的RNN中.&lt;br /&gt;
正常顺序输入的Encoder序列与Decoder序列之间有一个比较大的&amp;rdquo;minimal time lag&amp;rdquo;, 将输入序列方向之后, 虽然输入序列与输出序列对应词语之间的平均距离没有改变, 但是输入序列最前面的一些词语与输出序列的对应词语更加近了, 也就是说&amp;rdquo;minimal time lag&amp;rdquo;减小了.&lt;br /&gt;
3. 该论文没有使用attention.&lt;br /&gt;
4. 该论文在输出翻译句子时使用了&lt;a href=&#34;https://en.wikipedia.org/wiki/Beam_search&#34;&gt;beam search&lt;/a&gt;方法, 而不是传统的greedy search方法.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;下面介绍一下Sequence to Sequence Learning当中的&lt;strong&gt;beam search&lt;/strong&gt;:&lt;br /&gt;
在训练好模型之后, 预测阶段我们需要逐个词语的进行预测, 通常的做法(greedy search)是从Decoder的第一个时刻开始选取概率最大的词作为下一个时刻的输入, 这样依次预测得到最终的结果. 但是这里存在一个问题, 就是&lt;strong&gt;最可能的预测结果序列可能并不是从选取的最可能的那个词语开始的.&lt;/strong&gt;为了找到概率最大的预测结果, 可以简单的采用列举所有可能的输出序列, 然后选取最大概率的一个, 但是计算的复杂度与句子长度呈指数级增长, 效率太低.&lt;/p&gt;

&lt;p&gt;Beam search的思想是首先指定一个数$b$, 称$b$为beam size或beam width. 接下来要做的不是找到最有可能的第一个词, 而是第一个词中最可能的前$b$个(这$b$个候选词就成为beam); 接下来由第一个词预测第二个词, 依次使用选出的$b$个候选词进行预测, 并计算所有这些长度为2的序列的概率(一共$b*n$个这样的序列, $n$为词典大小), 从中选出概率最大的$b$个. 接下来使用前面选定的b个最大概率序列(长度为2)来计算概率最大的长度为3的前$n$个序列, 以此类推.&lt;br /&gt;
Beam search的计算量是greedy search的$b$倍, $b$一般不会取太大(2-5貌似).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;So the RNN estimates the joint distribution:&lt;br /&gt;
$p(X_1, X_2, X_3, \cdots, X_N)$ over a set of random variables&lt;br /&gt;
What we really want is the mode of this distribution, that is the point with the highest probability. One way to get this is through sampling from the joint distribution and taking the highest probability sample, but this is slow.&lt;br /&gt;
The RNN factors the joint distribution for 3 random variables in this way:&lt;br /&gt;
$$p(x_1, x_2, x_3) = p(x_3 \mid x_1,x_2) * p(x_2 \mid x_1) * p(x_1)$$&lt;br /&gt;
Beam search uses a heuristic that assumes that chains of random variables with high probability have fairly high probability conditionals. Basically you take the k highest probability solutions for $p(x_1)$, then for each of those take the k highest probability solutions for $p(x_2 \mid x_1)$. You then take the k of those with the highest value for $p(x_2 \mid x_1) * p(x_1)$ and repeat.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Opinion Mining with Deep Recurrent Neural Networks笔记</title>
      <link>http://blog.songru.org/posts/notebook/Opinion_Mining_with_Deep_Recurrent_Neural_Networks_NOTE/</link>
      <pubDate>Tue, 24 May 2016 21:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/notebook/Opinion_Mining_with_Deep_Recurrent_Neural_Networks_NOTE/</guid>
      <description>&lt;p&gt;本文对传统的RNN进行改进，结合BidirectionalRNN，提出了Deep bidirectional RNN。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.songru.org/img/1464096878586.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;传统的RNN&lt;/strong&gt;（图a）信息传播方向是从前往后一个方向，某个时刻t的隐藏状态仅仅包含它之前词语的语义信息：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464097111614.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;br /&gt;
$f$为非线性激活函数（比如sigmoid），$g$为输出的计算函数（比如softmax）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bidirectional RNN&lt;/strong&gt;（图b）包含前向和后向RNN两个部分，分别从相反的反向进行信息的传播，再将每个时刻的两个方向的隐藏状态联合起来计算输出值：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464097064201.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;可以看到，前向RNN和反向RNN的参数是互相独立的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep  RNN&lt;/strong&gt;（图c）是将多个传统RNN进行叠加而来，即每一层计算隐藏状态的输入都为上一层的输出。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep Bidirectional  RNN&lt;/strong&gt;（图d）则是将Bidirectional RNN与Deep RNN结合起来：&lt;br /&gt;
第$i$层（$i&amp;gt;1$）第$t$个时刻的前向隐藏状态$\overrightarrow{{h_t}^{(i)}}$依赖于三个输入：第$i-1$层$t$时刻的前向隐藏状态$\overrightarrow{{h_t}^{(i-1)}}$和后向隐藏状态$\overleftarrow{{h_t}^{(i-1)}}$以及第$i$层$t-1$时刻的前向隐藏状态.&lt;br /&gt;
第$i$层（$i&amp;gt;1$）第$t$个时刻的后向隐藏状态$\overleftarrow{{h_t}^{(i)}}$的计算同理:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464098995066.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;第1层隐藏状态机算比较特殊,也比较简单:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464099041075.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;输出的计算有两种选择:一是使用所有时刻的隐藏状态计算输出,或者仅使用最后一个时刻的隐藏状态.这里使用第二种方案:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1464097751597.png&#34; alt=&#34;Alt text&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;该文是通过堆叠（stack）RNN的方式达到Deep RNN的结构，&lt;a href=&#34;http://arxiv.org/abs/1312.6026&#34;&gt;Pascanu等&lt;/a&gt;的论文采取了另一种思路进行Deep RNN的扩展。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ubuntu下matplotlib绘图中文乱码</title>
      <link>http://blog.songru.org/posts/linux/Ubuntu_Matplotlib_fix/</link>
      <pubDate>Sat, 23 Apr 2016 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/linux/Ubuntu_Matplotlib_fix/</guid>
      <description>

&lt;p&gt;原因：在Ubuntu下安装了各种中文字体，但是修改matplotlibrc文件后，均提示找不到该字体，猜测可能matplotlib字体列表与系统字体列表不同。&lt;/p&gt;

&lt;h2 id=&#34;方法一-持久性修改:c4a9591d3c6525e63df3a6e304bb9db6&#34;&gt;方法一（持久性修改）&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;首先查看matplotlib支持的中文字体&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# -*- coding: utf-8 -*-
from matplotlib.font_manager import FontManager
import subprocess

fm = FontManager()
mat_fonts = set(f.name for f in fm.ttflist)

output = subprocess.check_output(
    &#39;fc-list :lang=zh -f &amp;quot;%{family}\n&amp;quot;&#39;, shell=True)
# print &#39;*&#39; * 10, &#39;系统可用的中文字体&#39;, &#39;*&#39; * 10
# print output
zh_fonts = set(f.split(&#39;,&#39;, 1)[0] for f in output.split(&#39;\n&#39;))
available = mat_fonts &amp;amp; zh_fonts

print &#39;*&#39; * 10, &#39;可用的字体&#39;, &#39;*&#39; * 10
for f in available:
    print f
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出为：
Droid Sans Fallback
YaHei Consolas Hybrid
就是求出系统字体列表与matplotlib字体列表的交集&lt;/p&gt;

&lt;p&gt;2.修改matplotlibrc文件&lt;br /&gt;
Ubuntu默认对应的是/etc/matplotlibrc，可以复制到～/.matplotlibrc/matplotlibrc，然后配置后者即可
修改&lt;strong&gt;font.sans-serif&lt;/strong&gt;为上面的一个输出结果即可, 还需要修改&lt;strong&gt;axes.unicode_minus&lt;/strong&gt;为&lt;strong&gt;False&lt;/strong&gt;,否则图像是负号&amp;rsquo;-&amp;lsquo;会显示为方块.&lt;/p&gt;

&lt;h2 id=&#34;方法二-临时性修改:c4a9591d3c6525e63df3a6e304bb9db6&#34;&gt;方法二（临时性修改）&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# -*- coding: utf-8 -*-
 import matplotlib as mpl
 import matplotlib.pyplot as plt
 
 mpl.rcParams[&#39;font.sans-serif&#39;] = [&#39;Droid Sans Fallback&#39;] # 指定字体名字
 mpl.rcParams[&#39;axes.unicode_minus&#39;] = False #解决保存图像是负号&#39;-&#39;显示为方块的问题
 plt.figure()
 plt.xlabel(u&#39;性别&#39;)
 plt.ylabel(u&#39;人数&#39;)
 plt.xticks((0,1),(u&#39;男&#39;,u&#39;女&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# -*- coding: utf-8 -*-
 import matplotlib.pyplot as plt
 from matplotlib import font_manager

 zh_font = font_manager.FontProperties(fname=r&#39;/home/lsr/Documents/simsun.ttf&#39;, size=14) # 指定字体文件
 
 plt.figure()
 plt.xlabel(u&#39;性别&#39;, fontproperties=zh_font) # 使用字体配置
 plt.ylabel(u&#39;人数&#39;,fontproperties=zh_font)
 plt.xticks((0,1),(u&#39;男&#39;,u&#39;女&#39;)) # 没有使用字体配置，乱码
 plt.bar(left = (0,1),height = (1,0.5),width = 0.35)
 plt.show()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Neural Machine Translation By Jointly Learning To Align And Translate笔记</title>
      <link>http://blog.songru.org/posts/notebook/Neural_Machine_Translation_By_Jointly_Learning_To_Align_And_Translate_NOTE/</link>
      <pubDate>Sun, 27 Mar 2016 22:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/notebook/Neural_Machine_Translation_By_Jointly_Learning_To_Align_And_Translate_NOTE/</guid>
      <description>&lt;p&gt;本文主要创新在传统的神经机器翻译上进行改进，确切的说是改进了基本的RNN Encoder-Decoder模型,提出了Alignment model，即实现了Attention model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;传统的encoder-decoder模型如下图所示:
&lt;img src=&#34;http://blog.songru.org/img/1458651310553.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
该模型通过神经网络将所有的输入信息压缩为一个固定长度的向量$w$,然后通过decoder的神经网络解码这个$w$，最后得出翻译结果.&lt;br /&gt;
使用固定长度的向量是该模型的一个缺点,因为该向量很难将所有需要的信息编码到其中，对于长度大的句子,该模型的效果会显著下降.&lt;/p&gt;

&lt;p&gt;本文作者提出的改进模型结构为:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458651809449.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
主要有4个方面改进:&lt;br /&gt;
1. &lt;strong&gt;GRU 和 Bidirectional RNN&lt;/strong&gt;&lt;br /&gt;
RNN网络的结果采用的是GRU单元，双向神经网络(Bidirectional).一个BiRNN由前向(forward)和后向(backward)RNN组成,前向RNN &amp;amp;(\stackrel{\rightarrow}{\mbox{f}})&amp;amp; 按顺序读取输入序列(从&amp;amp;(x_f)&amp;amp;到&amp;amp;(x_{T_x})&amp;amp;)，计算得出前向RNN的隐含状态序列&amp;amp;((\vec{h_1},\cdots,\vec{h_{T_x}} ))&amp;amp; 。反向RNN$\stackrel{\leftarrow}{\mbox{f}}$则逆序读取输入序列(从$x_{T_x}$到$x_1$)，计算得出$(\stackrel{\leftarrow}{h_1},\cdots,\stackrel{\leftarrow}{h_{T_x}})$ .&lt;br /&gt;
对于一个词$x_j$直接concatenating前向$\vec{h_j}$与后向$\stackrel{\leftarrow}{h_j}$，即$h_j=\left[\begin{array}{c}\stackrel{\rightarrow}{h_j}  \\  \stackrel{\leftarrow}{h_j}  \end{array} \right]$，计算时词向量矩阵$E$是前向与后向网络共享的，其他参数则不是.&lt;br /&gt;
2. &lt;strong&gt;对齐模型(alignment model)&lt;/strong&gt;&lt;br /&gt;
该模型也可以说是实现了注意力模型(Attention model).&lt;br /&gt;
通过该模型计算得到输入时刻$j$在预测输出时刻$i$时所占的权重$\alpha _{ij}$.比如说翻译&lt;strong&gt;&amp;ldquo;我喜欢飞机&amp;rdquo;&lt;/strong&gt;到&lt;strong&gt;&amp;ldquo;I like airplane&amp;rdquo;&lt;/strong&gt;,翻译输出&lt;strong&gt;&amp;ldquo;I&amp;rdquo;&lt;/strong&gt;时，&lt;strong&gt;&amp;ldquo;我&amp;rdquo;&lt;/strong&gt;字所占的权重会比较大.&lt;br /&gt;
权重$\alpha _{ij}$是通过一个单层的多层感知机计算得到,该模型与算法中其他部分同时训练:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458736905308.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
3. &lt;strong&gt;Encoder 和 Decoder&lt;/strong&gt;&lt;br /&gt;
Encoder的计算如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458737365201.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
$E$表示词向量的矩阵，$x_i$表示$i$时刻的词,是一个$k$维的向量(词典大小维度的向量,应该是one-hot的)，反向过程的计算与上面类似。&lt;/p&gt;

&lt;p&gt;Decoder的计算如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458737469906.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
初始的隐藏状态为：$$s_0 = tanh(W_s\stackrel{\leftarrow}{h_1})$$&lt;br /&gt;
每一步的context向量都需要通过Alignment model重新计算:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738176634.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738239955.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
最后计算$y_i$的概率:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738415785.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738447372.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458738462004.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
使用了maxout（第二个公式），取相邻两个数中较大的一个。需要计算词典中所有词出现的概率，最后取最大的那一个？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;具体的实现细节见论文的附录&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation笔记</title>
      <link>http://blog.songru.org/posts/notebook/Learning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation_NOTE/</link>
      <pubDate>Sun, 27 Mar 2016 21:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/notebook/Learning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation_NOTE/</guid>
      <description>&lt;p&gt;本文主要创新在于提出一个新的神经网络模型RNN Encoder-Decoder，并提出GRU单元.将训练的模型作为standard phrase-based statistical machine translation system的一部分，用于计算phrase table中的每一个phrase的得分。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;1. RNN Encoder-Decoder&lt;/strong&gt;
该模型由两个RNN组成，分别作为Encoder和Decoder，Encoder的作用是读取一个变长的序列数据，将其编码为一个固定长度的向量，再通过Decoder将这个向量解码为一个变长的序列数据.&lt;/p&gt;

&lt;p&gt;Encoder为一个由GRU单元组成的RNN网络:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458734687528.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
$e(x_t)$代表t时刻输入词的向量表示，初始的隐藏状态$h^{(0)}$固定为$0$，最后计算得出一个固定长度的编码结果：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458735958249.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
Decoder也为一个由GRU单元组成的RNN网络:
它首先初始化隐藏状态：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458736097645.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
其网络的计算公式为：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/1458736032536.png&#34; alt=&#34;Alt text | center&#34; /&gt;
&lt;br /&gt;
这里每一个时刻的计算都用到了Encoder传递过来的编码向量$c$，并且它的值是不变的。&lt;br /&gt;
最后通过Softmax计算概率（没看懂），还有maxout。。。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theano 中使用pydot报错</title>
      <link>http://blog.songru.org/posts/machine-learning/theano%20pydot/</link>
      <pubDate>Sun, 03 Jan 2016 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/machine-learning/theano%20pydot/</guid>
      <description>&lt;p&gt;运行:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 切换为使用cpu,有这句var_with_name_simple=True的时候会报错...,换成gpu就不报错...
theano.printing.pydotprint(forward_prop, var_with_name_simple=True, compact=True, outfile=&#39;img/nn-theano-forward_prop.png&#39;, format=&#39;png&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;报错:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pywintypes.error(2,&#39;RegOpenKeyEx&#39;,&#39;\xcf\xb5\xcd\xb3\xd5\xd2\xb2\xbb\xb5\xb...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因是未安装好graphviz,于是下载&lt;a href=&#34;http://www.graphviz.org/Download_windows.php&#34;&gt;安装包&lt;/a&gt;,并把安装目录的bin加入到path中(重要!)&lt;/p&gt;

&lt;p&gt;后来又报错:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RuntimeError: Failed to import pydot. You must install pydot for `pydotprint` to work.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因是使用的pyparsing版本太高(我的是2.0.3),使用低版本的即可:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    pip install pyparsing==1.5.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我还安装了pydot2,不知有什么用:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    pip install pydot2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我之后做了一个实验,再把pyparsing升级到2.0.3,报了一个不一样的错:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;InvocationException: Program terminated with status: -1073741819. stderr follows: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将pyparsing再降级即可&amp;hellip;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Window7安装theano、anaconda、CUDA</title>
      <link>http://blog.songru.org/posts/machine-learning/theano%20cuda%20install/</link>
      <pubDate>Sat, 02 Jan 2016 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/machine-learning/theano%20cuda%20install/</guid>
      <description>

&lt;p&gt;最开始我使用的是anaconda3，但是一直没有成功，并且很多库对python2的支持更好，所以最好尝试使用anaconda2.
因为要使用mingw，但是新版的anaconda都没有自带mingw，所以下载了老版本anaconda1.9.2.&lt;/p&gt;

&lt;h1 id=&#34;1-安装cuda:10082a9e681b37c239307c871d1bb53d&#34;&gt;1.安装CUDA&lt;/h1&gt;

&lt;p&gt;我的gpu是GTX 750ti|，使用的是CUDA7，安装简单（已久忘了细节，传说中使用默认路径安装好些，笔记本上是GTX 950M，CUDA7.5）。安装好CUDA后，跑程序可能遇到cuda is installed,but unavailale的错误，这个我查的结果说是显卡驱动太低，然后我就更新为最新的显卡驱动！！
安装完之后在cmd下执行nvcc -V查看版本成功的话，则表示小成。
要的话，进到sample目录，用相应的vs打开解决方案，然后生成解决方案，中途可能会有无法打开”d3dx9.h”、”d3dx10.h”、”d3dx11.h”头文件，可以&lt;a href=&#34;http://www.microsoft.com/en-us/download/details.aspx?id=6812&#34;&gt;下载DXSDK_Jun10.exe&lt;/a&gt;，然后安装到默认目录下；再编译工程即可；然后到bin目录跑生成的程序（deviceQuery.exe还有一些图形程序等），没问题的话则大成了。&lt;br /&gt;
下面是一个详细的步骤（copy来的）:&lt;br /&gt;
　　　1. 查看本机配置，查看显卡类型是否支持NVIDIA GPU，选中计算机&amp;ndash;&amp;gt; 右键属性 &amp;ndash;&amp;gt; 设备管理器 &amp;ndash;&amp;gt; 显示适配器：NVIDIA GeForce GT 610，在&lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;这里&lt;/a&gt;可以查到相应显卡的compute capability；&lt;br /&gt;
　　　2. 在&lt;a href=&#34;http://www.nvidia.cn/Download/index.aspx?lang=cn&#34;&gt;这里&lt;/a&gt;下载合适驱动347.88-desktop-win8-win7-winvista-64bit-international-whql.exe 并安装；&lt;br /&gt;
　　　3. 从&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;https://developer.nvidia.com/cuda-toolkit&lt;/a&gt;   根据本机类型下载相应的最新版本CUDA7.0安装；&lt;br /&gt;
　　　4. 按照&lt;a href=&#34;http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-microsoft-windows/index.html#axzz3W8BU10Ol&#34;&gt;官方文档&lt;/a&gt;步骤，验证是否安装正确：&lt;br /&gt;
　　　　　(1) 打开C:\ProgramData\NVIDIACorporation\CUDA Samples\v7.0目录下的Samples_vs2010.sln工程，分别在Debug、Release x64下编译整个工程；&lt;br /&gt;
　　　　　(2) 编译过程中，会提示找不到”d3dx9.h”、”d3dx10.h”、”d3dx11.h”头文件，可以&lt;a href=&#34;http://www.microsoft.com/en-us/download/details.aspx?id=6812&#34;&gt;下载DXSDK_Jun10.exe&lt;/a&gt;，然后安装到默认目录下；再编译工程即可；&lt;br /&gt;
　　　　　(3) 打开C:\ProgramData\NVIDIACorporation\CUDA Samples\v7.0\bin\win64\Release目录，打开cmd命令行，将deviceQuery.exe直接拖到cmd中，回车，会显示GPU显卡、CUDA版本等相关信息，最后一行显示：Result = PASS；&lt;br /&gt;
　　　　　(4) 将bandwidthTest.exe拖到cmd中，回车，会显示Device0: GeForce GT 610等相关信息，后面也会有一行显示：Result = PASS；&lt;/p&gt;

&lt;h1 id=&#34;2-安装anaconda:10082a9e681b37c239307c871d1bb53d&#34;&gt;2.安装anaconda&lt;/h1&gt;

&lt;p&gt;开始使用的是anacond192版本，最后都配置成功了，但是这个版本的ipython notebook等等东西版本太低，我就用conda update了一下，导致一切都玩完了，theano又跑不起来了。所以后来就换成了anaconda2.4.1，是anaconda2的最新版本，但是没有mingw（这时我已经知道怎么配了，所以没有也不怕，我自己安）。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;安装anaconda很简单，傻瓜式的&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;装mingw，运行conda install mingw即可&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;装theano，因为conda库中没有theano，所以使用pip安装，执行pip install theano即可&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;配置.theanorc.txt&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在home目录下(我的是c:/uesrs/lisongru)创建.theanorc.txt,输入一下内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[blas]
ldflags =
[gcc]
cxxflags = -IE:\Anaconda2\MinGW    #安装的mingw目录
[nvcc]
fastmath = True
flags=-LE:\Anaconda2\libs
compiler-bindir=C:\Program Files (x86)\Microsoft Visual Studio 10.0\VC\bin   #vs的目录,不知是否可以不要,因为下面path也配了
flags =  -arch=sm_30   #这句没有也行,好像,未测试
base_compiledir=path_to_a_directory_without_such_characters   #这句没有也行,好像,未测试
[global]
openmp = False
floatX = float32
device = gpu   #cpu则使用cpu
allow_input_downcast=True
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;3-配置环境变量:10082a9e681b37c239307c871d1bb53d&#34;&gt;3.配置环境变量&lt;/h1&gt;

&lt;p&gt;这一步非常关键,开始我的电脑有cygwin，并且也在path中，然后运行import theano总是报错，原因是cygwin被用来编译了，但我们要用的是。。。（我也不清楚),后来删除它的path,加入vs_bin的path,就好了.
下面是一些相关的path&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;VS10_VC_BIN%        #vs的bin目录,和上面的compiler-bindir一样,这必须有,没有则包找不到cl.exe...
%CUDA_PATH%\bin     #安装cuda好像默认会有,没有自己加,有它才能在cmd中nvcc -V查看
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5\libnvvp;        #cuda安装自带
C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common      #自带
%ANACONDA2_HOME%;%ANACONDA2_SCRIPTS%;%ANACONDA2_BIN%        #anaconda自带
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;4-遇到的问题:10082a9e681b37c239307c871d1bb53d&#34;&gt;4.遇到的问题&lt;/h1&gt;

&lt;p&gt;在笔记本上安装好一切后，报collect2: ld returned 1 exit status错，查的结果说是python 32位与64位冲突，那时笔记本上正好有32位的一个python，就卸载了，但还是不行。
再查，说是少了libpython包,安装之:conda install libpython，然后就好了。。。（这里有点奇怪，在台式机上没有这个包也成功了）。&lt;br /&gt;
&amp;gt; Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux命令（目前使用过的）</title>
      <link>http://blog.songru.org/posts/linux/linuxComands/</link>
      <pubDate>Tue, 22 Sep 2015 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/linux/linuxComands/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;移动文件（夹）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mv originalDir/source.txt targetDir/target.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;删除文件（夹）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rm dir/target.file   //删除文件
rm -r dir //删除文件夹
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;利用软件源下载安装软件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install package 安装包
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;解压文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf XXX压缩包
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建文件夹&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir dir    // -p参数可以创建子文件夹
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;touch dir/a.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;解压文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -zxvf file.tar.gz    //解压到当前目录
tar -zxvf file.tar.gz -C dir    //解压到制定目录
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是zip格式的话：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;unzip  abc.zip -d /home/test/
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;复制文件（夹）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp target.file copy.file    //复制文件
cp -r targetDir copyDir //复制文件夹，使用-r递归复制
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;打印当前路径&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pwd
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;管理员权限&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo otherComand...     //使用root权限执行后续命令,一次性的
sudo su    //登录root,以后的命令都是使用root权限
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看文件系统使用情况信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    df -Tha    //T显示分区类型，h易读格式，a显示所有   
    fdisk -l /dev/sda    // 查看指定磁盘分区情况
    parted /dev/sda    // parted为分区工具，p可查看分区情况，quit退出
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看文件(夹)大小&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;du filename/dir    //-s易于阅读,-h显示总览
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看内存使用情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;free -m    //-m表示单位为m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;任务管理器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;top     //退出按 q
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看系统信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;uname -a    //-a查看所有信息
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看网络接口信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ifconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;MD5校验&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo md5sum filename
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看文件(夹)权限情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ls -l path/filename      //查看path路径下名为filename的文件或文件夹的权限
ls -ls path    //查看path路径下的所有文件的权限
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改文件(夹)权限(change mode)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo chmod -（代表类型）×××（所有者）×××（组用户）×××（其他用户）    //其中×××指文件名（也可以是文件夹名，不过要在chmod后加-ld）,使用 -R 参数来递归修改所有子文件   


//三位数的每一位都表示一个用户类型的权限设置。取值是0～7，即二进制的[000]~[111],这个三位的二进制数的每一位分别表示读、写、执行权限。      
//如000表示三项权限均无，而100表示只读。这样，我们就有了下面的对应：
0 [000] 无任何权限
4 [100] 只读权限
6 [110] 读写权限
7 [111] 读写执行权限


//常用的
sudo chmod 600 ××× （只有所有者有读和写的权限）
sudo chmod 644 ××× （所有者有读和写的权限，组用户只有读的权限）
sudo chmod 700 ××× （只有所有者有读和写以及执行的权限）
sudo chmod 666 ××× （每个人都有读和写的权限）
sudo chmod 777 ××× （每个人都有读和写以及执行的权限）
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改文件所有者(change owner)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo chown -R lsr software/    // -R 递归修改 software文件夹所有者为lsr
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;读取文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat a.txt   # 读取a.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;转换文件编码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;iconv -f gbk -t utf-8   # 将文件编码从gbk转到utf8
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查找行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep -E &amp;quot;&amp;lt;content&amp;gt;|&amp;lt;contenttitle&amp;gt;&amp;quot;  # 保留含有&amp;lt;content&amp;gt;或者&amp;lt;contenttitle&amp;gt;的行,-E表示使用正则
grep -v &amp;quot;hehe&amp;quot; # -v 排除匹配的行
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;删除文本中指定内容&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tr -d &amp;quot;&amp;lt;content&amp;gt;|&amp;lt;contenttitle&amp;gt;|&amp;lt;/content&amp;gt;|&amp;lt;/contenttitle&amp;gt;|&amp;gt;&amp;quot;   # -d表示删除,删除引号内的字符
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多个命令连续执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat a.txt | iconv -f gbk -t utf-8 | grep -E &amp;quot;&amp;lt;content&amp;gt;|&amp;lt;contenttitle&amp;gt;&amp;quot; | tr -d &amp;quot;&amp;lt;content&amp;gt;|&amp;lt;contenttitle&amp;gt;|&amp;lt;/content&amp;gt;|&amp;lt;/contenttitle&amp;gt;|&amp;gt;&amp;quot; &amp;gt; corpus.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查找文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;find SogouCS/ -name &amp;quot;m_*&amp;quot;   # 在SogouCS目录中按文件名-name查找含有 m_*的文件
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命令参数传递&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# xargs将文件名传递过来, -i 表示使用传过来的参数替换 {}
find SogouCS/ -name &amp;quot;m_*&amp;quot; | xargs -i mv {} SogouCS_M/
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;大小写转换&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tr A-Z a-z  # 全部转换为小写
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;文本替换&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 替换’为&#39;，′为&#39;，&#39;为&#39; , -e 表示多个编辑,即多个sed同时, s表示替换,g表示全局替换
sed -e &amp;quot;s/’/&#39;/g&amp;quot; -e &amp;quot;s/′/&#39;/g&amp;quot; -e &amp;quot;s/&#39;&#39;/ /g&amp;quot;


# 替换A-Za-z&#39;_ \n之外的字符为空格,-c 表示排除..之外的都..
tr -c &amp;quot;A-Za-z&#39;_ \n&amp;quot; &amp;quot; &amp;quot;


# 例子             输入文件                                                                    输出到文件
sed -e &amp;quot;s/’/&#39;/g&amp;quot; -e &amp;quot;s/′/&#39;/g&amp;quot; -e &amp;quot;s/&#39;&#39;/ /g&amp;quot; &amp;lt; news.2012.en.shuffled | tr -c &amp;quot;A-Za-z&#39;_ \n&amp;quot; &amp;quot; &amp;quot; &amp;gt; news.2012.en.shuffled-norm0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;从第3000行开始，显示1000行。即显示3000~3999行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat filename | tail -n +3000 | head -n 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;显示1000行到3000行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat filename| head -n 3000 | tail -n +1000
*注意两种方法的顺序
分解：
    tail -n 1000：显示最后1000行
    tail -n +1000：从1000行开始显示，显示1000行以后的
    tail -n -1000：相当于tail -n 1000
    head -n 1000：显示前面1000行
    head -n +1000：显示前面1000行
    head -n -1000：从第1行到倒数1000行
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;用sed命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -n &#39;5,10p&#39; filename # 查看文件的第5行到第10行, p表示打印匹配行
sed -n &#39;5p&#39; filename # 只显示第5行
sed -n &#39;5,$p&#39; filename # 查看文件的第5行到最后一行, $表示最后一行
sed -n &#39;3,/movie/&#39;p temp.txt   # 只在第3行查找movie并打印
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Linux统计文件行数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wc -l file # - c 统计字节数, - l 统计行数, - w 统计字数。


1.统计demo目录下，js文件数量：
find demo/ -name &amp;quot;*.js&amp;quot; | wc -l


2.统计demo目录下所有js文件代码行数：
find demo/ -name &amp;quot;*.js&amp;quot; | xargs cat | wc -l 或 
wc -l `find ./ -name &amp;quot;*.js&amp;quot;` | tail -n 1 # 取最后一行,总行数


3.统计demo目录下所有js文件代码行数，过滤了空行：
find /demo -name &amp;quot;*.js&amp;quot; | xargs cat | grep -v ^$ | wc -l
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;打印进程信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ps aux
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ssh连接主机,并执行命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh username@ip &#39;command&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;shell中的一种for循环&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for((i=1;i&amp;lt;=10;i++))
do
    echo compute-0-$i
    ssh compute-0-$i &amp;quot;nvidia-smi | grep &#39;%&#39; &amp;quot;
 done
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;awk中的while循环&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;awk &#39;BEGIN{s=&amp;quot;&amp;quot;;i=11}{while(i&amp;lt;=NF){ s=s&amp;quot; &amp;quot;&amp;quot;&amp;quot;$i;i++; } print s;}&#39; # 提取出command &amp;quot;&amp;quot;用于拼接字符串, NF表示总的字段数
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;半角字符与全角字符之间的unicode码相差65248,而全角空格为12288,半角为32,需特殊对待.&lt;/p&gt;

&lt;p&gt;java代码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * 全角转半角
 * @param str
 * @return
 */
public static String FullWidth2HalfWidth(String str){
    if (null == str || str.length() &amp;lt;= 0) {
        return &amp;quot;&amp;quot;;
    }
    char []cs;
    cs = str.toCharArray();
    int k;
    for(int i = 0; i &amp;lt; cs.length; i++){
        k = (int)cs[i];
        if(k &amp;gt;= 65281 &amp;amp;&amp;amp; k &amp;lt;= 65374){
            cs[i] -= 65248;
        }else if(k == 12288 || k == 58380){
            cs[i] = 32;
        }
    }
    return new String(cs);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写的bat代码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@echo off
echo Deploying updates to GitHub...

:: Build the project.
hugo -t red

:: Add changes to git.
git add -A

:: Commit changes.
set msg=rebuilding site %date% %time%
if not &amp;quot;%1&amp;quot;==&amp;quot;&amp;quot; (
  set msg=%1
)
git commit -m &amp;quot;%msg&amp;quot;

:: Push source and build repos.
git push origin hugo
git subtree push --prefix=public origin master

pause
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对应的shell代码:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
echo -e &amp;quot;\033[0;32mDeploying updates to GitHub...\033[0m&amp;quot;

# Build the project.
hugo -t red

# Add changes to git.
git add -A

# Commit changes.
msg=&amp;quot;rebuilding site `date`&amp;quot;
if [ $# -eq 1 ]
  then msg=&amp;quot;$1&amp;quot;
fi
git commit -m &amp;quot;$msg&amp;quot;

# Push source and build repos.
git push origin hugo
git subtree push --prefix=public origin master
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>【自然语言处理之三】最小编辑距离（Minimum Edit Distance）</title>
      <link>http://blog.songru.org/posts/nlp/nlp_3_MinimumEditDistance/</link>
      <pubDate>Sun, 13 Sep 2015 21:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/nlp/nlp_3_MinimumEditDistance/</guid>
      <description>

&lt;h1 id=&#34;一-最小编辑距离:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;一、最小编辑距离&lt;/h1&gt;

&lt;h2 id=&#34;1-1-定义:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.1 定义&lt;/h2&gt;

&lt;p&gt;最小编辑距离（Minimum Edit Distance，MED），又称Levenshtein距离，是指两个字符串之间，由一个转成另一个所需要的最少编辑操作次数。允许的编辑操作包括：将一个字符替换成另一个字符（substitution，s），插入一个字符（insert，i）或者删除一个字符（delete，d），如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_1.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;1-2-求解:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.2 求解&lt;/h2&gt;

&lt;p&gt;在大学算法设计相关课程上，想必大家都已经学习过使用动态规划算法解最小编辑距离，形式化定义如下：
对于两个字符串X={x1,x2,x3&amp;hellip;,xn}和Y={y1,y2,y3,&amp;hellip;,ym}，x的长度是n，y的长度是m，则定义D(i,j)为子字符串X{x1,x2,&amp;hellip;,xi}于Y{y1,y2,&amp;hellip;,yj}间的最小编辑距离。我们的目标是求出D(n,m)。
则可以推出如下关系:&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_3.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;根据这个递推关系，我们计算所有的i和j的取值填入一个矩阵中：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_4.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;最终可以求得右上角的D(n,m)。&lt;/p&gt;

&lt;h2 id=&#34;1-3-回溯法对齐字符串:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.3 回溯法对齐字符串&lt;/h2&gt;

&lt;p&gt;通过上面的动态规划方法虽然可以求出来两个字符串之间的最小编辑距离，但是我们并不知道源字符串被编辑的情况，即哪些字符被删除，增加或者替换了。
我们只要能够将源字符串与目标字符串相应字符对齐，即可以得出具体的编辑情况。这里只需要对上面的动态规划方法稍加改进即可实现。
我们只需要记住当前位置是从之前哪一个位置计算而来即可，然后从D(n,m)逐个向前回溯就能够将两个字符串对齐，如图所示：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_5.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;其递推公式为：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_6.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;1-4-加权最小编辑距离:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.4 加权最小编辑距离&lt;/h2&gt;

&lt;p&gt;在基本的编辑距离基础上，结合实际应用，往往需要做一些变形或改进，如某些拼写错误相对其他更容易发生，同义词替换、虚词或修饰成分被删除（或插入）应该得到较小的惩罚，等等。
下图是一个统计好的拼写错误的转移矩阵：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_8.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;加权递推公式如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_9.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;1-5-最小编辑距离的应用:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;1.5 最小编辑距离的应用&lt;/h2&gt;

&lt;p&gt;最小编辑距离通常作为一种相似度计算函数被用于多种实际应用中，详细如下： （特别的，对于中文自然语言处理，一般以词为基本处理单元）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;拼写纠错（Spell Correction）：又拼写检查（Spell Checker），将每个词与词典中的词条比较，英文单词往往需要做词干提取等规范化处理，如果一个词在词典中不存在，就被认为是一个错误，然后试图提示N个最可能要输入的词——拼写建议。常用的提示单词的算法就是列出词典中与原词具有最小编辑距离的词条。&lt;br /&gt;
这里肯定有人有疑问：对每个不在词典中的词（假如长度为len）都与词典中的词条计算最小编辑距离，时间复杂度是不是太高了？的确，所以一般需要加一些剪枝策略，如：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;因为一般拼写检查应用只需要给出Top-N的纠正建议即可（N一般取10），那么我们可以从词典中按照长度依次为len、len-1、len+1、len-2、len-3、&amp;hellip;的词条比较；&lt;/li&gt;
&lt;li&gt;限定拼写建议词条与当前词条的最小编辑距离不能大于某个阈值；&lt;/li&gt;
&lt;li&gt;如果最小编辑距离为1的候选词条超过N后，终止处理；&lt;/li&gt;
&lt;li&gt;缓存常见的拼写错误和建议，提高性能。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;DNA分析：基因学的一个主要主题就是比较 DNA 序列并尝试找出两个序列的公共部分。如果两个 DNA 序列有类似的公共子序列，那么这些两个序列很可能是同源的。在比对两个序列时，不仅要考虑完全匹配的字符，还要考虑一个序列中的空格或间隙（或者，相反地，要考虑另一个序列中的插入部分）和不匹配，这两个方面都可能意味着突变（mutation）。在序列比对中，需要找到最优的比对（最优比对大致是指要将匹配的数量最大化，将空格和不匹配的数量最小化）。如果要更正式些，可以确定一个分数，为匹配的字符添加分数、为空格和不匹配的字符减去分数。&lt;br /&gt;
全局序列比对尝试找到两个完整的序列 S1和 S2之间的最佳比对。以下面两个 DNA 序列为例&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S1= GCCCTAGCG
S2= GCGCAATG
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果为每个匹配字符一分，一个空格扣两分，一个不匹配字符扣一分，那么下面的比对就是全局最优比对：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S1&#39;= GCCCTAGCG
S2&#39;= GCGC-AATG
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;连字符（-）代表空格。在 S2&amp;rsquo;中有五个匹配字符，一个空格（或者反过来说，在 S1&amp;rsquo;中有一个插入项），有三个不匹配字符。这样得到的分数是 (5 * 1) + (1 * -2) + (3 * -1) = 0，这是能够实现的最佳结果。&lt;br /&gt;
使用局部序列比对，不必对两个完整的序列进行比对，可以在每个序列中使用某些部分来获得最大得分。使用同样的序列 S1和 S2，以及同样的得分方案，可以得到以下局部最优比对 S1&amp;rdquo;和 S2&amp;rdquo;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S1      = GCCCTAGCG
S1&#39;&#39;=           GCG
S2&#39;&#39;=           GCG
S2      =       GCGCAATG
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个局部比对的得分是 (3 * 1) + (0 * -2) + (0 * -1) = 3。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;命名实体抽取（Named Entity Extraction）：由于实体的命名往往没有规律，如品牌名，且可能存在多种变形、拼写形式，如“IBM”和“IBM Inc.”，这样导致基于词典完全匹配的命名实体识别方法召回率较低，为此，我们可以使用编辑距离由完全匹配泛化到模糊匹配，先抽取实体名候选词。&lt;br /&gt;
具体的，可以将候选文本串与词典中的每个实体名进行编辑距离计算，当发现文本中的某一字符串的编辑距离值小于给定阈值时，将其作为实体名候选词；获取实体名候选词后，根据所处上下文使用启发式规则或者分类的方法判定候选词是否的确为实体名。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;实体共指（Entity Coreference）：通过计算任意两个实体名之间的最小编辑距离判定是否存在共指关系？如“IBM”和“IBM Inc.”, &amp;ldquo;Stanford President John Hennessy &amp;ldquo;和&amp;rdquo;Stanford University President John Hennessy&amp;rdquo;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;机器翻译（Machine Translation）：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;识别平行网页对：由于平行网页通常有相同或类似的界面结构，因此平行网页在HTML结构上应该有很大近似度。首先将网页的HTML标签抽取出来，连接成一个字符串，然后用最小编辑距离考察两个字符串的近似度。实际中，此策略一般与文档长度比例、句对齐翻译模型等方法结合使用，以识别最终的平行网页对。&lt;/li&gt;
&lt;li&gt;自动评测：首先存储机器翻译原文和不同质量级别的多个参考译文，评测时把自动翻译的译文对应到与其编辑距离最小的参考译文上，间接估算自动译文的质量，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_3_10.jpg&#34; alt=&#34;最小编辑距离&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;字符串核函数（String Kernel）：最小编辑距离作为字符串之间的相似度计算函数，用作核函数，集成在SVM中使用。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;二-参考资料:00940faa4e8eb28a14d8cda2941cecdb&#34;&gt;二、参考资料&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Lecture Slides：&lt;a href=&#34;http://spark-public.s3.amazonaws.com/nlp/slides/med.pptx&#34;&gt;Minimum Edit Distance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org&#34;&gt;http://en.wikipedia.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.google.com.hk/url?sa=t&amp;amp;rct=j&amp;amp;q=%E6%9C%80%E5%B0%8F%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB+%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91&amp;amp;source=web&amp;amp;cd=7&amp;amp;ved=0CG0QFjAG&amp;amp;url=http%3A%2F%2Fwww.ecice06.com%2FCN%2Farticle%2FdownloadArticleFile.do%3FattachType%3DPDF%26id%3D10174&amp;amp;ei=GNinT4iaKKaViQfDpbnKAw&amp;amp;usg=AFQjCNGbWIzPc5GpPghvdYQAnbQ8iIPMIw&#34;&gt;双语平行网页挖掘系统的设计与实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wenku.baidu.com/view/a50949dc7f1922791688e81d.html&#34;&gt;机器翻译系统评测规范&lt;/a&gt; .&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wenku.baidu.com/view/a50949dc7f1922791688e81d.html&#34;&gt;matrix67-漫话中文分词算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ir.hit.edu.cn/ir_papers/Vol_3/Improved-Edit-Distance%20Kernel%20for%20Chinese%20Relation%20Extraction.pdf&#34;&gt;Improved-Edit-Distance Kernel for Chinese Relation Extraction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>【自然语言处理之二】文本处理基础（Basic Text Processing）</title>
      <link>http://blog.songru.org/posts/nlp/nlp_2_BasicTextProcessing/</link>
      <pubDate>Fri, 11 Sep 2015 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/nlp/nlp_2_BasicTextProcessing/</guid>
      <description>

&lt;h1 id=&#34;一-文本处理基础:2e62519647c0e571a1761fc424fb31ad&#34;&gt;一、文本处理基础&lt;/h1&gt;

&lt;h2 id=&#34;1-1-正则表达式:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.1 正则表达式&lt;/h2&gt;

&lt;p&gt;自然语言处理过程中面临大量的文本处理工作，如词干提取、网页正文抽取、分词、断句、文本过滤、模式匹配等任务，而正则表达式往往是首选的文本预处理工具。
现在主流的编程语言对正则表达式都有较好的支持，如Grep、Awk、Sed、Python、Perl、Java、C/C++(推荐re2)等。
&lt;em&gt;注：课程中给出的正则表达式语法和示例在此略去&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-2-分词:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.2 分词&lt;/h2&gt;

&lt;p&gt;分词的操作就是将一句话中的词语全部切分开来。&lt;/p&gt;

&lt;h3 id=&#34;1-2-1-词典规模:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.2.1 词典规模&lt;/h3&gt;

&lt;p&gt;同一词条可能存在不同的时态、变形，那么给定文本语料库，如何确定词典规模呢？
首先定义两个变量Type和Token：
    &lt;strong&gt;Type&lt;/strong&gt;：词典中的元素，即独立词条
    &lt;strong&gt;Token&lt;/strong&gt;：词典中独立词条在文本中的每次出现
如果定义 N = number of tokens 和 V = vocabulary = set of types，|V| is the size of the vocabulary，那么根据Church and Gale (1990)的研究工作可知: |V| &amp;gt; O(N½) ，验证如下：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_2_1.jpg&#34; alt=&#34;词典规模&#34; /&gt;
&lt;/p&gt;

&lt;h3 id=&#34;1-2-2-分词算法:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.2.2 分词算法&lt;/h3&gt;

&lt;p&gt;任务：统计给定文本文件（如shake.txt）中词频分布，子任务：分词，频率统计，实现如下：&lt;/p&gt;

&lt;p&gt;英文分词非常简单，因为英文中每个单词之间（少数特殊写法额外考虑，We&amp;rsquo;re,isn&amp;rsquo;t&amp;hellip;)都由空格分割开来，所以分词只需要按空格将它们切分，然后对一些特殊的写法进一步处理即可。&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_2_2.jpg&#34; alt=&#34;分词算法&#34; /&gt;
&lt;br /&gt;
以上实现将非字母字符作为token分隔符作为简单的分词器实现，但是，这难免存在许多不合理之处，如下面的例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    - Finland’s capital  -&amp;gt;  Finland Finlands Finland’s  ?
    - what’re, I’m, isn’t  -&amp;gt;  What are, I am, is not
    - Hewlett-Packard   -&amp;gt;  Hewlett Packard ?
    - state-of-the-art     -&amp;gt;   state of the art ?
    - Lowercase  -&amp;gt;  lower-case lowercase lower case ?
    - San Francisco -&amp;gt;   one token or two?
    - m.p.h., PhD.  -&amp;gt;  ??
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的方法对英语这种包含固定分隔符的语言行之有效，对于汉语、日语、德语等文本则不再适用，所以就需要专门的分词技术。其中，最简单、最常用，甚至是最有效的方法就是最大匹配法（Maximum Matching），它是一种基于贪心思想的切词策略，主要包括正向最大匹配法（Forward Maximum Matching，FMM）、逆向最大匹配法（Backward Maximum Matching）与双向最大匹配法（Bi-direction Maximum Matching，BMM）。&lt;/p&gt;

&lt;p&gt;以FMM中文分词为例，步骤如下：
1.  选取包含N(N通常取6~8)个汉字的字符串作为最大字符串；
2. 把最大字符串与词典中的单词条目相匹配（词典通常使用Double Array Trie组织）；
3. 如果不能匹配，就去除最后一个汉字继续匹配，直到在词典中找到相应的词条为止。
例如：句子“莎拉波娃现在居住在美国东南部的佛罗里达“的分词结果是”莎拉波娃/   现在/   居住/   在/  美国/   东南部/     的/  佛罗里达/”。
另外，使用较多的分词方法有最少分词法、最短路径法、最大概率法（或词网格法，大规模语料库+HMM/HHMM）、字标注法等。&lt;/p&gt;

&lt;h3 id=&#34;1-2-3-分词难点:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.2.3 分词难点&lt;/h3&gt;

&lt;p&gt;-切分歧义：主要包括交集型歧义和覆盖型歧义，在汉语书面文本中占比并不大，而且一般都可以通过规则或建立词表解决。&lt;/p&gt;

&lt;p&gt;-未登录词：就是未在词典中记录的人名（中、外）、地名、机构名、新词、缩略语等，构成了中文自然语言处理永恒的难点。常见的解决方法有互信息、语言模型，以及基于最大熵或隐马尔科夫模型的统计分类方法。&lt;/p&gt;

&lt;h2 id=&#34;1-3-文本归一化-标准化:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.3 文本归一化(标准化)&lt;/h2&gt;

&lt;p&gt;文本归一化主要包括大小写转换、词干提取、繁简转换等问题。&lt;/p&gt;

&lt;h2 id=&#34;1-4-断句:2e62519647c0e571a1761fc424fb31ad&#34;&gt;1.4 断句&lt;/h2&gt;

&lt;p&gt;句子一般分为大句和小句，大句一般由“！”，“。”，“；”，“\“”、“？”等分割，可以表达完整的含义，小句一般由“，”分割，起停顿作用，需要上下文搭配表达特定的语义。
中文断句通常使用正则表达式将文本按照有分割意义的标点符号(如句号)分开即可，而对于英文文本，由于英文句号”.“在多种场景下被使用，如缩写“Inc.”、“Dr.”、“.02%”、“4.3”等，无法通过简单的正则表达式处理，为了识别英文句子边界，课程中给出了一种基于决策树（Decision Tree）的分类方法，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://blog.songru.org/img/nlp_2_3.jpg&#34; alt=&#34;断句&#34; /&gt;
&lt;br /&gt;
此方法的核心就是如何选取有效的特征？如句号前后的单词是否大写开头、是否为缩略词、前后是否存在数字、句号前的单词长度、句号前后的单词在语料库中作为句子边界的概率等等。当然，你也可以基于上述特征采用其他分类器解决断句问题，如罗辑回归（Logistic regression）、支持向量机（Support Vector Machine）、神经网络（Neural Nets）等。&lt;/p&gt;

&lt;h1 id=&#34;二-参考资料:2e62519647c0e571a1761fc424fb31ad&#34;&gt;二、参考资料&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Lecture Slides：&lt;a href=&#34;http://spark-public.s3.amazonaws.com/nlp/slides/textprocessingboth.pptx&#34;&gt;Basic Text Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org&#34;&gt;http://en.wikipedia.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;关毅，统计自然语言处理基础 课程PPT&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=4&amp;amp;ved=0CHAQFjAD&amp;amp;url=http%3A%2F%2Fwww2.denizyuret.com%2Fref%2Fchurch%2Fpublished_1990_darpa.ps.gz&amp;amp;ei=guWlT4rCFOWZ2QW3mbymAg&amp;amp;usg=AFQjCNFcEeYyaP8TQUYJxNVUkdoHZl98hg&amp;amp;sig2=17cCPZhMQzpWCHeWm-knag&#34;&gt;Gale, W. A. and K. W. Church (1990) “Estimation Procedures for Language Context: Poor Estimates are Worse than None,” Proceedings in Computational Statistics, 1990, p.69-74, Physica-Verlag, Heidelberg&lt;/a&gt; .&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.52nlp.cn/matrix67-%E6%BC%AB%E8%AF%9D%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95&#34;&gt;matrix67-漫话中文分词算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E5%AD%97%E6%A0%87%E6%B3%A8%E6%B3%951&#34;&gt;中文分词入门之字标注法&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>【自然语言处理之一】介绍</title>
      <link>http://blog.songru.org/posts/nlp/nlp_1_introduction/</link>
      <pubDate>Thu, 16 Jul 2015 10:09:12 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/nlp/nlp_1_introduction/</guid>
      <description>

&lt;h1 id=&#34;一-什么是自然语言处理-nature-language-processing:a1b5d0a3988a3fc353b3287af28cee1e&#34;&gt;一、什么是自然语言处理（Nature Language Processing）？&lt;/h1&gt;

&lt;p&gt;首先看什么是自然语言，自然语言是指一种自然地随文化演化的语言。各个国家地区的语言,英语、汉语、日语等等都属于自然语言，它们都是随这文化发展自然形成的，自然语言是人类交流和思维的主要工具，是人类智慧的结晶。与之相对的是“人造语言”，比如各种编程语言。&lt;/p&gt;

&lt;p&gt;自然语言处理的任务就是让计算机理解人类的自然语言，从而实现人与计算机之间用自然语言进行有效通信。&lt;/p&gt;

&lt;p&gt;自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。&lt;/p&gt;

&lt;h1 id=&#34;二-自然语言处理的实际应用:a1b5d0a3988a3fc353b3287af28cee1e&#34;&gt;二、自然语言处理的实际应用&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;机器翻译&lt;/strong&gt;&lt;br /&gt;
我们最常接触的自然语言处理的应用就是机器翻译，它能够将某种语言 译为你指定的目标语言。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;回答问题&lt;/strong&gt;&lt;br /&gt;
比如Iphone中的Siri能够理解用户所说的话,并能做出回应。&lt;br /&gt;
说到回答问题，不得不提IBM创造的Watson，它是一台懂得人类语言的超级电脑。2011年2月17日,它在美国最受欢迎的智力竞猜电视节目《危险边缘》中击败该节目历史上两位最成功的选手肯-詹宁斯和布拉德-鲁特，成为《危险边缘》节目新的王者。《危险边缘》是哥伦比亚广播公司益智问答游戏节目，已经经历了数十年历史。该节目的比赛以一种独特的问答形式进行，问题设置的涵盖面非常广泛，涉及到各个领域。与一般问答节目相反，《危险边缘》以答案形式提问、提问形式作答，比如这样一个题目：
&lt;code&gt;这种带刺类型的植物有大约50个种类；因此是依据它的多刺的果实来命名的。&lt;/code&gt;
这个题目的答案是仙人掌。显然这需要watson具有理解自然语言的能力才能对这种题目进行作答。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;情感分析&lt;/strong&gt;&lt;br /&gt;
又称倾向性分析和意见挖掘，它是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程，如从大量网页文本中分析用户对“数码相机”的“变焦、价格、大小、重量、闪光、易用性”等属性的情感倾向。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;信息抽取&lt;/strong&gt;&lt;br /&gt;
其目的是将非结构化或半结构化的自然语言描述文本转化结构化的数据，如自动根据邮件内容生成Calendar。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;三-自然语言处理的研究进展:a1b5d0a3988a3fc353b3287af28cee1e&#34;&gt;三、自然语言处理的研究进展&lt;/h1&gt;

&lt;p&gt;自然语言处理也被分为许多小的研究领域，并且在某些方面已经取得了很好成果，但很多方面的研究依然很困难。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;基本解决&lt;/strong&gt;：词性标注、命名实体识别、Spam识别&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;取得长足进展&lt;/strong&gt;：情感分析、共指消解、词义消歧、句法分析、机器翻译、信息抽取&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;挑战&lt;/strong&gt;：自动问答、复述、文摘、会话机器人&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.songru.org/img/nlp_challenge.png&#34; alt=&#34;challenge&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;这里记录几个专业术语：
英文 | 中文
&amp;mdash;- | &amp;mdash;-
Part-of-speech (POS) tagging | 词性标注
Named entity recognition(NER) | 命名实体识别
Sentiment analysis | 情感分析
Coreference resolution | 指代消解
Word sense disambiguation(WSD) | 词义消歧
Paraphrase | 复述
Summarization | 摘要生成
neologisms | 新词
idioms | 成语&lt;/p&gt;

&lt;h1 id=&#34;四-自然语言处理的难点:a1b5d0a3988a3fc353b3287af28cee1e&#34;&gt;四、自然语言处理的难点&lt;/h1&gt;

&lt;p&gt;有多方面原因导致自然语言处理变得相当困难：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;自然语言具有多样性（不同语种、不同地域、不同人群）。让计算机理解一种语言已经足够困难，何况世界上一共有n种语言。&lt;/li&gt;
&lt;li&gt;自然语言具有进化性。网络语言就是随着社会的发展进化而来，它和传统的正式语言又有一定的差别，出现了很多新的表达方式（网络新词等等）。&lt;/li&gt;
&lt;li&gt;自然语言具有模糊性。&lt;/li&gt;
&lt;li&gt;自然语言具有歧义性。处理歧义问题是NLP的核心问题。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;五-自然语言处理的基本方法:a1b5d0a3988a3fc353b3287af28cee1e&#34;&gt;五、自然语言处理的基本方法&lt;/h1&gt;

&lt;p&gt;在NLP领域中，目前使用较多是概率模型（probabilistic model）或称为统计模型（statistical model），或者称为“经验主义模型”，其建模过程基于大规模真实语料库，从中各级语言单位上的统计信息，并且，依据较低级语言单位上的统计信息，运行相关的统计、推理等技术计算较高级语言单位上的统计信息。与其相对的“理想主义模型”，即基于Chomsky形式语言的确定性语言模型，它建立在人脑中先天存在语法规则这一假设基础上，认为语言是人脑语言能力推导出来的，建立语言模型就是通过建立人工编辑的语言规则集来模拟这种先天的语言能力。&lt;/p&gt;

&lt;p&gt;本课程主要侧重于&lt;strong&gt;基于统计的NLP技术，如Viterbi、贝叶斯和最大熵分类器、N-gram语言模型&lt;/strong&gt;等等。
&amp;gt; Written with &lt;a href=&#34;https://stackedit.io/&#34;&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>统计学习方法读书笔记(2)-感知机</title>
      <link>http://blog.songru.org/posts/machine-learning/statical-learning-2-Inception/</link>
      <pubDate>Fri, 03 Jul 2015 16:32:27 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/machine-learning/statical-learning-2-Inception/</guid>
      <description>

&lt;p&gt;感知机(perceptron)是二类分类的线性分类模型,其输入为实例的特征向量,输出为实例的类别,取+1和–1二值。感知机对应于输入空间(特征空间)中将实例划分为正负两类的分离超平面,&lt;strong&gt;属于判别模型&lt;/strong&gt;。感知机学习旨在求出将训练数据进行线性划分的分离超平面,为此,导入基于误分类的损失函数,利用梯度下降法对损失函数进行极小化,求得感知机模型。&lt;/p&gt;

&lt;h1 id=&#34;感知机的定义:7af17af622005836a4a55655a1236804&#34;&gt;感知机的定义&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;感知机的定义&lt;/strong&gt;:  假设输入空间(特征空间)是&lt;code&gt;$\mathcal{X} \subseteq R^n$&lt;/code&gt;,输出空间是 &lt;code&gt;$ \mathcal{Y}=\{+1,-1\}$&lt;/code&gt;。输入&lt;code&gt;$x \in \mathcal{X}$&lt;/code&gt;表示实例的特征向量,对应于输入空间(特征空间)的点;输出&lt;code&gt;$y \in \mathcal{Y}$&lt;/code&gt;表示实例的类别。由输入空间到输出空间的如下函数:
&lt;code&gt;$$  f(x) = sign(w \cdot x + b) $$&lt;/code&gt;
其中, &lt;code&gt;$w$&lt;/code&gt;和&lt;code&gt;$b$&lt;/code&gt;为感知机模型参数, &lt;code&gt;$w\in R^n$&lt;/code&gt;叫作权值(weight)或权值向量(weightvector),&lt;code&gt;$b \in R$&lt;/code&gt;叫作偏置(bias),&lt;code&gt;$w \cdot x$&lt;/code&gt;表示w和x的内积。sign是符号函数,即
    &lt;code&gt;$$  sign(x)=\begin{cases}
        +1, &amp;amp;x \geqslant 0\\
        -1, &amp;amp; x &amp;lt; 0
        \end{cases}  $$&lt;/code&gt;&lt;br /&gt;
感知机是一种线性分类模型,属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类
模型(linear classification    model)或线性分类器(linear  classifier),即函数集合&lt;code&gt;$\{f \mid f(x)=w \cdot x+b\}$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;感知机有如下几何解释:线性方程 &lt;code&gt;$$w \cdot x + b =0$$&lt;/code&gt;
对应于特征空间&lt;code&gt;$R^n$&lt;/code&gt;中的一个超平面S,其中w是超平面的法向量,b是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点(特征向量)分别被分为正、负两类。因此,超平面S称为分离超平面(separating hyperplane).&lt;/p&gt;

&lt;h1 id=&#34;感知机学习策略:7af17af622005836a4a55655a1236804&#34;&gt;感知机学习策略&lt;/h1&gt;

&lt;p&gt;为了找出一个能够将训练集正实例点和负实例点完全正确分开的分离超平面,即确定感知机模型参数&lt;code&gt;$w,b$&lt;/code&gt;,需要确定一个学习策略,即定义(经验)损失函数并将损失函数极小化。&lt;br /&gt;
损失函数的一个自然选择是误分类点的总数。但是,这样的损失函数不是参数w,b的连续可导函数,不易优化。&lt;strong&gt;损失函数的另一个选择是误分类点到超平面S的总距离&lt;/strong&gt;,这是感知机所采用的。为此,首先写出输入空间&lt;code&gt;$R^n$&lt;/code&gt;中任一点&lt;code&gt;$x_0$&lt;/code&gt;到超平面S的距离:
&lt;code&gt;$$ \frac{1}{\| w \|} | w \cdot x_0 + b |  $$&lt;/code&gt;
 这里,&lt;code&gt;$||w||$&lt;/code&gt;是w的&lt;code&gt;$L_2$&lt;/code&gt;范数。&lt;br /&gt;
 对于任意误分类的数据&lt;code&gt;$(x_i ,y_i )$&lt;/code&gt;来说,
 &lt;code&gt;$$ -y_i (w \cdot x_i +b) &amp;gt; 0 $$&lt;/code&gt;
 成立.因此,误分类点&lt;code&gt;$x_i$&lt;/code&gt;到超平面S的距离是
&lt;code&gt;$$ -\frac{1}{\| w \|} y_i( w \cdot x_i + b )  $$&lt;/code&gt;
这样,假设超平面S的误分类点集合为M,那么所有误分类点到超平面S的总距离为
&lt;code&gt;$$ -\frac{1}{\| w \|}  \sum_{x_i \in M} y_i( w \cdot x_i + b )  $$&lt;/code&gt;
不考虑 &lt;code&gt;$ \frac{1}{\| w \|}$&lt;/code&gt;, 给定训练数据集&lt;code&gt;$T=\{ (x_1, y_1),(x_2, y_2), \cdots,(x_N, y_N) \}$&lt;/code&gt;,其中,&lt;code&gt;$x_i \in \mathcal{X}=R^n$&lt;/code&gt;, &lt;code&gt;$ y_i \in \mathcal{Y} = \{ +1, -1\}, i = 1,2,\cdots,N $&lt;/code&gt;.感知机&lt;code&gt;$sign(w\cdot x + b)$&lt;/code&gt;学习的损失函数定义为:
&lt;code&gt;$$  L(w, b) = -\sum_{x_i \in M} y_i (w \cdot x_i +b) $$&lt;/code&gt;
其中M为误分类点的集合,。这个损失函数就是感知机学习的经验风险函数。&lt;br /&gt;
显然,损失函数&lt;code&gt;$L(w,b)$&lt;/code&gt;是非负的。如果没有误分类点,损失函数值是0。而且,误分类点越少,误分类点离超平面越近,损失函数值就越小。一个特定的样本点的损失函数:在误分类时是参数&lt;code&gt;$w,b$&lt;/code&gt;的线性函数,在正确分类时是0。因此,给定训练数据集T,损失函数&lt;code&gt;$L(w,b)$&lt;/code&gt;是&lt;code&gt;$w,b$&lt;/code&gt;的连续可导函数。&lt;/p&gt;

&lt;h1 id=&#34;感知机学习算法:7af17af622005836a4a55655a1236804&#34;&gt;感知机学习算法&lt;/h1&gt;

&lt;p&gt;感知机学习算法是误分类驱动的,具体采用随机梯度下降法(stochastic   gradient descent)。首先,任意选取一个超平面&lt;code&gt;$w_0,b_0$&lt;/code&gt;,然后用梯度下降法不断地极小化目标函数。&lt;strong&gt;极小化过程中不是一次使M中所有误分类点的梯度下降,而是一次随机选取一个误分类点使其梯度下降&lt;/strong&gt;。&lt;br /&gt;
假设误分类点集合M是固定的,那么损失函数&lt;code&gt;$L(w,b)$&lt;/code&gt;的梯度为:
&lt;code&gt;$$ \nabla_w L(w,b) = -\sum_{x_i \in M} y_i x_i \\ 
\nabla_b L(w,b) = -\sum_{x_i \in M} y_i $$&lt;/code&gt;
随机选取一个误分类点&lt;code&gt;$(x_i,y_i)$&lt;/code&gt;,对&lt;code&gt;$w,b$&lt;/code&gt;进行更新:
&lt;code&gt;$$ w \leftarrow w + \eta y_i x_i \\
b \leftarrow b + \eta y_i$$&lt;/code&gt;
&lt;code&gt;$\eta(0&amp;lt;\eta\le1)$&lt;/code&gt;是学习率.&lt;/p&gt;

&lt;h2 id=&#34;感知机学习算法的原始形式-算法:7af17af622005836a4a55655a1236804&#34;&gt;感知机学习算法的原始形式(算法)&lt;/h2&gt;

&lt;p&gt;输入:训练数据集&lt;code&gt;$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$&lt;/code&gt;,&lt;br /&gt;
其中,&lt;code&gt;$x_i \in \mathcal{X}=R^n$&lt;/code&gt;, &lt;code&gt;$ y_i \in \mathcal{Y} = \{ +1, -1\}, i = 1,2,\cdots,N $学习率$\eta(0&amp;lt;\eta\le1)$&lt;/code&gt;.&lt;br /&gt;
输出:&lt;code&gt;$w,b$&lt;/code&gt;;感知机模型&lt;code&gt;$f(x)=sign(w \cdot x+b)$&lt;/code&gt;。&lt;br /&gt;
步骤:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;选取初值&lt;code&gt;$w_0,b_0$&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;在训练集中选取数据&lt;code&gt;$(x_i,y_i)$&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;如果&lt;code&gt;$y_i(w \cdot x_i+b)≤0$&lt;/code&gt;&lt;br /&gt;
&lt;code&gt;$$ w \leftarrow w + \eta y_i x_i \\
b \leftarrow b + \eta y_i$$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;转至(2),直至训练集中没有误分类点。&lt;br /&gt;
这种学习算法直观上有如下解释:当一个实例点被误分类,即位于分离超平面的错误一侧时,则调整&lt;code&gt;$w,b$&lt;/code&gt;的值,使分离超平面向该误分类点的一侧移动,以减少该误分类点与超平面间的距离,直至超平面越过该误分类
点使其被正确分类。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;感知机算法的收敛性:7af17af622005836a4a55655a1236804&#34;&gt;感知机算法的收敛性&lt;/h2&gt;

&lt;p&gt;为了便于叙述与推导,将偏置b并入权重向量w,记作&lt;code&gt;$\hat{w} =(w^T,b)^T$&lt;/code&gt;,同样也将输入向量加以扩充,加进常数1,记作 &lt;code&gt;$\hat{x}=(x^T,1)^T$&lt;/code&gt;。这样, &lt;code&gt;$\hat{w} , \hat{x} \in R^{N+1}$&lt;/code&gt;。显然, &lt;code&gt;$\hat{w} \hat{x}=w·x+b$&lt;/code&gt;。&lt;br /&gt;
&lt;strong&gt;定理2.1(Novikoff)&lt;/strong&gt; 设训练数据集&lt;code&gt;$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$&lt;/code&gt;是线性可分的,其中&lt;code&gt;$x_i \in \mathcal{X}=R^n$&lt;/code&gt;, &lt;code&gt;$ y_i \in \mathcal{Y} = \{ +1, -1\}, i = 1,2,\cdots,N $&lt;/code&gt;,则&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;存在满足条件&lt;code&gt;$|| \hat{w}_{opt}||=1$&lt;/code&gt;的超平面 &lt;code&gt;$\hat{w}_{opt} \cdot  \hat{x} =w_{opt}·x+b_{opt}=0$&lt;/code&gt;将训练数据集完全正确分开;且存在&lt;code&gt;$ \gamma&amp;gt;0$&lt;/code&gt;,对所有&lt;code&gt;$i=1,2,...,N$&lt;/code&gt;有
&lt;code&gt;$$ y_i( \hat{w}_{opt} \cdot  \hat{x}_i ) = y_i( w_{opt} \cdot x_i + b_{opt}) \ge \gamma $$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;令$R=\max \limits_{1 \le i \le N} || \hat{x}_i ||$&lt;code&gt;,则感知机算法在训练数据集上的误分类次数k满足不等式:
&lt;/code&gt;$$ k \le \left(\frac{R} {\gamma} \right)^2  $$`&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;感知机学习算法的对偶形式:7af17af622005836a4a55655a1236804&#34;&gt;感知机学习算法的对偶形式&lt;/h2&gt;

&lt;p&gt;对偶形式的基本想法是,将&lt;code&gt;$w$&lt;/code&gt;和&lt;code&gt;$b$&lt;/code&gt;表示为实例&lt;code&gt;$x_i$&lt;/code&gt;和标记&lt;code&gt;$y_i$&lt;/code&gt;的线性组合的形式,通过求解其系数而求得&lt;code&gt;$w$&lt;/code&gt;和&lt;code&gt;$b$&lt;/code&gt;。&lt;br /&gt;
不失一般性,可假设初始值$w_0,b_0$均为0。对误分类点&lt;code&gt;$(x_i,y_i)$&lt;/code&gt;通过
&lt;code&gt;$$ w \leftarrow w + \eta y_i x_i \\
b \leftarrow b + \eta y_i$$&lt;/code&gt;
逐步修改&lt;code&gt;$w,b$&lt;/code&gt;,设修改&lt;code&gt;$n_i$&lt;/code&gt;次,则&lt;code&gt;$w,b$&lt;/code&gt;关于样本&lt;code&gt;$(x_i,y_i)$&lt;/code&gt;的增量分别是&lt;code&gt;$\alpha_i y_i x_i$&lt;/code&gt;和&lt;code&gt;$\alpha_i y_i$&lt;/code&gt;,这里&lt;code&gt;$\alpha_i=n_i \eta$&lt;/code&gt;看出,最后学习到的&lt;code&gt;$w,b$&lt;/code&gt;可以分别表示为
&lt;code&gt;$$ w = \sum_{i=1}^N \alpha_i y_i x_i \\
     b = \sum_{i=1}^N \alpha_i y_i $$&lt;/code&gt;
这里, &lt;code&gt;$\alpha_i \ge 0 ,i = 1,2, \cdots, N$&lt;/code&gt;,当&lt;code&gt;$\eta =1$&lt;/code&gt;时,表示第i个实例点由于误分而进行更新的次数。实例点更新次数越多,意味着它距离分离超平面越近,也就越难正确分类。换句话说,这样的实例对学习结果影响最大。&lt;/p&gt;

&lt;h2 id=&#34;感知机学习算法的对偶形式-算法:7af17af622005836a4a55655a1236804&#34;&gt;感知机学习算法的对偶形式(算法)&lt;/h2&gt;

&lt;p&gt;输入:训练数据集&lt;code&gt;$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$&lt;/code&gt;,&lt;br /&gt;
其中,&lt;code&gt;$x_i \in \mathcal{X}=R^n$&lt;/code&gt;, &lt;code&gt;$ y_i \in \mathcal{Y} = \{ +1, -1\}, i = 1,2,\cdots,N $&lt;/code&gt;学习率&lt;code&gt;$\eta(0&amp;lt;\eta\le1)$&lt;/code&gt;.&lt;br /&gt;
输出: &lt;code&gt;$\alpha, b$&lt;/code&gt;: 感知机模型&lt;code&gt;$f(x) = sign\left(  \sum_{j=1}^N \alpha_j y_j x_j \cdot x + b \right)$&lt;/code&gt;,其中&lt;code&gt;$\alpha = (\alpha_1, \alpha_2, \cdots, \alpha_N)^T$&lt;/code&gt;&lt;br /&gt;
步骤:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;初始化&lt;code&gt;$\alpha \leftarrow 0, b \leftarrow 0$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在训练集中选取数据&lt;code&gt;$(x_i,y_i)$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;如果&lt;code&gt;$ y_i \left( \sum_{j=1}^N \alpha_j y_j x_j \cdot x_i + b  \right) \le 0 $&lt;/code&gt;
&lt;code&gt;$$ \alpha_i \leftarrow \alpha_i + \eta \\  
b \leftarrow b + \eta y_i  $$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;转至(2)直到没有误分类数据。&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对偶形式中训练实例仅以内积的形式出现。为了方便,可以预先将训练集中实例间的内积计算出来并以矩
阵的形式存储,这个矩阵就是所谓的Gram矩阵(Gram matrix)
&lt;code&gt;$$ G=[x_i \cdot x_j]_{N \times N} $$&lt;/code&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>统计学习方法读书笔记(1)-统计学习方法概论</title>
      <link>http://blog.songru.org/posts/machine-learning/statical-learning-1/</link>
      <pubDate>Fri, 03 Jul 2015 15:48:57 +0800</pubDate>
      
      <guid>http://blog.songru.org/posts/machine-learning/statical-learning-1/</guid>
      <description>

&lt;p&gt;统计学习方法都是由模型、策略和算法这三个要素构成。&lt;/p&gt;

&lt;h1 id=&#34;1-模型:b06856972157b699cfa45f18de0ae7a3&#34;&gt;1. 模型&lt;/h1&gt;

&lt;p&gt;在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间(hypothesis    space)包含所有可能的条件概率分布或决策函数。例如,假设决策函数是输入变量的线性函数,那么模型的假设空间就是所有这些线性函数构成的函数集合。假设空间中的模型一般有无穷多个。&lt;br /&gt;
假设空间用&lt;code&gt;$\mathcal{F}$&lt;/code&gt;表示。假设空间可以定义为决策函数的集合：
&lt;code&gt;$$\mathcal{F}=\{f \mid Y=f(x)\}$$&lt;/code&gt;
其中, &lt;code&gt;$X$&lt;/code&gt;和&lt;code&gt;$Y$&lt;/code&gt;是定义在输入空间&lt;code&gt;$\mathcal{X}$&lt;/code&gt;和输出空间&lt;code&gt;$\mathcal{Y}$&lt;/code&gt;上的变量。这时$\mathcal{F}$通常是由一个参数向量决定的函数族:&lt;br /&gt;
&lt;code&gt;$$\mathcal{F}=\{f \mid Y=f_{\theta}(x), \theta \in R^n \}$$&lt;/code&gt;
参数向量&lt;code&gt;$\theta$&lt;/code&gt;取值于n维欧氏空间&lt;code&gt;$R^n$&lt;/code&gt;,称为参数空间(parameter space)。假设空间也可以定义为条件概率的集合:&lt;br /&gt;
&lt;code&gt;$$\mathcal{F}=\{P \mid P(Y \mid X) \}$$&lt;/code&gt;&lt;br /&gt;
这时&lt;code&gt;$\mathcal{F}$&lt;/code&gt;通常是由一个参数向量决定的条件概率分布族:&lt;br /&gt;
&lt;code&gt;$$\mathcal{F}=\{P \mid P_{\theta}(Y \mid X) , \theta \in R^n\}$$&lt;/code&gt;&lt;br /&gt;
参数向量&lt;code&gt;$\theta$&lt;/code&gt;取值于n维欧氏空间&lt;code&gt;$R^n$&lt;/code&gt;,也称为参数空间。&lt;br /&gt;
称由决策函数表示的模型为非概率模型,由条件概率表示的模型为概率模型。为了简便起见,当论及模型时,有时只用其中一种模型。&lt;/p&gt;

&lt;h1 id=&#34;2-策略:b06856972157b699cfa45f18de0ae7a3&#34;&gt;2. 策略&lt;/h1&gt;

&lt;p&gt;有了模型的假设空间,统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。统计学习的
目标在于从假设空间中选取最优模型。&lt;br /&gt;
首先引入损失函数与风险函数的概念。损失函数度量模型一次预测的好坏,风险函数度量平均意义下模型
预测的好坏。&lt;/p&gt;

&lt;h2 id=&#34;损失函数和风险函数:b06856972157b699cfa45f18de0ae7a3&#34;&gt;损失函数和风险函数&lt;/h2&gt;

&lt;p&gt;监督学习问题是在假设空间&lt;code&gt;$\mathcal{F}$&lt;/code&gt;中选取模型f作为决策函数,对于给定的输入X,由&lt;code&gt;$f(X)$&lt;/code&gt;给出相应的输出Y,这个输出的预测值&lt;code&gt;$f(X)$&lt;/code&gt;与真实值Y可能一致也可能不一致,用一个损失函数(loss function)或代价函数(cost
function)来度量预测错误的程度。损失函数是&lt;code&gt;$f(X)$&lt;/code&gt;和Y的非负实值函数,记作&lt;code&gt;$L(Y,f(X))$&lt;/code&gt;。
统计学习常用的损失函数有以下几种:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;0-1损失函数(0-1  loss function)
&lt;code&gt;$$  L(Y, f(X))=\begin{cases}
    1, &amp;amp;Y \neq f(X)\\
    0, &amp;amp;Y = f(X)
    \end{cases}  $$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;平方损失函数(quadratic loss function)
&lt;code&gt;$$  L(Y, f(X))=(Y - f(X))^2 $$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;绝对值损失函数(absolute loss function)
&lt;code&gt;$$   L(Y, f(X))= \lvert Y - f(X) \rvert $$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;对数损失函数(logarithmic loss function)或对数似然损失函数(loglikelihood loss    function)
&lt;code&gt;$$ L(Y, P(Y \mid X))=  - \log P(Y \mid X)$$&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;损失函数值越小,模型就越好。由于模型的输入、输出&lt;code&gt;$(X,Y)$&lt;/code&gt;是随机变量,遵循联合分布&lt;code&gt;$P(X,Y)$&lt;/code&gt;,所以损失函数的期望是&lt;br /&gt;
&lt;code&gt;$$ R_{exp} (f) = E_p \left[  L(Y, f(X)) \right] =  \int_{X \times Y}  L(y, f(x)) P(x, y) \ dxdy  $$&lt;/code&gt;
这是理论上模型&lt;code&gt;$f(X)$&lt;/code&gt;关于联合分布&lt;code&gt;$P(X,Y)$&lt;/code&gt;的平均意义下的损失,称为风险函数(risk    function)或期望损失
(expected   loss)。&lt;br /&gt;
学习的目标就是选择期望风险最小的模型。由于联合分布&lt;code&gt;$P(X,Y)$&lt;/code&gt;是未知的,&lt;code&gt;$R_{exp}(f)$&lt;/code&gt;不能直接计算。实际上,如果知道联合分布&lt;code&gt;$P(X,Y)$&lt;/code&gt;,可以从联合分布直接求出条件概率分布&lt;code&gt;$P(Y|X)$&lt;/code&gt;,也就不需要学习了。正因为不知道联合概率分布,所以才需要进行学习。这样一来,一方面根据期望风险最小学习模型要用到联合分布,另一方面联合分布又是未知的,所以监督学习就成为一个病态问题(ill-formed    problem)。&lt;br /&gt;
给定一个训练数据集
&lt;code&gt;$$  T = \{  (x_1, y_1), (x_2, y_2), \cdots , (x_N, y_N) \} $$&lt;/code&gt;&lt;br /&gt;
模型$f(X)$关于训练数据集的平均损失称为经验风险(empirical risk)或经验损失(empirical   loss),记作&lt;code&gt;$R_{emp}$&lt;/code&gt;:
&lt;code&gt;$$ R_{emp} (f) =  \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i)) $$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;期望风险&lt;code&gt;$R_{exp}(f)$&lt;/code&gt;是模型关于联合分布的期望损失,经验风险&lt;code&gt;$R_{emp}(f)$&lt;/code&gt;是模型关于训练样本集的平均损失。根据大数定律,当样本容量N趋于无穷时,经验风险&lt;code&gt;$R_{emp}(f)$&lt;/code&gt;趋于期望风险&lt;code&gt;$R_{exp}(f)$&lt;/code&gt;。所以一个很自然的想法是用经验风险估计期望风险。但是,由于现实中训练样本数目有限,甚至很小,所以用经验风险估计期望风险常常并不理想,要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略:&lt;strong&gt;经验风险最小化和结构风险最小化&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&#34;经验风险最小化与结构风险最小化:b06856972157b699cfa45f18de0ae7a3&#34;&gt;经验风险最小化与结构风险最小化&lt;/h2&gt;

&lt;p&gt;经验风险最小化(empirical   risk minimization,ERM)的策略认为,经验风险最小的模型是最优的模型。根据这一策略,按照经验风险最小化求最优模型就是求解最优化问题:
&lt;code&gt;$$ \min \limits_{f \in F} \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i)) $$&lt;/code&gt;&lt;br /&gt;
其中，&lt;code&gt;$F$&lt;/code&gt;是假设空间。
当样本容量足够大时,经验风险最小化能保证有很好的学习效果,在现实中被广泛采用。比如,极大似然估计(maximum likelihood estimation)就是经验风险最小化的一个例子。&lt;strong&gt;当模型是条件概率分布,损失函数是对数损失函数时,经验风险最小化就等价于极大似然估计&lt;/strong&gt;。&lt;br /&gt;
但是,当样本容量很小时,经验风险最小化学习的效果就未必很好,会产生“过拟合(over-fitting)”现象。&lt;br /&gt;
结构风险最小化(structural risk minimization,SRM)是为了防止过拟合而提出来的策略。&lt;strong&gt;结构风险最小化
等价于正则化(regularization)&lt;/strong&gt;。结构风险在经验风险上加上表示模型复杂度的正则化项(regularizer)或罚项(penalty term)。在假设空间、损失函数以及训练数据集确定的情况下,结构风险的定义是
&lt;code&gt;$$ R_{SRM} (f) = \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i)) + \lambda J(f) $$&lt;/code&gt;&lt;br /&gt;
其中&lt;code&gt;$J(f)$&lt;/code&gt;为模型的复杂度,是定义在假设空间 上的泛函。模型f越复杂,复杂度&lt;code&gt;$J(f)$&lt;/code&gt;就越大;反之,模型f越简
单,复杂度&lt;code&gt;$J(f)$&lt;/code&gt;就越小。也就是说,复杂度表示了对复杂模型的惩罚。 &lt;code&gt;$\lambda \ge 0$&lt;/code&gt;是系数,用以权衡经验风险和模型复杂度。&lt;strong&gt;结构风险小需要经验风险与模型复杂度同时小&lt;/strong&gt;。&lt;br /&gt;
贝叶斯估计中的最大后验概率估计(maximum posterior probability   estimation,MAP)就是结构风险最小化的一个例子。&lt;strong&gt;当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时,结构风险最小化就等价于最大后验概率估计&lt;/strong&gt;。&lt;/p&gt;

&lt;h1 id=&#34;3-算法:b06856972157b699cfa45f18de0ae7a3&#34;&gt;3. 算法&lt;/h1&gt;

&lt;p&gt;算法是指学习模型的具体计算方法。统计学习基于训练数据集,根据学习策略,从假设空间中选择最优模
型,最后需要考虑用什么样的计算方法求解最优模型。&lt;/p&gt;

&lt;h2 id=&#34;正则化:b06856972157b699cfa45f18de0ae7a3&#34;&gt;正则化&lt;/h2&gt;

&lt;p&gt;模型选择的典型方法是正则化(regularization)。正则化是结构风险最小化策略的实现,是在经验风险上
加一个正则化项(regularizer)或罚项(penalty term)。正则化项一般是模型复杂度的单调递增函数,模型越复
杂,正则化值就越大。比如,&lt;strong&gt;正则化项可以是模型参数向量的范数&lt;/strong&gt;。正则化一般具有如下形式:
&lt;code&gt;$$ \min \limits_{f \in F} \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i))  + \lambda J(f)$$&lt;/code&gt;&lt;br /&gt;
其中,第1项是经验风险,第2项是正则化项.&lt;br /&gt;
正则化项可以取不同的形式。例如&lt;strong&gt;,回归问题中,损失函数是平方损失,正则化项可以是参数向量的L2范
数&lt;/strong&gt;:
&lt;code&gt;$$ \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i))  + \frac{\lambda}{2} \| w \|^2  \quad (\text{L2范数之后再平方})  $$&lt;/code&gt;&lt;br /&gt;
这里,&lt;code&gt;$\| w \|$&lt;/code&gt;表示参数向量w的L2范数。&lt;br /&gt;
正则化项也可以是参数向量的L1范数:
&lt;code&gt;$$ \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i))  + \frac{\lambda}{2} \| w \|_1  \quad (\text{L1范数是绝对值之和})    $$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;正则化符合奥卡姆剃刀(Occam&amp;rsquo;s  razor)原理。奥卡姆剃刀原理应用于模型选择时变为以下想法:在所有可能选择的模型中,能够很好地解释已知数据并且十分简单才是最好的模型,也就是应该选择的模型。从贝叶斯估计的角度来看,正则化项对应于模型的先验概率。可以假设复杂的模型有较小的先验概率,简单的模型有较大的先验概率。&lt;/p&gt;

&lt;h1 id=&#34;4-生成模型与判别模型:b06856972157b699cfa45f18de0ae7a3&#34;&gt;4. 生成模型与判别模型&lt;/h1&gt;

&lt;p&gt;监督学习的任务就是学习一个模型,应用这一模型,对给定的输入预测相应的输出。这个模型的一般形式
为决策函数:
&lt;code&gt;$$ Y=f(X) $$&lt;/code&gt;
或者条件概率分布：
&lt;code&gt;$$ P(Y \mid X) $$&lt;/code&gt;
监督学习方法又可以分为生成方法(generative approach)和判别方法(discriminative approach)。所学到的模型分别称为生成模型(generative    model)和判别模型(discriminative model)。&lt;br /&gt;
&lt;strong&gt;生成方法由数据学习联合概率分布&lt;/strong&gt;&lt;code&gt;$P(X,Y)$&lt;/code&gt;,然后求出条件概率分布&lt;code&gt;$P(Y|X)$&lt;/code&gt;作为预测的模型,即生成模型:
&lt;code&gt;$$ P(Y \mid X) = \frac{P(X, Y)} {P(X)} $$&lt;/code&gt;
&lt;strong&gt;这样的方法之所以称为生成方法,是因为模型表示了给定输入X产生输出Y的生成关系&lt;/strong&gt;。典型的生成模型有:朴素贝叶斯法和隐马尔可夫模型.&lt;br /&gt;
判别方法由数据直接学习决策函数&lt;code&gt;$f(X)$&lt;/code&gt;或者条件概率分布&lt;code&gt;$P(Y \mid X)$&lt;/code&gt;作为预测的模型,即判别模型。判别方法关心的是对给定的输入X,应该预测什么样的输出Y。典型的判别模型包括:k近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等.&lt;br /&gt;
&lt;strong&gt;生成方法的特点&lt;/strong&gt;:生成方法可以还原出联合概率分布&lt;code&gt;$P(X,Y)$&lt;/code&gt;,而判别方法则不能;生成方法的学习收敛速
度更快,即当样本容量增加的时候,学到的模型可以更快地收敛于真实模型;当存在隐变量时,仍可以用生成
方法学习,此时判别方法就不能用。&lt;br /&gt;
&lt;strong&gt;判别方法的特点&lt;/strong&gt;:判别方法直接学习的是条件概率&lt;code&gt;$P(Y \mid X)$&lt;/code&gt;或决策函数&lt;code&gt;$f(X)$&lt;/code&gt;,直接面对预测,往往学习的准确率更高;由于直接学习&lt;code&gt;$P(Y \mid X)$&lt;/code&gt;或&lt;code&gt;$f(X)$&lt;/code&gt;,可以对数据进行各种程度上的抽象、定义特征并使用特征,因此可以简化学习问题。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>